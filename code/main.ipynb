{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import csv\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path(\"../data\")\n",
    "path_meta = Path(\"../meta\")\n",
    "path_rq1 = path_meta / 'rq1'\n",
    "path_rq2 = path_meta / 'rq2'\n",
    "\n",
    "platform_mapping = {\n",
    "    'ai2': 'AI2',\n",
    "    'gh': 'GitHub',\n",
    "    'hf': 'HuggingFace',\n",
    "    'pwc': 'PapersWithCode',\n",
    "    'shw': 'Self-hosted website',\n",
    "}\n",
    "\n",
    "platform_nontemplate = [\n",
    "    'gh', \n",
    "    'hf', \n",
    "    'shw'\n",
    "]\n",
    "\n",
    "leaderboard_splitting_criteria_mapping = {\n",
    "    'Functionality Evaluation': ['Domain task', 'Model capability', 'Evaluation benchmark', 'Language support', 'Task modality'],\n",
    "    'Evaluation Configuration': ['#prompt example', '#prompt token', 'Evaluation data', 'Evaluation metrics', 'Evaluator', 'Tokenizer'],\n",
    "    'Leaderboard Version': ['Leaderboard series', 'Release date'],\n",
    "    'Model Information': ['Model accessibility', 'Model size', 'Model type'],\n",
    "}\n",
    "\n",
    "metrics_synonyms = {\n",
    "    'accuracy': [\n",
    "        'acc',\n",
    "        'accuarcy',\n",
    "        'qa accuracy'\n",
    "    ],\n",
    "    'average': [\n",
    "        'avg',\n",
    "        '平均'\n",
    "    ],\n",
    "    'average accuracy': [\n",
    "        'avg. accuracy'\n",
    "    ],\n",
    "    'average score': ['平均分'],\n",
    "    'bleu': ['bleu score'],\n",
    "    'bleu-1': [\n",
    "        'narrativeqa bleu-1',\n",
    "        'socialiqa bleu-1',\n",
    "        'mcscript bleu-1',\n",
    "        'cosmosqa bleu-1'\n",
    "    ],\n",
    "    'bleu-4': ['bleu4'],\n",
    "    'code': ['代码'],\n",
    "    'elo rating': [\n",
    "        'chatbot arena elo',\n",
    "        'elo'\n",
    "    ],\n",
    "    'exact match': ['em', 'exact match accuracy'],\n",
    "    'lerc': [\n",
    "        'cosmosqa lerc',\n",
    "        'mcscript lerc',\n",
    "        'socialiqa lerc',\n",
    "        'narrativeqa lerc'\n",
    "    ],\n",
    "    'link': ['url'],\n",
    "    'mean rank': [\n",
    "        'text-to-video mean rank',\n",
    "        'video-to-text mean rank'\n",
    "    ],\n",
    "    'median rank': [\n",
    "        'text-to-video median rank',\n",
    "        'video-to-text median rank',\n",
    "        'text-to-videomedian rank',\n",
    "        'text-to-video medianr'\n",
    "    ],\n",
    "    'meteor': [\n",
    "        'cosmosqa meteor',\n",
    "        'narrativeqa meteor',\n",
    "        'socialiqa meteor',\n",
    "        'mcscript meteor'\n",
    "    ],\n",
    "    'neg mean rank': [\n",
    "        'i->t neg mean rank',\n",
    "        't->i neg mean rank'\n",
    "    ],\n",
    "    'organization': [\n",
    "        '发布机构',\n",
    "        '机构',\n",
    "        'orgaisation',\n",
    "    ],\n",
    "    'others': ['其他'],\n",
    "    'overall': ['xiezhi overall'],\n",
    "    'overall score': [\n",
    "        '总分',\n",
    "        '总体分数'\n",
    "    ],\n",
    "    'pass@1': [\n",
    "        'interview pass@1',\n",
    "        'competition pass@1',\n",
    "        'introductory pass@1'\n",
    "    ],\n",
    "    'pass@5': [\n",
    "        'interview pass@5',\n",
    "        'introductory pass@5',\n",
    "        'competition pass@5'\n",
    "    ],\n",
    "    'pass@1000': [\n",
    "        'interview pass@1000',\n",
    "        'competition pass@1000',\n",
    "        'introductory pass@1000'\n",
    "    ],\n",
    "    'pass@any': [\n",
    "        'introductory pass@any',\n",
    "        'competition pass@any',\n",
    "        'interview pass@any'\n",
    "    ],\n",
    "    '#parameters':  [\n",
    "        '#params',\n",
    "        '# params',\n",
    "        '#size',\n",
    "        '参数量',\n",
    "        'model size',\n",
    "        'model size/b',\n",
    "        'number of params',\n",
    "        'param',\n",
    "        'parameters',\n",
    "        'params',\n",
    "        'size'\n",
    "    ],\n",
    "    'perplexity': ['ppl'],\n",
    "    'precision@1': ['i->t p@1'],\n",
    "    'precision@20': ['p@20'],\n",
    "    'recall@1': [\n",
    "        'r@1',\n",
    "        'text-to-videor@1',\n",
    "        'video-to-text r@1',\n",
    "        'text-to-video r@1',\n",
    "        'text-to-image r@1',\n",
    "        'image-to-text r@1'\n",
    "    ],\n",
    "    'recall@5': [\n",
    "        'text-to-image r@5',\n",
    "        'video-to-text r@5',\n",
    "        'image-to-text r@5',\n",
    "        'text-to-video r@5',\n",
    "        'r@5',\n",
    "    ],\n",
    "    'recall@10': [\n",
    "        'recall@10 on 1 rounds',\n",
    "        'recall@10 on 2 rounds',\n",
    "        'recall@10 on 3 rounds',\n",
    "        'r@10',\n",
    "        'video-to-text r@10',\n",
    "        'text-to-image r@10',\n",
    "        'text-to-video r@10',\n",
    "        'image-to-text r@10'\n",
    "    ],\n",
    "    'recall@50': [\n",
    "        'text-to-video r@50',\n",
    "        'video-to-text r@50',\n",
    "    ],\n",
    "    'score': ['分数'],\n",
    "    'submission date': ['提交时间'],\n",
    "    'top-1 accuracy': ['top 1 accuracy'],\n",
    "    'top-5 accuracy': ['top 5 accuracy'],\n",
    "    'type': ['model type'],\n",
    "    'win rate': ['胜率'],\n",
    "    'word error rate': ['wer']\n",
    "}\n",
    "\n",
    "model_naming_convention = {\n",
    "    'Model Evaluator': [\n",
    "        'HallusionBench'\n",
    "    ],\n",
    "    'Model Publisher': [\n",
    "        'DS-1000',\n",
    "        'PubMedQA',\n",
    "        'Spider',\n",
    "        'Large Language Model Leaderboard',\n",
    "        'QuALITY',\n",
    "        'LawBench',\n",
    "        'BIRD',\n",
    "        'SuperGLUE',\n",
    "        'OpenEval (text)',\n",
    "        'HellaSwag'\n",
    "    ],\n",
    "    'Model Size': [\n",
    "        'SEED-Bench Leaderboard',\n",
    "        'InstructEval',\n",
    "        'LLMonitor',\n",
    "        'LawBench',\n",
    "        'HEIM',\n",
    "        'AlignBench',\n",
    "        'LLMPerf Leaderboard',\n",
    "        'SuperCLUE',\n",
    "        'Open Ko-LLM Leaderboard',\n",
    "        'AgentBench',\n",
    "        'CLiB',\n",
    "        'MathVista',\n",
    "        'LLM Benchmarker Suite',\n",
    "        'Xiezhi',\n",
    "        'HELM Classic',\n",
    "        'AlpacaEval',\n",
    "        'FacTool',\n",
    "        'SciGraphQA',\n",
    "        'MMMU',\n",
    "        'Colossal-AI',\n",
    "        'SuperCLUElyb',\n",
    "        'Coding LLMs Leaderboard',\n",
    "        'MMBench',\n",
    "        'ODEX',\n",
    "        'Spider',\n",
    "        'DocVQA',\n",
    "        'Toolbench Leaderboard',\n",
    "        'SummEdits',\n",
    "        'LLM-Leaderboard',\n",
    "        'LLM-Perf Leaderboard',\n",
    "        'FinanceIQ',\n",
    "        'SafetyBench',\n",
    "        'LLMEval',\n",
    "        'SuperLim (v2)',\n",
    "        'Chatbot Arena Leaderboad',\n",
    "        'M3KE',\n",
    "        'TabMWP',\n",
    "        'InfiMM-Eval',\n",
    "        'QuALITY',\n",
    "        'HELM Lite',\n",
    "        'C-Eval',\n",
    "        'TruthfulQA',\n",
    "        'GAOKAO-Bench',\n",
    "        'CLEVA',\n",
    "        'PromptBench',\n",
    "        'Q-Bench',\n",
    "        'tStoryCloze',\n",
    "        'InfiCoder-Eval',\n",
    "        'CMMLU',\n",
    "        'ScienceQA',\n",
    "        'RAFT',\n",
    "        'Big Code Models Leaderboard',\n",
    "        'Large Language Model Leaderboard',\n",
    "        'JustEval',\n",
    "        'FActScore',\n",
    "        'UHGEval',\n",
    "        'MMCU',\n",
    "        'HalluQA',\n",
    "        'Open LLM Leaderboard',\n",
    "        'SWE-bench',\n",
    "        'Hallucination Leaderboard',\n",
    "        'SuperCLUE-Auto',\n",
    "        'VisIT-Bench',\n",
    "        'Open Multilingual LLM Evaluation Leaderboard',\n",
    "        'SuperCLUE-Safety',\n",
    "        'FlagEval',\n",
    "        'LMExamQA',\n",
    "        'Video-Bench Leaderboard',\n",
    "        'DecodingTrust',\n",
    "        'MATH401',\n",
    "        'SuperGLUE',\n",
    "        'CG-Eval',\n",
    "        'BIRD',\n",
    "        'DS-1000',\n",
    "        'MMLU',\n",
    "        'MBPP',\n",
    "        'CanAiCode Leaderboard',\n",
    "        'EvalPlus',\n",
    "        'Toloka LLM Leaderboard',\n",
    "        'FELM',\n",
    "        'SuperCLUEgkzw',\n",
    "        'CMB',\n",
    "        'L-Eval',\n",
    "        'Multi-modal Modal Leaderboard',\n",
    "        'SuperCLUE-Agent',\n",
    "        'MME',\n",
    "        'OpenEval (text)',\n",
    "        'InterCode',\n",
    "        'MTEB',\n",
    "        'HumanEval',\n",
    "        'ANGO',\n",
    "        'LongBench',\n",
    "        'CCBench',\n",
    "        'BotChat',\n",
    "        'ScandEval'\n",
    "    ],\n",
    "    'Repository Username': [\n",
    "        'Big Code Models Leaderboard',\n",
    "        'Colossal-AI',\n",
    "        'LLMPerf Leaderboard',\n",
    "        'LLM-Perf Leaderboard',\n",
    "        'Open LLM Leaderboard',\n",
    "        'Open Ko-LLM Leaderboard',\n",
    "        'ScandEval'\n",
    "    ],\n",
    "    'Official Verification': [\n",
    "        'CCBench',\n",
    "        'MMBench',\n",
    "    ],\n",
    "}\n",
    "\n",
    "inverted_metrics_synonyms = {}\n",
    "for key, values in metrics_synonyms.items():\n",
    "    for value in values:\n",
    "        inverted_metrics_synonyms[value] = key\n",
    "\n",
    "leaderboard_splitting_criteria_mapping_inverse = {}\n",
    "for key, values in leaderboard_splitting_criteria_mapping.items():\n",
    "    for value in values:\n",
    "        leaderboard_splitting_criteria_mapping_inverse[value] = key\n",
    "\n",
    "model_naming_convention_count = {k: len(v) for k, v in model_naming_convention.items()}\n",
    "\n",
    "task_list = ['Question answering', 'Information retrieval', 'Summarization', 'Translation', 'Text classification']\n",
    "task_list_mm = ['Image classification', 'Image generation', 'Video classification', 'Video generation']\n",
    "capability_list = ['Knowledge', 'Reasoning', 'Alignment', 'Safety', 'Robustness', 'Efficiency']\n",
    "\n",
    "benchmark_description_list = {\n",
    "    '2WikiMQA': '''The 2WikiMultiHopQA benchmark is a multi-hop question answering (QA) dataset that aims to test and evaluate the reasoning and inference skills of machine learning models. Its main goal is to provide a comprehensive explanation for the reasoning process from the question to the answer, which is achieved through the use of evidence information containing a reasoning path for multi-hop questions. The defining features of the 2WikiMultiHopQA dataset include its use of both structured and unstructured data from Wikipedia and Wikidata, as well as its careful design of a pipeline and set of templates to ensure the quality of the questions and the multi-hop steps required to answer them. The dataset also utilizes logical rules to create questions that are natural but still require multi-hop reasoning. In terms of specific aspects of machine learning models that it aims to test and evaluate, the 2WikiMultiHopQA dataset focuses on multi-hop reasoning and inference skills. It is designed to be challenging for multi-hop models and requires them to undertake multi-hop reasoning to answer many of the questions. The dataset also provides comprehensive explanations for predictions, which can be used to evaluate the reasoning skills of a model.''',\n",
    "    'A-OKVQA': '''The A-OKVQA benchmark is a Visual Question Answering (VQA) dataset that aims to test and evaluate the reasoning capabilities of machine learning models. Its main goal is to provide a meaningful testbed for the development of AI models that can jointly reason over visual and natural language inputs, requiring a broad base of commonsense and world knowledge to answer. The defining features of A-OKVQA include a diverse set of about 25K questions paired with both multiple choice (MC) answer options and ten free-form answers to allow for direct answer (DA) evaluation. The questions cannot be answered by simply querying a knowledge base, and instead require some form of commonsense reasoning about the scene depicted in the image. In addition to the questions and answers, rationales are provided for each question to allow for models to use this extra annotation to train reasoning or knowledge retrieval methods or to build more explainable VQA models. The specific aspects of machine learning models that A-OKVQA aims to test and evaluate include their ability to perform visual recognition in the wild, language understanding, basic reasoning capabilities, and background knowledge about the world. The dataset requires models to use outside knowledge graphs and to perform commonsense reasoning about the scene depicted in the image. The paper provides a detailed analysis of the contents of A-OKVQA and baseline performance measurements over a variety of state-of-the-art vision-language models. The authors evaluate a large set of recent, high-performance baselines and show that while they show impressive performance on the proposed task, they lack the reasoning capability and/or the knowledge required to answer the questions, and there is a large room for improvement. Through extensive analyses, they show different models have different weaknesses and strengths.''',\n",
    "    'AC-NLG': '''The AC-NLG benchmark proposed in the paper aims to address the problem of generating judgment-discriminative court's views in civil cases while ensuring fairness in the judgment. The main goal of the benchmark is to evaluate the effectiveness of the proposed Attentional and Counterfactual based Natural Language Generation (AC-NLG) method in generating court's views that are both supportive and non-supportive of the plaintiff's claim, and to eliminate the bias that arises from the data generation mechanism. The defining features of the AC-NLG benchmark include a claim-aware encoder, a pair of counterfactual decoders, and a synergistic judgment predictor. The claim-aware encoder is designed to represent the fact description that emphasizes the plaintiff's claim. The counterfactual decoders are inspired by the backdoor adjustment in causal inference to address the confounding bias and the imbalance problem in judgment. The synergistic judgment predictor is jointly learned with the two decoders and decides which output to be selected as the final generated court's view. The specific aspects of machine learning models that the AC-NLG benchmark aims to test and evaluate include: 1. The ability of the claim-aware encoder to represent the fact description that emphasizes the plaintiff's claim. 2. The effectiveness of the counterfactual decoders in generating judgment-discriminative court's views (both supportive and non-supportive views) and eliminating the bias that arises from the data generation mechanism. 3. The performance of the synergistic judgment predictor in jointly learning with the two decoders and deciding which output to be selected as the final generated court's view. 4. The overall performance of the AC-NLG method in generating court's views that are both supportive and non-supportive of the plaintiff's claim, and ensuring fairness in the judgment.''',\n",
    "    'AFQMC': '''The AFQMC (Ant Financial Question Matching Corpus) benchmark is a significant component of the CLUE benchmark, designed to evaluate the ability of machine learning models to understand and process natural language in the context of question matching. The main goals, defining features, and specific aspects of machine learning models that it aims to test and evaluate are as follows: Main Goals: The primary goal of the AFQMC benchmark is to assess the performance of machine learning models in understanding and matching questions in the Chinese language. This involves evaluating the models' ability to comprehend the semantic similarity and relevance between pairs of questions. Defining Features: The AFQMC benchmark is characterized by the following defining features: - Focus on Question Matching: The benchmark specifically targets the task of question matching, where models are required to determine the similarity and relevance between pairs of questions. - Chinese Language Understanding: The benchmark is designed to assess the language understanding capabilities of machine learning models in the context of Chinese natural language processing. - Real-world Relevance: The benchmark aims to evaluate models based on their ability to handle real-world question matching scenarios, reflecting practical applications in information retrieval and question answering systems. Specific Aspects of Machine Learning Models: The AFQMC benchmark evaluates machine learning models based on specific aspects related to question matching, including: - Semantic Similarity: Models are tested on their capacity to capture semantic similarity between pairs of questions, reflecting their ability to understand the underlying meaning and context of the questions. - Relevance Assessment: The benchmark assesses models' capability to determine the relevance and relatedness of question pairs, which is crucial for tasks such as information retrieval and question answering. - Linguistic Phenomena: The benchmark may also involve testing models on unique linguistic properties of Chinese, such as handling aspect markers and anaphora resolution, to evaluate their robustness in understanding diverse linguistic phenomena.''',\n",
    "    'AGIEval': '''AGIEval aims to evaluate the general abilities of foundation models in tasks that are pertinent to human cognition and problem-solving. The benchmark is designed to assess the performance of large language models in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. The benchmark is derived from official, public, and high-standard admission and qualification exams intended for general human test-takers, with millions of individuals participating annually. The defining feature of AGIEval is its emphasis on human-level cognitive tasks. The benchmark incorporates a diverse range of official, public, and high-standard admission and qualification exams that cater to general human test-takers. These exams establish officially recognized standards for assessing human-level capabilities, making them well-suited for evaluating foundation models in the context of human-centric tasks. Additionally, AGIEval includes bilingual tasks in both Chinese and English, allowing for a more comprehensive evaluation of the models’ capabilities. The specific aspects of machine learning models that AGIEval aims to test and evaluate include their understanding, knowledge, and reasoning abilities. The benchmark focuses on tasks that require human-like cognitive capabilities, such as comprehension, inference, and problem-solving. AGIEval evaluates the performance of large language models in scenarios directly relevant to human decision-making, providing a more meaningful and comprehensive evaluation of their capabilities. In conclusion, the AGIEval benchmark is a novel and comprehensive benchmark designed to evaluate the general abilities of foundation models in tasks that are pertinent to human cognition and problem-solving. The benchmark incorporates a diverse range of official, public, and high-standard admission and qualification exams that cater to general human test-takers, establishing officially recognized standards for assessing human-level capabilities. AGIEval aims to test and evaluate the understanding, knowledge, and reasoning abilities of large language models in scenarios directly relevant to human decision-making.''',\n",
    "    'ALFWorld': '''The ALFWorld benchmark is a novel framework that aligns text-based environments with physically embodied robotic simulation. The benchmark aims to address the limitations of existing work by providing infrastructure for agents to learn abstract, text-based policies and then execute goals in a rich visual environment. Here's a comprehensive analysis of the ALFWorld benchmark: Main Goals: 1. Alignment of Text and Embodied Environments: The primary goal of ALFWorld is to align text-based environments with physically embodied environments, enabling agents to learn abstract, text-based policies and execute goals in a rich visual environment. 2. Generalization and Transfer Learning: ALFWorld aims to evaluate the generalization capabilities of machine learning models by training agents in abstract text-based environments and testing their performance in visually grounded embodied settings. The goal is to demonstrate that training in the abstract domain can lead to better generalization to unfamiliar embodied environments. Defining Features: 1. Parallel Interactive Environment: ALFWorld provides two views of the same underlying world and two modes of interaction: TextWorld, an abstract, text-based environment, and ALFRED, the embodied simulator. This parallel environment allows agents to learn and interact in both text-based and visually grounded settings. 2. Abstract Text-Based Policies: The framework enables agents to learn abstract, text-based policies in TextWorld and then transfer these policies to physically embodied tasks in ALFRED. This feature facilitates the alignment of abstract knowledge with concrete, visually grounded actions. 3. Infrastructure for Learning and Execution: ALFWorld provides the infrastructure necessary for agents to reason abstractly and execute concretely, addressing the limitations of existing work in providing both reasoning and execution capabilities for embodied agents. Specific Aspects of Machine Learning Models: 1. Generalization: ALFWorld evaluates the generalization capabilities of machine learning models by training agents in abstract text-based environments and testing their performance in visually grounded embodied settings. The goal is to demonstrate that agents can generalize to unfamiliar embodied environments after training in the abstract domain. 2. Transfer Learning: The framework aims to test the transfer learning capabilities of machine learning models by training agents to perform abstract tasks in TextWorld and then transferring the learned policies to embodied tasks in ALFRED. This transfer of knowledge from the abstract domain to the embodied world is a key aspect of the benchmark. In summary, the ALFWorld benchmark introduces a unique framework that aligns text-based and physically embodied environments, with a focus on evaluating the generalization and transfer learning capabilities of machine learning models. By providing infrastructure for learning abstract, text-based policies and executing visually grounded tasks, ALFWorld aims to advance the state of the art in interactive learning for embodied agents.''',\n",
    "    'ANGO': '''The ANGO benchmark is a Chinese evaluation benchmark for generation-oriented language models (LLMs) that aims to provide a more objective and effective guidance for model training and evaluation. Its main goals are to address the limitations of existing benchmarks, such as ranking distortion on leaderboards and lack of truly objective and effective guidance for model training, and to provide a more comprehensive evaluation platform for LLMs. The defining feature of ANGO is its use of Keypoint instead of Subject as the criteria for categorizing questions. A single question in ANGO can correspond to 1-6 different keypoints, and include 988 keypoint combinations, far exceeding the typical 50-70 subject categories. This allows ANGO to address the limitations of existing benchmarks that only allow for single-question with single-subject, disregarding that questions often require proficiency in multiple subjects and overlooking the existence of further subdivisions within each subject. In addition, ANGO incorporates quantifiable metrics for assessing question difficulty based on real human performance, thereby enriching the diversity of evaluation criteria. This addresses the limitations of existing benchmarks that lack quantifiable standards based on human performance, typically determined by the educational grade associated with the subject, despite significant variations in difficulty for different question types with the same subject. The specific aspects of machine learning models that ANGO aims to test and evaluate include their ability to handle a diverse range of disciplines, their proficiency in multiple subjects, their combined language and computational capabilities, and their ability to comprehend complex problems that require different abilities, such as analysis, reasoning, and computation. ANGO also aims to test and evaluate the effectiveness of models in handling long-context comprehension, Chinese reasoning, and sophisticated reasoning problems.''',\n",
    "    'ARC': '''AI2 Reasoning Challenge (ARC) benchmark aims to encourage progress in AI research on questions that require reasoning, use of commonsense knowledge, and other advanced methods for deeper text comprehension. The benchmark consists of a collection of 7787 natural science questions, authored for use on standardized tests, which have varying levels of difficulty and involve a wide variety of linguistic and inferential phenomena. The defining feature of the ARC benchmark is its partitioning into a Challenge Set and an Easy Set. The Challenge Set contains 2590 questions that are hard to answer with simple baselines, as they are answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm. The Easy Set contains the remaining 5197 questions, which are easier to answer and can be solved by simple AI methods. The specific aspects of machine learning models that the ARC benchmark aims to test and evaluate are their ability to reason, use commonsense knowledge, and comprehend text at a deeper level. The benchmark requires models to go beyond surface-level cues and use advanced methods to answer questions that involve complex linguistic and inferential phenomena.''',\n",
    "    'APPS': '''The APPS benchmark aims to evaluate the ability of machine learning models to generate Python code from natural language specifications. The main goals of the benchmark are to provide a comprehensive and rigorous testbed for evaluating models, to track advancements in program synthesis from natural language, and to provide foresight about the performance of future large-scale language models at the critical task of program synthesis from natural language. The defining features of the APPS benchmark are its use of natural language specifications, its evaluation of models on their ability to understand task descriptions and devise algorithms to solve these tasks, and its inclusion of hundreds of thousands of test cases and ground-truth solutions across different difficulty levels. Unlike prior work on code generation with Transformer language models, which mostly focuses on code translation and pseudocode-to-code, the APPS benchmark evaluates models on their ability to take specifications given in natural language and write code that meets these specifications. This setting mirrors how human coders are evaluated and is a more realistic and informative setting in which to benchmark models. The specific aspects of machine learning models that the APPS benchmark aims to test and evaluate include their ability to generate syntactically correct programs, their ability to understand natural language specifications and devise algorithms to solve tasks, and their ability to improve performance over time. The benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges. The problems were collected from different open-access coding websites such as Codeforces and Kattis, and they measure coding and problem-solving ability. The dataset includes over an order of magnitude more ground-truth solutions than other datasets, test cases, and natural language problem descriptions.''',\n",
    "    'AQUA-RAT': '''The AQUA-RAT benchmark is a dataset and evaluation framework for machine learning models that aim to solve algebraic word problems. The main goal of the benchmark is to test the ability of models to generate not only the correct answer to a problem but also a natural language explanation, or rationale, of the steps taken to arrive at the answer. The defining feature of the AQUA-RAT benchmark is the inclusion of these rationales, which are intended to provide a scaffold for the structure of the arithmetic programs that must be executed to solve the problem. The rationales are sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. The specific aspects of machine learning models that the AQUA-RAT benchmark aims to test and evaluate include their ability to perform mathematical reasoning, to understand natural language specifications of algebraic word problems, and to induce and model arithmetic programs that lead to the final answer. The benchmark also evaluates the fluency of the rationales generated by the models and their ability to solve the problem correctly.''',\n",
    "    'ActivityNet-QA': '''The ActivityNet-QA benchmark aims to provide a large-scale, fully annotated dataset for evaluating VideoQA performance. It aims to address the limitations of existing VideoQA datasets by offering a comprehensive replatform for understanding complex web videos through question answering. The dataset consists of 58,000 QA pairs on 5,800 complex web videos derived from the popular ActivityNet dataset. Defining Features: 1. Large-Scale and Fully Annotated: The ActivityNet-QA dataset is of significant scale, containing 58,000 QA pairs on 5,800 videos. It is fully annotated by humans, ensuring high-quality data for evaluation. 2. Video Length and Complexity: The benchmark focuses on long web videos, which presents a unique challenge for VideoQA models. This emphasis on complex video content sets it apart from datasets with shorter videos, providing a more realistic evaluation scenario. 3. Bilingual QA Pairs: The dataset includes bilingual question-answer pairs with alignment, which may inspire multi-lingual VideoQA studies. 4. Statistical Analysis: The paper presents detailed statistics and visualization analyses of the dataset, providing insights into question and answer lengths, as well as other distribution characteristics. Specific Aspects of Machine Learning Models: The ActivityNet-QA benchmark aims to test and evaluate VideoQA models in the following specific aspects: 1. Video Understanding: Given the complexity and length of the web videos in the dataset, VideoQA models are expected to demonstrate robust video understanding capabilities, including spatio-temporal reasoning. 2. Question Answering: The benchmark evaluates the ability of VideoQA models to accurately answer questions based on the content of long and complex web videos, requiring a deep understanding of both visual and textual information. 3. Video Representation Strategies: The paper explores various video representation strategies to improve VideoQA performance, especially for long videos. This aspect aims to test the effectiveness of different video feature representation models in the context of VideoQA.''',\n",
    "    'ArguAna': '''The ArguAna dataset is one of the nine heterogeneous retrieval tasks included in the BEIR benchmark. Its main goal is to evaluate the performance of machine learning models in retrieving arguments from online debates. The dataset consists of 1,000 debates from the website debate.org, each containing a topic, a description, and a set of arguments for and against the topic. The arguments are labeled as either \"pro\" or \"con\" and are further annotated with their stance (i.e., whether they support or oppose the topic) and their relevance to the topic. One defining feature of the ArguAna dataset is that it includes arguments that are not explicitly labeled as supporting or opposing the topic, but rather as \"neutral\" or \"off-topic\". This adds an additional layer of complexity to the retrieval task, as models must not only identify relevant arguments but also determine their stance. Another important aspect of the ArguAna dataset is that it includes arguments that are not explicitly stated but rather implied or inferred from the text. This requires models to have a deeper understanding of the language and context in order to accurately retrieve relevant arguments. Overall, the ArguAna dataset aims to test and evaluate the performance of machine learning models in retrieving and classifying arguments from online debates, taking into account the nuances of stance and implicit information.''',\n",
    "    'bAbI': '''The bAbI benchmark is a set of 20 toy tasks designed to evaluate the ability of machine learning models to perform reading comprehension via question answering. The main goal of the benchmark is to provide a standardized and controlled environment for evaluating and comparing different models' performance on these tasks. The defining features of the bAbI benchmark include the use of a simulated environment to generate the tasks and the provision of both training and test sets for each task. The tasks are designed to be relatively simple and self-contained, with a limited vocabulary size and a small number of entities, actors, locations, and objects used per task. The questions are unambiguous and require some thinking to answer, but are answerable by adult humans or children. The specific aspects of machine learning models that the bAbI benchmark aims to test and evaluate include the models' ability to reason from patterns hidden within the training data, their ability to handle different types of reasoning (such as chaining facts, simple induction, and deduction), and their ability to generalize to new situations. The benchmark also aims to evaluate the models' performance on tasks that require a far stronger form of supervision (using supporting facts) than is typically realistic.''',\n",
    "    'BBQ': '''The BBQ benchmark is a hand-built dataset designed to measure how social biases manifest in the outputs of question-answering (QA) models. The main goal of the benchmark is to provide a tool for researchers and model developers to identify and mitigate potential harms caused by biased models. The defining feature of the BBQ benchmark is that it covers a broad range of socially-salient attributes of individuals, many of which fall under protected categories, and each example template targets one specific bias that has been attested to cause harm. The benchmark evaluates model responses at two levels: (i) given an under-informative context, it tests how strongly responses reflect social biases, and (ii) given an adequately informative context, it tests whether the model's biases override a correct answer choice. The specific aspects of machine learning models that the BBQ benchmark aims to test and evaluate are their ability to reproduce harmful biases in their outputs, particularly in situations where the context is under-informative. The benchmark measures how often models rely on stereotypes and how accurately they answer questions that target biases against protected groups. It also evaluates the agreement between human annotators and the accuracy of different machine learning models, including UnifedQA's 11B parameter model, RoBERTa, and DeBERTaV3.''',\n",
    "    'BEIR': '''The BEIR benchmark aims to provide a comprehensive evaluation of information retrieval (IR) models in a zero-shot setting. The benchmark is designed to be heterogeneous, covering a wide range of tasks and domains, and to facilitate the evaluation of the generalization capabilities of IR models. The defining features of the BEIR benchmark are its diversity of tasks and domains, its inclusion of publicly available datasets, and its evaluation of state-of-the-art retrieval systems. The benchmark includes 18 datasets from nine different retrieval tasks, ranging from fact-checking to entity retrieval, and covering diverse domains such as news, biomedical literature, and social media. The datasets vary in size, query length, and document length, providing a comprehensive evaluation of IR models across a range of scenarios. The specific aspects of machine learning models that the BEIR benchmark aims to test and evaluate include their ability to generalize to new tasks and domains, their performance in a zero-shot setting, and their effectiveness in handling diverse query and document types. The benchmark evaluates 10 state-of-the-art retrieval systems, including lexical, sparse, dense, late-interaction, and re-ranking architectures, and provides a standardized data format and code examples for easy adaptation of new models.''',\n",
    "    'BBH': '''The BBH (BIG-Bench Hard) benchmark serves several key purposes and possesses defining features that set it apart from other evaluation suites: 1. Main Goals: - Challenging Evaluation Suite: The primary goal of BBH is to provide a challenging evaluation suite for language models. It focuses on tasks that are beyond the capabilities of current language models. - Assessment of Model Capabilities: BBH aims to measure the capabilities and limitations of large language models. It seeks to identify tasks for which the highest reported model performances are below the average human-rater score, indicating areas where language models fall short of human performance. - Understanding Model Performance: The benchmark aims to understand the performance of language models on diverse and challenging tasks, particularly those requiring multi-step reasoning. It seeks to explore whether traditional few-shot prompting setups underestimate the true capabilities of language models and whether alternative prompting techniques, such as chain-of-thought (CoT) prompting, can better capture the capabilities of these models. 2. Defining Features: - Diverse and Challenging Tasks: BBH is characterized by its diverse set of over 200 text-based tasks, spanning categories such as traditional NLP, mathematics, commonsense reasoning, and question-answering. The subset of 23 particularly challenging tasks in BBH presents a significant challenge for language models, as evidenced by their performance falling below the average human-rater score. - Human-Rater Baselines: The benchmark incorporates human-rater baselines to establish the empirical difficulty of tasks. While the human-rater scores may not be fully representative of the entire population, they provide a reference point for understanding the difficulty of tasks and whether they are likely to be challenging for language models. - Prompting Techniques: BBH explores the effectiveness of different prompting techniques, particularly the use of CoT prompting, to unlock the emergent task capabilities in language models at sufficiently large scales. It highlights the importance of prompt design in capturing the full potential of these models. 3. Specific Aspects of Machine Learning Models Tested and Evaluated: - Model Capacity and Training Data: BBH evaluates the performance of language models in relation to their capacity and the amount of training data. Empirical studies have shown that model performance improves with increasing model capacity and training data, and BBH aims to understand the implications of these factors on model capabilities. - Prompting Techniques: The benchmark specifically tests the impact of different prompting techniques, such as CoT prompting, on model performance. It explores whether traditional few-shot prompting setups underestimate model capabilities and whether CoT prompting can enable language models to surpass human-rater performance on challenging tasks. - Multi-Step Reasoning: BBH focuses on tasks that require multi-step reasoning, aiming to assess the ability of language models to perform complex reasoning and problem-solving. This aspect of the benchmark highlights the need for language models to demonstrate advanced cognitive abilities beyond simple pattern recognition.''',\n",
    "    'Big-Bench': '''The Big-Bench benchmark serves several key purposes and possesses distinctive features that set it apart from other benchmarks: 1. Main Goals: - The primary goal of the Big-Bench benchmark is to evaluate the capabilities and limitations of existing language model architectures across a wide range of diverse and challenging tasks. - It aims to provide a comprehensive assessment of language models' performance on more than 200 extremely diverse and difficult tasks, thereby quantifying the behavior of language models. - The benchmark also seeks to analyze how the performance of language models changes as a function of model scale, spanning from millions to hundreds of billions of parameters. - Additionally, the benchmark aims to establish a baseline for human performance across the benchmark tasks. 2. Defining Features: - Open Development: The benchmark was developed openly on GitHub, with contributors adding tasks through GitHub pull requests. Peer review of proposed tasks was performed through discussions on the pull request. - Incentivized Contributions: Authors of accepted tasks were given the opportunity to be co-authors on the paper introducing Big-Bench, incentivizing contributions to the benchmark. - Big-Bench Lite: A curated subset of 24 tasks, known as Big-Bench Lite (BBL), was introduced for lightweight evaluation. BBL consists exclusively of JSON tasks and was selected to address the computational expense of full evaluation. - Living Benchmark: The paper acknowledges that Big-Bench is a living benchmark, with plans to continue including tasks targeting broader capabilities and welcoming discussions of the benchmark's limitations. 3. Specific Aspects of Machine Learning Models Tested and Evaluated: - The benchmark evaluates dense and sparse transformer models from Google and OpenAI across six orders of magnitude of model scale. - It focuses on analyzing how specific model capabilities evolve with scale, rather than just the performance of any single model. - Big-Bench aims to support arbitrary programmatic tasks, including those involving many sequential model calls, and tasks that can be difficult to adapt to some evaluation pipelines. - The benchmark also addresses the limitations and gaps in coverage, aiming to prevent development from focusing on particular areas and neglecting others.''',\n",
    "    'BLiMP': '''The BLiMP benchmark has several main goals, including: 1. To provide a linguistically-motivated benchmark for assessing the sensitivity of language models to acceptability contrasts across a wide range of English phenomena, covering both previously-studied and novel contrasts. 2. To evaluate the linguistic knowledge of state-of-the-art language models on major grammatical phenomena in English. 3. To compare the performance of different language models in a fine-grained way, providing indirect evidence about each model's linguistic knowledge. The defining features of BLiMP include: 1. The use of minimal pairs of sentences that contrast in grammatical acceptability and isolate specific phenomena in syntax, morphology, or semantics. 2. The generation of data according to linguist-crafted grammar templates, ensuring that the dataset covers a wide range of linguistic phenomena. 3. The inclusion of 67 individual datasets, each containing 1,000 minimal pairs, organized by phenomenon into 12 categories. BLiMP aims to test and evaluate specific aspects of machine learning models, including: 1. The ability of language models to identify morphological contrasts related to agreement reliably. 2. The ability of language models to handle subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands. 3. The extent to which language models acquire robust knowledge of various linguistic phenomena, including ellipsis, control/raising, argument structure, and the semantic properties of quantifiers. 4. The comparison of different language models in terms of their sensitivity to various linguistic contrasts, providing insights into the strengths and weaknesses of each model.''',\n",
    "    'BOLD': '''The BOLD benchmark introduced in the paper aims to systematically study and benchmark social biases in open-ended language generation. It consists of a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. The dataset was extracted from Wikipedia sentences, which provides articles from many domains and demographics, represents authors from diverse backgrounds, and contains a quality control procedure. The BOLD benchmark also proposes new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. These metrics directly measure social biases in the generated texts, rather than measuring the probability that an LM chooses a stereotypical text over an unbiased text. The specific aspects of machine learning models that the BOLD benchmark aims to test and evaluate are their propensity to generate texts with negative connotations towards a particular group of people or an idea.''',\n",
    "    'BUSTM': '''The BUSTM dataset is one of the nine tasks included in the FewCLUE benchmark. Its main goal is to evaluate the ability of machine learning models to perform text classification on short texts, specifically on Chinese microblogs. The defining feature of the BUSTM dataset is that it is a few-shot learning task, meaning that it has a limited number of labeled examples available for training. In this case, the dataset includes only 16 or 32 labeled examples per class, making it a challenging task for machine learning models. The BUSTM dataset aims to test and evaluate the performance of few-shot learning methods, including fine-tuning, zero-shot learning, and five state-of-the-art few-shot learning methods. The experimental results show that the few-shot learning methods outperform the fine-tuning and zero-shot learning schemes, indicating the effectiveness of these methods in addressing the limited labeled data problem. In addition to evaluating the performance of different few-shot learning methods, the BUSTM dataset also includes an analysis of the impact of different factors on the performance of machine learning models, such as the number of labeled examples, the size of the pre-trained language model, and the use of data augmentation techniques.''',\n",
    "    'BiPaR': '''The BiPaR benchmark serves several key purposes and exhibits distinctive characteristics that set it apart from existing datasets: 1. Main Goals: - The primary goal of BiPaR is to provide a bilingual parallel novel-style machine reading comprehension (MRC) dataset that supports multilingual and cross-lingual reading comprehension. - It aims to address the lack of publicly available bilingual parallel reading comprehension datasets and to facilitate research in multilingual and cross-lingual MRC. - The dataset is designed to enable the evaluation of monolingual, multilingual, and cross-lingual MRC models on novels, thereby serving as a challenging testbed for assessing the performance of MRC systems. 2. Defining Features: - Bilingual Parallelism: Each triple (Passage, Question, Answer) in BiPaR is written parallelly in two languages (Chinese and English), making it a unique bilingual parallel dataset. - Novel-Based Content: The passages and questions in BiPaR are derived from novels, distinguishing it from existing datasets that primarily focus on news articles, encyclopedic texts, or other non-literary Platform. - Diversified Question Types: BiPaR offers good diversification in question prefixes, answer types, and relationships between questions and passages, thereby covering a broad range of answer types and reading comprehension skills. - Challenging Nature: Answering questions in BiPaR requires advanced reading comprehension skills such as coreference resolution, multi-sentence reasoning, and understanding of implicit causality, making it a challenging dataset for MRC. 3. Specific Aspects for Machine Learning Models: - Monolingual MRC: BiPaR provides a platform for evaluating monolingual MRC models, with experiments demonstrating that even strong baseline models are over 30 points behind human performance, indicating the challenging nature of the dataset. - Multilingual MRC: The bilingual nature of BiPaR supports multilingual and cross-lingual MRC tasks, allowing the training of a single multilingual MRC model to handle machine reading comprehension on multiple languages, thereby promoting cost-effectiveness and efficiency. - Cross-Lingual MRC: BiPaR enables the evaluation of cross-lingual MRC models, including tasks such as using questions in one language to find answers from passages written in another language, without relying on machine translation. In summary, the BiPaR benchmark stands out for its bilingual parallelism, novel-based content, diversified question types, and challenging nature.''',\n",
    "    'BigPatent': '''The BigPatent benchmark serves several key purposes and exhibits distinctive characteristics that set it apart from existing summarization datasets: 1. Main Goals: - Richer Discourse Structure: The benchmark aims to provide human-written abstractive summaries with a richer discourse structure compared to existing datasets. - Even Distribution of Salient Content: It emphasizes that salient content is evenly distributed in the input documents, setting it apart from other datasets where summary-worthy content often appears at the beginning of input articles. - Abstractive Summaries with High Compression Ratio: Unlike existing datasets where large segments from input articles are present verbatim in their respective summaries, BIGPATENT aims to facilitate the training and evaluation of systems capable of producing abstractive summaries with a high compression ratio. 2. Defining Features: - U.S. Patent Documents: The dataset consists of 1.3 million U.S. patent documents, offering a diverse and challenging set of documents for training and evaluating summarization models. - Human-Written Abstractive Summaries: The dataset contains human-written abstractive summaries, which are considerably more abstractive while reusing fewer and shorter phrases from the input. - Salient Content Distribution: The benchmark demonstrates a relatively uniform distribution of salient content from the summaries, a feature that distinguishes it from existing datasets. - Entity Distribution: The dataset provides insights into the distribution of entities in the generated summaries, highlighting the tendency of neural abstractive models to repeat entities more often than humans do. 3. Specific Aspects of Machine Learning Models: - Content Understanding: The benchmark aims to test and evaluate models' abilities to understand the global content structure of the input documents and generate well-formed informative summaries. - Entity Distribution Pattern: It seeks to assess models' understanding of entity roles and their importance, as well as their ability to engage in discourse-level text planning. - Abstractive Summarization: The dataset challenges models to produce abstractive and coherent summaries, addressing the limitations of existing models in generating summaries with high compression ratios and rich discourse structures.''',\n",
    "    'BioASQ': '''The BioASQ benchmark dataset is a significant replatform designed to address the challenges of biomedical question answering. Its main goals, defining features, and the specific aspects of machine learning models it aims to test and evaluate are detailed below: Main Goals: The primary goal of the BioASQ benchmark dataset is to provide a realistic and challenging replatform for the development and evaluation of biomedical question answering systems. It aims to reflect the real information needs of biomedical experts, making it more realistic and challenging than most existing datasets. Furthermore, the dataset includes ideal answers (effect summaries), which are particularly useful for research on multi-document summarization. Defining Features: 1. Realistic and Challenging: The dataset is designed to reflect the real information needs of biomedical experts, making it more realistic and challenging than most existing datasets. 2. Ideal Answers: Unlike most previous QA benchmarks that contain only exact answers, the BioASQ dataset also includes ideal answers (effect summaries), which are particularly useful for research on multi-document summarization. 3. Comprehensive Coverage: The dataset contains questions in English, along with golden standard (reference) answers and supporting material. 4. Expert Involvement: BioASQ employs a team of trained experts who provide a set of around 500 questions annually from their specialized field of expertise. Specific Aspects of Machine Learning Models: The BioASQ benchmark dataset aims to test and evaluate machine learning models in the following specific aspects: 1. Question Answering: The dataset provides a platform for evaluating the performance of machine learning models in accurately answering complex biomedical questions. 2. Multi-Document Summarization: The inclusion of ideal answers in the dataset allows for the evaluation of machine learning models in generating effective summaries from multiple documents. 3. Information Retrieval: The dataset supports the evaluation of machine learning models in retrieving relevant information from a large corpus of biomedical literature.''',\n",
    "    'BoolQ': '''The BoolQ benchmark is a reading comprehension dataset of naturally occurring yes/no questions. Its main goal is to test and evaluate the ability of machine learning models to perform complex, non-factoid inference tasks. The defining feature of BoolQ is that it contains questions that are generated in unprompted and unconstrained settings, making them more challenging than questions that are generated in a controlled environment. The questions in BoolQ require a wide range of inference abilities to solve, including the ability to detect contradictions and to reason about causality and temporal relationships. To evaluate the performance of machine learning models on BoolQ, the authors use a range of transfer learning baselines, including unsupervised pre-training in BERT, transfer from entailment data, and transfer from extractive QA data. They find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it continues to be beneficial even when starting from massive pre-trained language models such as BERT. The authors also propose a specific method for answering natural yes/no questions, which involves training BERT on MultiNLI and then re-training it on the BoolQ train set. This method achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a significant gap for future work.''',\n",
    "    'GLUE Diagnostics': '''The Diagnostic dataset in the GLUE benchmark is a manually-curated test set designed to evaluate the performance of machine learning models on a range of linguistic phenomena. The dataset is organized into four broad categories: Lexical Semantics, Predicate-Argument Structure, Logic, and Knowledge. Each category includes a number of specific phenomena, such as lexical entailment, core arguments, negation, and common sense knowledge. The goal of the Diagnostic dataset is to provide a more fine-grained evaluation of machine learning models than is possible with the main benchmark. While the main benchmark evaluates models on a range of tasks, the Diagnostic dataset is designed to test models on specific linguistic phenomena that are important for natural language understanding. By evaluating models on these specific phenomena, the dataset aims to provide insights into the strengths and weaknesses of different machine learning approaches to NLU. One defining feature of the Diagnostic dataset is that it includes human-annotated similarity scores for each sentence pair. This allows for a more nuanced evaluation of models, as they are not simply evaluated on whether they correctly classify a sentence pair as entailment, contradiction, or neutral, but also on how well they capture the similarity between the two sentences. Another important aspect of the Diagnostic dataset is that it includes examples from a variety of domains, including news, Reddit, Wikipedia, and academic papers. This helps to ensure that the dataset is diverse and representative of real-world language use.''',\n",
    "    'C-Eval': '''C-Eval serves as a comprehensive evaluation suite that aims to analyze important strengths and shortcomings of foundation models, foster their development and growth for Chinese users, and guide developers in understanding the abilities of their models from multiple dimensions. 1. Main Goals: - C-EVAL aims to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. - It seeks to provide a comprehensive evaluation suite to bridge the gap between Chinese LLM development and evaluation. - The benchmark is designed to help developers understand the abilities of their models from multiple dimensions and target the shortcomings of the models for improvement. 2. Defining Features: - C-EVAL comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. - It spans 52 diverse disciplines, ranging from humanities to science and engineering, providing a broad coverage of domains. - It includes C-EVAL HARD, a subset of particularly challenging subjects that demand highly advanced reasoning abilities to solve, such as advanced mathematics and college physics. - The benchmark aims to mitigate data leakage by collecting questions from mock exams as PDF or Microsoft Word documents, rather than exact questions from past national exams in China. 3. Specific Aspects of Machine Learning Models Tested and Evaluated: - C-EVAL evaluates the performance of the most advanced LLMs on tasks that require reasoning, knowledge retrieval, and complex problem-solving in a Chinese language environment. - It assesses the models' abilities to handle questions from diverse disciplines and difficulty levels, including sophisticated reasoning problems in the C-EVAL HARD subset. - The benchmark focuses on evaluating LLMs' accuracy, reasoning abilities, safety, bias, and robustness, transcending the scope of casual conversational bots and guiding developers in preparing LLMs to function in more complex scenarios.''',\n",
    "    'C-SEM': '''The C-SEM (Chinese SEMantic evaluation dataset) is a significant benchmark dataset under the FlagEval flagship project. Its primary aim is to address the challenges in evaluating large Chinese language models, focusing specifically on semantic understanding from a linguistic perspective. Main Goals of C-SEM: Enhanced Semantic Understanding: C-SEM is designed to probe the depth of semantic understanding in large language models. It does so by mimicking human language cognition habits and assessing how well these models can \"think\" and understand meanings. Multi-Level Evaluation: Unlike many existing benchmarks, C-SEM is structured to evaluate semantic understanding at various levels and complexities. This approach helps in identifying potential flaws and inadequacies in current models. Continuous Evolution: C-SEM is intended to evolve over time, adding more dimensions to semantic understanding and thereby forming a comprehensive multi-level evaluation system. Defining Features of C-SEM: Diverse Sub-Evaluation Items: C-SEM consists of different sub-evaluation items, each targeting a specific aspect of semantic understanding. These include Lexical Level Semantic Relationship Classification (LLSRC), Sentence Level Semantic Relationship Classification (SLSRC), Sentence Level Polysemous Words Classification (SLPWC), and Sentence Level Rhetoric Figure Classification (SLRFC). Open-Platform Accessibility: The current version, C-SEM v1.0, is open-platform, making it widely accessible for research and comparative studies. Integration with FlagEval: C-SEM is integrated into the FlagEval large model evaluation platform, ensuring that the evaluations are comprehensive and up-to-date. Specific Aspects C-SEM Aims to Test and Evaluate: Lexical Level Understanding (LLSRC): This evaluates a model's ability to discern semantic relationships between two words or phrases without context, such as \"synonymy,\" \"antonymy,\" etc. Contextual Semantic Understanding (SLSRC): Here, the focus is on whether the model can understand the semantics of words within a given context, a crucial aspect of natural language understanding. Polysemy Recognition (SLPWC): This tests the model's capability to comprehend one word with multiple meanings, depending on the context. This is a challenging aspect of language understanding as it requires nuanced cognition. Rhetorical Analysis (SLRFC): This part tests whether the model can identify and understand the use of rhetorical figures in sentences, such as metaphor, parallelism, personification, etc.''',\n",
    "    'C3': '''C3 benchmark is designed to provide a challenging dataset for machine reading comprehension, with the goal of evaluating model generalizability, assessing reading comprehension abilities, and studying the integration of prior knowledge into text comprehension tasks. 1. Main Goals: - The main goal of the C3 benchmark is to provide a free-form multiple-choice Chinese machine reading comprehension dataset that requires both the information in the given document and prior knowledge to answer questions. - It aims to assess model generalizability across different domains by including a dialogue-based task (C3D) and a mixed-genre task (C3M). - The benchmark is designed to evaluate the reading comprehension abilities of language learners of Chinese by collecting problems from real-world Chinese-as-a-second-language examinations. - Additionally, C3 aims to facilitate future cross-lingual machine reading comprehension studies by comparing its characteristics with its English counterparts. 2. Defining Features: - C3 contains a high percentage of questions requiring knowledge gained from the accompanying document as well as at least one type of prior knowledge. - It includes a dialogue-based task (C3D) and a mixed-genre task (C3M), which have different linguistic and general world knowledge requirements. - The dataset requires linguistic knowledge, domain-specific knowledge, and general world knowledge, which can be further broken down into eight types such as arithmetic, connotation, cause-effect, and implication. - C3D and C3M exhibit differences in the distribution of linguistic and general world knowledge requirements, providing a diverse set of challenges for machine reading comprehension models. 3. Aspects of Machine Learning Models Tested and Evaluated: - The C3 benchmark aims to test the ability of machine learning models to integrate prior knowledge with information presented in a text. - It evaluates the performance of state-of-the-art neural models in handling the challenges posed by the dataset, including the impacts of distractor plausibility and data augmentation. - The benchmark also aims to study how to leverage different types of prior knowledge for in-depth text comprehension and facilitate future work on cross-lingual and multilingual machine reading comprehension.''',\n",
    "    'CAIL': '''CAIL benchmark aims to provide a comprehensive and challenging dataset for legal judgment prediction, evaluate the performance of machine learning models in addressing the imbalance issue, and enable researchers to implement and evaluate text classification baselines for judgment prediction in real-world scenarios. 1. Goals of CAIL Benchmark: - Introduce the first large-scale Chinese legal dataset for judgment prediction, containing over 2.6 million criminal cases published by the Supreme People’s Court of China. - Provide a comprehensive dataset with detailed annotations of judgment results, including applicable law articles, charges, and prison terms, to facilitate research in the field of legal judgment prediction. - Address the imbalance issue in legal datasets and evaluate the performance of machine learning models in predicting low-frequency charges and law articles. - Enable researchers to implement and evaluate text classification baselines for judgment prediction on three sub-tasks: law articles, charges, and prison terms. 2. Defining Features of CAIL: - Large-scale: Contains over 2.6 million criminal cases, making it the largest publicly available legal judgment prediction dataset at the time of publication. - Detailed Annotations: Provides annotations for 183 criminal law articles and 202 charges, allowing for a rich and comprehensive evaluation of machine learning models. - Imbalance Challenge: Addresses the imbalance issue in legal datasets, making it challenging to predict low-frequency charges and law articles, thus providing a realistic and challenging benchmark for machine learning models. - Real-world Scenarios: Reserves more detailed annotations consistent with real-world scenarios, making it a valuable replatform for evaluating the performance of legal judgment prediction models. 3. Specific Aspects of Machine Learning Models Tested and Evaluated: - Text Classification Baselines: The benchmark evaluates several typical text classification baselines for judgment prediction, including TFIDF+ SVM, to assess their performance on predicting law articles, charges, and prison terms. - Imbalance Issue: The benchmark aims to test the ability of machine learning models to predict low-frequency charges and law articles, addressing the imbalance issue in legal datasets. - Real-world Performance: By providing a large-scale dataset with detailed annotations, the benchmark enables researchers to evaluate the real-world performance of machine learning models in legal judgment prediction.''',\n",
    "    'CArena': '''The CArena dataset in the SuperCLUE benchmark serves as a platform for user-model interactions and gathers user-reported ratings from a model battle platform called LangYa Leaderboard. The main goals of the CArena dataset are to provide a comprehensive understanding of user preferences for language models and to measure model performance in real-life scenarios based on user interactions. Key features of the CArena dataset include: 1. User-Model Interactions: It allows users to communicate with two anonymous models and then rate the models based on their interactions. 2. User-Reported Ratings: The dataset contains user-reported ratings, which serve as the gold standard for measuring model performance in real-life scenarios. 3. Large-Scale User Queries: The dataset analyzes large-scale users' queries and carefully annotates a subset of data with different model capabilities. Specific aspects of machine learning models that the CArena dataset aims to test and evaluate include: 1. User Preference: It aims to understand user preferences for language models by capturing their interactions and ratings. 2. Model Performance: It measures the performance of language models in real-life scenarios based on user-reported ratings and interactions. 3. Real-Life Applications: The dataset provides insights into how language models perform in real-world applications by capturing user interactions and preferences.''',\n",
    "    'CBBQ': '''The Chinese Bias Benchmark Dataset (CBBQ) is a collaborative effort between human experts and generative language models to measure societal biases in large language models (LLMs) within the context of Chinese culture and values. The main goals of CBBQ, as outlined in the paper, include: 1. Comprehensive Measurement of Societal Biases: CBBQ aims to holistically measure biases in LLMs across 14 social dimensions related to Chinese culture and values. This comprehensive approach allows for a thorough evaluation of biases prevalent in Chinese society. 2. Dataset Curation Process: The curation process of CBBQ involves four essential steps: bias identification through extensive literature review, ambiguous context generation, AI-assisted disambiguation, and manual review & recomposition. This meticulous process ensures the inclusion of diverse and relevant biases in the dataset. 3. Evaluation of Large Language Models: CBBQ is designed to evaluate the biases present in large language models, particularly in the context of Chinese culture. It aims to assess the interpretive capabilities and behavioral inconsistencies of LLMs, providing a comprehensive testbed for pre-deployment testing of Chinese or multilingual large language models. Defining Features of CBBQ: 1. Cultural Context: CBBQ is rooted in the Chinese social and cultural context, covering a wider range of bias categories and socially prevalent stereotypes in Chinese society. 2. Dataset Design and Evaluation Method: CBBQ features a revised dataset design and evaluation method tailored for LLMs bias evaluation. The design of disambiguated contexts and the bias score evaluation process have been adapted to better suit the evaluation of large language models. Specific Aspects of Machine Learning Models Tested and Evaluated: 1. Interpretive Capabilities: CBBQ aims to assess the interpretive capabilities of large language models, particularly in understanding and responding to biased or stereotypical content within the Chinese cultural context. 2. Behavioral Inconsistencies: The dataset aims to evaluate the behavioral inconsistencies of LLMs in relation to biases and stereotypes, providing insights into the model's responses to different societal contexts and values.''',\n",
    "    'CDIAL-BIAS': '''The DIAL-BIAS benchmark is a fine-grained dialog bias measurement benchmark that aims to evaluate the social bias risks of machine learning models in dialog scenarios. The benchmark is based on the CDIAL-BIAS dataset, which is the first well-annotated Chinese dataset for measuring social bias in dialogs. The benchmark has several defining features, including its focus on four widely-discussed bias topics (Race, Gender, Region, and Occupation), its use of four auxiliary labels collected through data crawling and sequential labeling procedures, and its consideration of context-sensitivity, data type, targeted group, and implicated attitude in the analysis of social bias in dialogs. The benchmark aims to test and evaluate the social bias risks of machine learning models in several ways. First, it evaluates the performance of off-the-shelf APIs in handling social bias issues contained in the CDIAL-BIAS dataset. Second, it tests the social bias risks of three representative dialog systems and one popular language model using both an automatic classifier and human evaluation. Third, it conducts in-depth analyses on the annotated dataset to shed light on the utility of detailed annotations in identifying social bias in dialogs. Overall, the DIAL-BIAS benchmark is a comprehensive and rigorous evaluation framework that can help researchers and practitioners build safer and more reliable dialog systems. By providing a fine-grained measurement of social bias in dialogs, the benchmark can contribute to the development of unbiased dialog systems and the mitigation of social bias in AI.''',\n",
    "    'CFM': '''The CFM dataset is a part of the LAiW benchmark and aims to test and evaluate the capability of LLMs to extract the logical and interactive arguments between the defense and prosecution in legal documents. The defining feature of the CFM dataset is its focus on controversial focus mining, which is a key component for tasks that relate to the case result. The dataset includes a set of legal documents that contain arguments from both the defense and prosecution, and the goal is to extract the key arguments and analyze their logical and interactive relationships. In terms of machine learning models, the CFM dataset evaluates the performance of LLMs on a range of NLP tasks, including named entity recognition, argument extraction, and argument relationship analysis. The dataset also includes a variety of evaluation metrics, such as accuracy, F1 score, and precision-recall curves, to provide a comprehensive evaluation of LLM performance.''',\n",
    "    'ChID': '''The ChID benchmark is a large-scale Chinese cloze test dataset that aims to evaluate the ability of machine learning models to comprehend Chinese idioms. The dataset contains 581K passages and 729K queries from news, novels, and essays, covering 3,848 Chinese idioms. The idioms in each passage are replaced by blank symbols, and the correct answer needs to be chosen from a list of candidate idioms. The defining feature of ChID is its focus on Chinese idioms, which are a unique yet common language phenomenon in Chinese. The authors note that the main challenges for machine reading comprehension with idioms lie in idiom representation and thorough discrimination among the near-synonyms of an idiom. The specific aspects of machine learning models that ChID aims to test and evaluate include the design of candidate idioms and the representation of idioms. The authors conduct extensive experiments on these aspects and compare state-of-the-art models. They find that the performance of these models is substantially worse than that of humans, indicating a large space for further research.''',\n",
    "    'CIFAR-10': '''The CIFAR-10 benchmark aims to provide a standardized dataset for evaluating machine learning models in the context of object recognition. 1. Main Goals: The primary goal of the CIFAR-10 benchmark is to facilitate the evaluation of machine learning models for object recognition tasks. By providing a dataset of 60,000 32x32 color images in 10 different classes, the benchmark aims to assess the performance of various models in accurately classifying objects within these categories. Additionally, the benchmark seeks to enable researchers to compare and contrast the effectiveness of different machine learning approaches in handling real-world image recognition challenges. 2. Defining Features: - The CIFAR-10 dataset consists of 60,000 images, with 6,000 images per class, making it a balanced dataset suitable for training and testing machine learning models. - Each image is of size 32x32 pixels, which presents a significant challenge for object recognition due to the low resolution and limited visual information available in each image. - The dataset encompasses 10 distinct classes, including common objects such as airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships, and trucks, representing a diverse set of visual categories. 3. Specific Aspects of Machine Learning Models: - The benchmark aims to test and evaluate the performance of machine learning models in classifying images into the 10 predefined classes. - It provides a platform for comparing the effectiveness of different input representations, such as raw pixels (whitened and unwhitened) and features learned by RBMs, in training machine learning models for object recognition. - The paper discusses the use of multinomial logistic regression and neural networks with different initialization strategies, emphasizing the impact of feature extraction and initialization methods on the models' performance. - The benchmark also highlights the importance of feature extraction and high-level feature representation in improving the classification accuracy of machine learning models, as evidenced by the superior performance of RBM features over raw pixels in the classification task.''',\n",
    "    'CJRC': '''The main goal of the CJRC benchmark is to provide a reliable human-annotated dataset for Chinese Judicial Reading Comprehension. The dataset contains approximately 10K documents and almost 50K questions with answers, and is designed to help researchers extract elements using reading comprehension technology, which is crucial in the legal field. The defining features of the CJRC dataset include its focus on Chinese judicial documents, its inclusion of both civil and criminal cases, and its coverage of various types of charges and causes of action. The dataset also includes three types of questions: span-extraction, YES/NO, and unanswerable questions, which are similar to those found in other reading comprehension datasets such as SQuAD2.0 and CoQA. In terms of machine learning models, the CJRC benchmark aims to test and evaluate models that can accurately answer questions based on the given documents. Specifically, the benchmark evaluates models based on their ability to perform span-extraction, YES/NO, and unanswerable question answering tasks. The benchmark also includes in-domain and out-of-domain test sets, which allows for the evaluation of models' ability to generalize to new domains.''',\n",
    "    'CLEVR': '''The CLEVR (Compositional Language and Elementary Visual Reasoning) benchmark is designed to address the need for diagnostic tests to analyze the progress and shortcomings of artificial intelligence systems in reasoning and answering questions about visual data. The main goals of CLEVR include enabling detailed analysis of visual reasoning, minimizing biases in visual question answering (VQA) datasets, and providing rich ground-truth representations for both images and questions. Key features of CLEVR include challenging images and questions that test visual reasoning abilities such as counting, comparing, logical reasoning, and storing information in memory. The dataset depicts simple 3D shapes to simplify recognition and focus on reasoning skills. Additionally, CLEVR's synthetic nature and detailed annotations facilitate in-depth analyses of reasoning abilities that are impossible with existing datasets. CLEVR aims to test and evaluate specific aspects of machine learning models, including their abilities in short-term memory tasks, compositional reasoning, and overcoming biases when conditions change. The benchmark reveals weaknesses in current state-of-the-art VQA models, particularly in tasks requiring short-term memory, such as comparing the attributes of objects, and compositional reasoning, such as recognizing novel attribute combinations.''',\n",
    "    'CLEVRER': '''The CLEVRER benchmark is a pioneering dataset designed to evaluate the ability of computational models to understand and reason about temporal and causal events depicted in videos. Its main goals include assessing complex logical reasoning on videos, providing effective diagnostics for models on these tasks, and complementing existing visual reasoning benchmarks. The dataset introduces a wide range of reasoning tasks, including descriptive, explanatory, predictive, and counterfactual questions, and is carefully designed in a fully-controlled synthetic environment to enable effective evaluation of computational models. Here are the key aspects of the CLEVRER benchmark: 1. Dataset Design: CLEVRER includes 20,000 synthetic videos of colliding objects, each lasting for 5 seconds. The videos are generated by a physics engine that simulates object motion and a graphs engine that renders the frames. The dataset is associated with ground-truth motion traces and event histories of each object in the videos. 2. Reasoning Tasks: The dataset includes four specific elements of complex logical reasoning on videos: descriptive (e.g., 'what color'), explanatory ('what’s responsible for'), predictive ('what will happen next'), and counterfactual ('what if'). Each question is paired with a functional program representing its underlying logic. 3. Question Types: CLEVRER includes machine-generated questions for descriptive, explanatory, predictive, and counterfactual reasoning. Descriptive questions evaluate a model’s capability to understand and reason about a video’s dynamical content and temporal relation. Explanatory questions query the causal structure of a video, predictive questions focus on predicting future events, and counterfactual questions explore alternative scenarios. 4. Annotations: The dataset provides ground-truth motion traces and event histories of each object in the videos, enabling effective evaluation of computational models on complex reasoning tasks. 5. Model Evaluation: The benchmark evaluates a wide range of baseline models for video reasoning, including language-only models, models for video question answering, and models for compositional visual reasoning. The performance of these models is analyzed in terms of their ability to handle descriptive, explanatory, predictive, and counterfactual questions. 6. Challenges: The benchmark highlights the challenges faced by state-of-the-art visual reasoning models, particularly in performing causal reasoning tasks such as explanatory, predictive, and counterfactual questions. Overall, the CLEVRER benchmark aims to provide a systematic evaluation of computational models on a wide range of reasoning tasks related to temporal and causal events in videos, and to identify the key elements essential to these tasks, such as object recognition, modeling dynamics and causal relations, and understanding symbolic logic.''',\n",
    "    'CLOSE Set': '''The CLOSE Set is a sub-task of the SuperCLUE benchmark, which aims to evaluate the performance of large language models (LLMs) in a Chinese context. The CLOSE Set is designed to test the ability of LLMs to answer closed-ended questions, which are questions that have a limited set of possible answers. The main goal of the CLOSE Set is to analyze whether the format of multiple-choice questions is a suitable alternative to open-ended questions in evaluating model performance. The dataset is constructed based on the Open set, which contains single- and multi-turn open-ended questions. The CLOSE Set targets the same questions as the Open set, but with the same stems as open-ended single-turn ones. The defining feature of the CLOSE Set is that it consists of closed-ended questions, which are designed to have a limited set of possible answers. This is in contrast to the Open set, which contains open-ended questions that allow for a wider range of possible answers. The CLOSE Set is intended to be a complementary evaluation method to the Open set, providing a different perspective on model performance. The specific aspects of machine learning models that the CLOSE Set aims to test and evaluate include their ability to understand and interpret natural language, their ability to reason and make inferences based on limited information, and their ability to select the correct answer from a limited set of options.''',\n",
    "    'CLUE diagnostics': '''The diagnostics dataset in the CLUE benchmark is a handcrafted dataset designed to evaluate the ability of machine learning models to handle specific linguistic and logical phenomena in the Chinese language. The main goals, defining features, and specific aspects of machine learning models that it aims to test and evaluate are as follows: Main Goals: The primary goal of the diagnostics dataset is to assess the performance of machine learning models in handling specific linguistic and logical phenomena in the Chinese language. This involves evaluating the models' ability to comprehend and process language in the context of unique linguistic properties and logical reasoning. Defining Features: The diagnostics dataset is characterized by the following defining features: - Focus on Linguistic and Logical Phenomena: The dataset specifically targets unique linguistic and logical phenomena in the Chinese language, such as aspect markers, anaphora resolution, and logical reasoning. - Handcrafted Annotations: The dataset is annotated by Chinese linguists, ensuring high-quality and accurate annotations that reflect the nuances of the Chinese language. - Diagnostic Evaluation: The dataset is designed to provide a diagnostic evaluation of machine learning models, identifying specific areas of strength and weakness in their language understanding capabilities. Specific Aspects of Machine Learning Models: The diagnostics dataset evaluates machine learning models based on specific aspects related to linguistic and logical phenomena, including: - Aspect Markers: Models are tested on their ability to handle aspect markers, which are unique to the Chinese language and indicate the aspectual properties of verbs. - Anaphora Resolution: The dataset assesses models' capability to resolve anaphora, which involves identifying the referent of a pronoun or noun phrase in a sentence. - Logical Reasoning: The dataset may also involve testing models on logical reasoning tasks, such as syllogisms and deductive reasoning, to evaluate their ability to handle complex logical structures. By focusing on unique linguistic and logical phenomena in the Chinese language and evaluating models based on aspect markers, anaphora resolution, and logical reasoning, the diagnostics dataset provides a comprehensive assessment of machine learning models' language understanding capabilities in specific areas.''',\n",
    "    'CLUEWSC': '''The WSC (Winograd Schema Challenge) dataset in the FewCLUE benchmark is designed to evaluate the ability of machine learning models to understand and reason about pronouns and their referents in Chinese. The main goal of the WSC dataset is to assess the models' capabilities in resolving coreference ambiguities and understanding contextual information to make accurate predictions. The defining feature of the WSC dataset is its focus on pronoun resolution and coreference resolution, which are crucial tasks in natural language understanding. The dataset consists of a collection of Winograd Schema-style questions in Chinese, where each question presents a scenario involving a pronoun and requires the model to determine the referent of the pronoun based on the context provided. Specifically, the WSC dataset aims to test and evaluate the performance of machine learning models in resolving complex linguistic ambiguities and understanding contextual dependencies. This includes assessing the models' ability to capture subtle linguistic cues and make accurate predictions based on the contextual information presented in the questions. The WSC dataset provides a challenging evaluation task for machine learning models, particularly in the few-shot learning setting, as it requires the models to generalize from a limited number of labeled examples. By evaluating the performance of different few-shot learning methods, the dataset offers insights into the effectiveness of these methods in addressing coreference resolution and pronoun disambiguation tasks in Chinese natural language understanding.''',\n",
    "    'CMMLU': '''The CMMLU benchmark is a comprehensive Chinese assessment suite designed to evaluate the advanced knowledge and reasoning abilities of large language models (LLMs) within the Chinese language and cultural context. Its main goals are to assess the multi-task language understanding capabilities of LLMs and to identify factors impacting their performance. The defining features of CMMLU include its coverage of various subjects, including natural science, social sciences, engineering, and humanities, and its use of multiple-choice questions to facilitate evaluation. CMMLU also incorporates a Sinicization degree to ensure that the questions are culturally relevant and accessible to Chinese speakers. CMMLU aims to test and evaluate specific aspects of machine learning models, including their ability to understand and reason with natural language, their knowledge of specific subject areas, and their ability to perform multi-task learning. The benchmark includes tasks such as reading comprehension, summarization, and common-sense reasoning, as well as more specialized tasks such as mathematical reasoning and code generation.''',\n",
    "    'CMNLI': '''The CMNLI (Chinese Multi-Genre NLI) dataset in the CLUE benchmark is a crucial component designed to evaluate the ability of machine learning models to understand and process natural language inference (NLI) in the context of diverse genres in the Chinese language. The main goals, defining features, and specific aspects of machine learning models that it aims to test and evaluate are as follows: Main Goals: The primary goal of the CMNLI dataset is to assess the performance of machine learning models in understanding natural language inference across multiple genres in the Chinese language. This involves evaluating the models' ability to comprehend and reason about the relationships between pairs of sentences in various contexts and genres. Defining Features: The CMNLI dataset is characterized by the following defining features: - Multi-Genre NLI: The dataset covers natural language inference tasks across diverse genres, including news, government documents, fiction, TV transcripts, and telephone transcripts, reflecting real-world applications of NLI in different contexts. - Chinese Language Understanding: The dataset is designed to assess the language understanding capabilities of machine learning models in the context of Chinese natural language processing, specifically in the domain of natural language inference. - Annotator Agreement: The dataset may involve high inter-annotator agreement, ensuring the quality and reliability of the annotations across different genres and contexts. Specific Aspects of Machine Learning Models: The CMNLI dataset evaluates machine learning models based on specific aspects related to natural language inference, including: - Cross-Genre Generalization: Models are tested on their capacity to generalize across diverse genres and contexts, reflecting their ability to understand and reason about sentence relationships in different domains. - Contextual Understanding: The dataset assesses models' capability to comprehend and infer relationships between sentences in specific contexts, such as news articles, government documents, fiction, and transcribed conversations. - Inference and Entailment: Models are evaluated based on their ability to determine entailment, contradiction, or neutrality between pairs of sentences, reflecting their capacity to perform fine-grained inference tasks.''',\n",
    "    'CMRC (2018)': '''The CMRC2018 benchmark is a Span-Extraction dataset for Chinese machine reading comprehension. Its main goals are to provide a dataset for evaluating machine reading comprehension models in the context of the Chinese language, and to encourage research and development in this area. The dataset is designed to test and evaluate the performance of machine learning models in understanding and answering questions based on Chinese text passages. Defining Features: 1. Language Diversity: The dataset aims to bring language diversity to the machine reading comprehension task by providing a comprehensive set of questions and passages in the Chinese language. 2. Human Annotation: The questions in the dataset are completely annotated by human experts, ensuring high-quality evaluation data. 3. Challenge Set: The dataset includes a challenging set of questions that require comprehensive understanding and multi-sentence inference throughout the context, providing a more rigorous evaluation of machine comprehension models. Specific Aspects of Machine Learning Models: The CMRC2018 benchmark aims to test and evaluate machine learning models in the following specific aspects: 1. Span-Extraction Reading Comprehension: The dataset focuses on the span-extraction reading comprehension task, where the answer is a span directly extracted from the passage. This task requires models to predict the start and end pointers in the passage to form the answer. 2. Multi-sentence Inference: The challenging set of questions in the dataset requires models to perform multi-sentence inference and comprehensive reasoning over multiple clues to provide accurate answers. 3. Language Transfer Learning: The dataset could also be used for cross-lingual research purposes when studied along with SQuAD and other similar datasets, allowing for evaluation of models' performance in cross-lingual settings.''',\n",
    "    'CNN/DM': '''The CNN/Daily Mail dataset is a benchmark dataset for text summarization tasks, specifically for abstractive summarization. The dataset consists of news articles from CNN and Daily Mail, paired with multi-sentence summaries. The goal of the dataset is to evaluate the ability of machine learning models to generate accurate and informative summaries that capture the essence of the original article. The defining feature of the dataset is its length, with articles averaging 781 tokens and summaries averaging 56 tokens. This makes it a challenging task for machine learning models to generate summaries that are both concise and informative. Additionally, the dataset includes a mix of news topics, including politics, sports, and entertainment, which adds to the complexity of the task. The specific aspects of machine learning models that the dataset aims to test and evaluate include their ability to accurately reproduce factual details, deal with out-of-vocabulary words, and avoid repetition.''',\n",
    "    'COCO-Text': '''The COCO-Text benchmark is a large-scale dataset and evaluation framework designed to advance the state-of-the-art in text detection and recognition in natural images. Its main goals are to provide the research community with a replatform to improve existing methods and to evaluate the shortcomings of current approaches. The defining features of the COCO-Text dataset include its size, diversity, and contextual information. It contains over 63,000 images with more than 173,000 labeled text regions, making it more than 14 times larger than the latest ICDAR 15 dataset. The text in the images is diverse, ranging from machine printed to handwritten, and includes a variety of scripts. Additionally, the dataset includes contextual information, as it is part of the larger context of the MS COCO dataset, which annotates common objects in their natural contexts. The COCO-Text benchmark evaluates the performance of machine learning models on several specific aspects of text detection and recognition. These include text localization, classification into machine printed and handwritten text, classification into legible and illegible text, script classification, and transcription of legible text with western script. The evaluation procedure follows the text detection and recognition evaluation scheme used in the ICDAR robust reading competition for end-to-end recognition of incidental scene text.''',\n",
    "    'COLD': '''The COLD benchmark for Chinese offensive language detection has several key goals, including: 1. Providing a reliable dataset for Chinese offensive language analysis: The COLDATASET contains 37,480 comments with binary offensive labels and covers diverse topics of race, gender, and region. It is designed to be a comprehensive and representative dataset for Chinese offensive language analysis. 2. Developing a baseline detector for Chinese offensive language: The COLDETECTOR is a machine learning model trained on the COLDATASET that can effectively detect offensive content in Chinese. 3. Evaluating the offensiveness of Chinese generative language models: The COLD benchmark evaluates the offensiveness of popular Chinese generative language models and investigates what types of prompts can trigger offensive generation. The defining features of the COLD benchmark include: 1. Comprehensive coverage of offensive language: The COLDATASET covers diverse topics of race, gender, and region, providing a comprehensive and representative dataset for Chinese offensive language analysis. 2. Fine-grained annotation: The test set of the COLDATASET is annotated at a fine-grained level with four categories: attacking individuals, attacking groups, attacking events, and attacking language. 3. Evaluation of generative language models: The COLD benchmark evaluates the offensiveness of popular Chinese generative language models and investigates what types of prompts can trigger offensive generation. The specific aspects of machine learning models that the COLD benchmark aims to test and evaluate include: 1. Offensive language detection: The COLDETECTOR is a machine learning model trained on the COLDATASET that can effectively detect offensive content in Chinese. 2. Offensiveness of generative language models: The COLD benchmark evaluates the offensiveness of popular Chinese generative language models and investigates what types of prompts can trigger offensive generation. 3. Bias in pre-training data: Large-scale language models often learn biases in pre-training data and generate offensive or unethical contents, which substantially hinders the deployment of models in practice. The COLD benchmark aims to address this issue by providing a reliable dataset for Chinese offensive language analysis.''',\n",
    "    'COPA': '''The COPA (Choice Of Plausible Alternatives) benchmark is a comprehensive evaluation metric designed to assess commonsense causal reasoning. Its main goals include providing a standardized test for evaluating the performance of machine learning models in understanding and reasoning about causal relationships in natural language. The benchmark aims to measure the ability of models to make plausible inferences about cause-and-effect relationships based on given premises and alternatives. Defining Features: 1. Forced-Choice Format: Each COPA question presents a premise followed by two plausible causes or effects, requiring the model to select the alternative that is more plausible than the other. This format emphasizes the model's ability to make reasoned choices between competing causal explanations. 2. Breadth and Clarity: The question set was carefully designed to cover a wide range of topics and scenarios, ensuring that the evaluation captures diverse aspects of commonsense causal reasoning. 3. Authoring Methodology: The development of the question set involved a specific authoring methodology to ensure clarity of language, high agreement among human raters, and a balanced representation of question topics. Specific Aspects of Machine Learning Models Tested and Evaluated: 1. Causal Reasoning: The benchmark assesses the model's capability to understand and reason about causal relationships in natural language, including identifying causes and effects based on given premises. 2. Commonsense Knowledge: It aims to test the model's ability to leverage commonsense knowledge to make plausible inferences about causal connections in everyday scenarios. 3. Natural Language Understanding: The benchmark evaluates the model's proficiency in comprehending and interpreting natural language premises to make informed choices between alternative causes or effects.''',\n",
    "    'COPEN': '''The COPEN benchmark is a probing benchmark designed to evaluate the conceptual knowledge of pre-trained language models (PLMs). The main goal of COPEN is to assess whether PLMs can organize entities by conceptual similarities, learn conceptual properties, and conceptualize entities in contexts. The defining feature of COPEN is its focus on conceptual knowledge, which is fundamental to human cognition and knowledge bases. Unlike existing knowledge probing works that only evaluate factual knowledge of PLMs, COPEN aims to evaluate conceptual knowledge, which often appears as implicit commonsense behind texts. COPEN consists of three probing tasks: (1) Conceptual Similarity (CS), (2) Conceptual Property (CP), and (3) Conceptualization in Context (CiC). The CS task requires PLMs to identify the most similar concept to a given entity based on their conceptual similarities. The CP task requires PLMs to identify the conceptual properties of a given entity. The CiC task requires PLMs to conceptualize entities in contexts and choose the most appropriate concept in a concept taxonomy according to its context. To construct COPEN, the authors curated a concept taxonomy with 446 concepts based on DBpedia and Wikidata and collected and annotated 24k data instances covering 393 concepts.''',\n",
    "    'CORGI-PM': '''The CORGI-PM benchmark aims to address the challenges of gender bias in natural language processing (NLP) by providing a high-quality Chinese human-annotated corpus for contextual-level gender bias probing and mitigation. The benchmark has several defining features and specific aspects related to machine learning models that it aims to test and evaluate. Main Goals: 1. Address Data Inadequacy: The benchmark aims to tackle the issue of data inadequacy and biased corpus, especially for languages with insufficient rePlatform, such as Chinese, by providing a high-quality Chinese human-annotated corpus for gender bias probing and mitigation. 2. Probing and Mitigation: The benchmark focuses on both probing and mitigation of gender bias in natural language text. It aims to provide a dataset for detecting, classifying, and mitigating textual gender bias, thereby contributing to the development of automatic methods for identifying and preventing toxic gender attitudes and stereotypes in textual format. Defining Features: 1. High-Quality Annotations: The benchmark features 32.9k human-annotated sentences, including both gender-biased and non-biased samples, with high-quality labels derived from an annotation scheme specifically developed for gender bias in the Chinese context. 2. Automatic Data Collection: The benchmark proposes an automatic method for building a potentially gender-biased sentence set from existing large-scale Chinese corpora. This method leverages language models for gender bias score calculation and carefully selects and labels the retrieved sentences to ensure the quality of the corpus. 3. Balanced Labels: The benchmark provides balanced labels for gender-biased and non-biased samples, ensuring that the dataset is suitable for evaluating gender bias detection, classification, and mitigation tasks in NLP. Specific Aspects for Machine Learning Models: 1. Gender Bias Detection: The benchmark aims to test the ability of machine learning models to detect gender bias in textual data. It provides clear definitions and evaluation protocols for NLP tasks related to gender bias probing, allowing for the evaluation of models' performance in identifying gender-biased content. 2. Gender Bias Classification: In addition to detection, the benchmark also focuses on the classification of gender bias in textual data. It provides clear definitions and evaluation protocols for NLP tasks related to gender bias classification, enabling the assessment of models' performance in categorizing different types of gender bias. 3. Gender Bias Mitigation: The benchmark addresses the challenge of gender bias mitigation as a natural language generation task, where the model is asked to generate corrected versions of biased sentences with human-annotated ones as references. It provides baselines for testing the performance of state-of-the-art language models in mitigating textual gender bias, including experiments with zero-shot and fine-tuning paradigms.''',\n",
    "    'CQADupStack': '''The CQADupStack benchmark dataset, as described in the paper, serves several key purposes and possesses defining features that make it a valuable replatform for evaluating machine learning models in the context of community question-answering (cQA) research. Main Goals: 1. Standardization: The dataset aims to address the lack of a publicly available benchmark dataset for identifying duplicate questions in cQA data. By providing a standardized dataset, it enables researchers to compare and evaluate different methods effectively. 2. Research Catalyst: The release of the dataset is intended to stimulate research on duplicate question detection systems, encouraging the development of new methods and the analysis of existing ones. 3. Realistic Representation: The dataset is designed to reflect the imbalance in questions with or without duplicates, mirroring real-world scenarios and presenting an important problem for researchers to address. Defining Features: 1. Annotated Threads: The dataset contains threads from twelve StackExchange subforums, annotated with information about duplicate questions. 2. Pre-defined Splits: It provides pre-defined training and test splits for retrieval and classification experiments, ensuring maximum comparability between different studies using the set. 3. Script for Data Manipulation: The dataset comes with a script to manipulate the data in various ways, offering flexibility for researchers to tailor the data to their specific needs. Specific Aspects for Machine Learning Models: 1. Duplicate Question Retrieval: The dataset facilitates benchmarking of machine learning models for the task of identifying duplicate questions in cQA data. This includes evaluating retrieval models using well-established methods such as TF-IDF, BM25, and maximum likelihood language model. 2. Evaluation Metrics: The dataset addresses the limitation of current retrieval evaluation metrics when handling queries for which there are no relevant results in the set, presenting an area for future work in the evaluation of machine learning models. 3. Challenging Set: The dataset is described as challenging due to the high imbalance in questions with or without duplicates, providing a realistic and important problem for researchers to tackle.''',\n",
    "    'CRASS': '''The CRASS benchmark is a novel tool designed to evaluate the counterfactual reasoning abilities of large language models (LLMs). Its main goals are to challenge current LLMs and guide their development by measuring progress beyond well-established benchmarks like SuperGLUE. The defining feature of the CRASS benchmark is its use of questionized counterfactual conditionals (QCCs) to evaluate LLMs. QCCs are if-then clauses consisting of an antecedent and consequence that relate two events. Each QCC in the CRASS data set presents a hypothetical scenario and asks the LLM to judge which of three possible answers is most suitable. The CRASS data set consists of 274 high-quality QCCs, each with three possible answers including the correct answer derived from the original xEffect of ATOMIC dataset. The benchmark supports scoring against a crowd-validated human baseline. The specific aspects of machine learning models that the CRASS benchmark aims to test and evaluate include their ability to understand the underlying real-world mechanisms at play, their grasp of reality, and their ability to evaluate the likely consequences of two possible alternative situations.''',\n",
    "    'CSL': '''The CSL benchmark serves as a comprehensive evaluation framework for assessing the performance of machine learning models across various scientific domain tasks. Its main goals include providing a standardized platform for testing models on Chinese scientific literature, identifying the challenges specific to Chinese scientific NLP, and facilitating the development of models tailored to this domain. Key features of the CSL benchmark include: 1. Diverse Tasks: The benchmark encompasses a range of tasks, including text classification, summarization, and keyword generation, representing real-world scenarios of automatic analysis of scientific literature. 2. Multi-Task Training: The benchmark supports multi-task training and evaluation, allowing models to learn and perform across different tasks simultaneously. 3. Large-Scale Dataset: The benchmark is based on the CSL dataset, which contains a substantial amount of scientific papers, enabling thorough evaluation and analysis. Specific aspects of machine learning models that the CSL benchmark aims to test and evaluate include: 1. Performance on Scientific NLP Tasks: The benchmark assesses the performance of models on tasks such as text classification for predicting categories and disciplines based on paper information, as well as summarization and keyword generation. 2. Generalization to Chinese Scientific Literature: It evaluates the ability of models to generalize to Chinese scientific literature, highlighting the challenges and opportunities specific to this domain. 3. Multi-Task Learning: The benchmark explores the effectiveness of multi-task learning, examining how models can share knowledge across different tasks and tasks' input and output combinations.''',\n",
    "    'SNLI (zh)': '''The CSNLI dataset is a Chinese Natural Language Inference dataset that was constructed in this paper to address the problem of limited scale of open NLI datasets for Chinese. The main goal of the dataset is to provide a large-scale dataset for NLI and semantic similarity calculation tasks in the Chinese language. The dataset contains 2,191,500 sentence pairs and their corresponding relations, making it the largest open NLI dataset in Chinese among the academic community. The sentence pairs were derived from four English datasets, including SNLI, MNLI, QQP, and STS-B, which cover various aspects of NLI and semantic similarity calculation. To ensure accuracy, the paper adopted machine translation and manual proofreading to translate the English datasets into Chinese. The resulting dataset is suitable for training and testing deep neural network models like BERT, which is applied to the NLP task based on this dataset in the paper. The defining features of the CSNLI dataset include its large scale, its coverage of various aspects of NLI and semantic similarity calculation, and its suitability for training and testing deep neural network models.''',\n",
    "    'CSP': '''The Common Syntactic Processes (CSP) benchmark is a systematic evaluation of the ability of machine learning models to capture eight grammatical phenomena that are pervasive in human language and central to much discussion in the field of linguistics. The benchmark aims to provide a standardized and comprehensive evaluation of the performance of machine learning models in understanding syntax and semantics. The eight grammatical phenomena tested in the CSP benchmark are binding principles and coreference, passives, word order, coordination, comparatives, negation, ellipsis, and structural ambiguity. These phenomena are widely discussed in linguistics and are routinely mastered by young children, making them a suitable benchmark for evaluating the performance of machine learning models. The CSP benchmark is designed to test and evaluate the ability of machine learning models to capture the compositional nature of human language. Compositional understanding refers to the ability to understand the meaning of a sentence by combining the meanings of its individual parts. This is a fundamental property of human language and is essential for understanding the meaning of complex sentences. The CSP benchmark consists of a set of prompts that are designed to test the ability of machine learning models to capture the eight grammatical phenomena. The prompts are carefully constructed to ensure that they are unambiguous and that they test specific aspects of machine learning models. The performance of machine learning models is evaluated based on their ability to generate outputs that are consistent with the syntax and semantics of the prompts. Overall, the CSP benchmark is a comprehensive evaluation of the ability of machine learning models to capture the compositional nature of human language.''',\n",
    "    'CUAD': '''The CUAD benchmark is a new dataset for legal contract review that was created with the help of legal experts and consists of over 13,000 annotations. The main goal of the dataset is to highlight important portions of a contract for human review. The dataset includes more than 500 contracts and more than 13,000 expert annotations that span 41 label categories. For each of the 41 different labels, models must learn to highlight the portions of a contract most salient to that label. This makes the task a matter of finding needles in a haystack. The defining feature of the CUAD dataset is that it was created with the collective effort of many annotators. Prior to labeling, law student annotators of CUAD attended training sessions to learn how to label each of the 41 categories, which included video instructions by and live workshops with experienced lawyers, detailed instructions, and quizzes. This ensures that the annotations are of high quality and reflect the expertise of legal professionals. The specific aspects of machine learning models that the CUAD dataset aims to test and evaluate are their ability to perform well in highly specialized domains and their ability to transfer to real-world document analysis tasks.''',\n",
    "    'CUB': '''The Caltech-UCSD Birds-200-2011 dataset is a comprehensive collection of 11,788 images of 200 bird species, organized by scientific classification (order, family, genus, species). The dataset was created with the goal of facilitating research in subordinate categorization by providing a comprehensive set of benchmarks and annotation types for one particular domain (birds). The defining features of this dataset include annotations for bounding boxes, part locations, and attribute labels for each image. The dataset also includes a vocabulary of 28 attribute groupings and 312 binary attributes, as well as annotations for 15 parts of each bird. These annotations open the door for new research opportunities, but are also subject to a larger degree of annotation error and user subjectivity as compared to object class labels. The specific aspects of machine learning models that this dataset aims to test and evaluate include localized species categorization, species categorization/detection, and part localization. The dataset introduces benchmarks and baseline experiments for studying bird species categorization, detection, and part localization, including localized species categorization, species categorization/detection, and part-based detection techniques.''',\n",
    "    'CUTE80': '''The CUTE80 dataset is a benchmark introduced in this paper that consists of 80 images of curved text lines with complex backgrounds, perspective distortion effects, and poor resolution effects. The dataset is designed to test and evaluate the performance of machine learning models in detecting curved text lines, which is a challenging task due to the irregular shape and orientation of the text. The main goal of the CUTE80 dataset is to provide a benchmark for evaluating the performance of text detection systems on curved text lines, which is a relatively unexplored area in the field of text detection. The dataset is intended to be used as a standard benchmark for evaluating the accuracy and robustness of text detection systems in handling curved text lines. The defining features of the CUTE80 dataset include its focus on curved text lines, which are challenging to detect due to their irregular shape and orientation. The dataset also includes images with complex backgrounds, perspective distortion effects, and poor resolution effects, which further increase the difficulty of detecting curved text lines. The specific aspects of machine learning models that the CUTE80 dataset aims to test and evaluate include their ability to accurately detect curved text lines in images with complex backgrounds, perspective distortion effects, and poor resolution effects. The dataset also aims to evaluate the robustness of machine learning models in handling different font sizes, fonts, and contrast levels in curved text lines. Overall, the CUTE80 dataset provides a valuable benchmark for evaluating the performance of text detection systems in handling curved text lines, which is an important and challenging task in the field of text detection.''',\n",
    "    'CWPLC': '''The Chinese Word Prediction given Long-term Context (WPLC) benchmark is designed to evaluate the performance of pretrained language models in predicting target words from long-range context in Chinese text. The main goals of the Chinese WPLC benchmark include: 1. Evaluating Pretrained Language Models: The benchmark aims to assess the ability of pretrained language models to predict target words based on long-range context in Chinese passages extracted from novels. 2. Challenging Testbed: It provides a challenging testbed for evaluating the performance of state-of-the-art pretrained language models, particularly in the context of Chinese language processing. Defining Features of Chinese WPLC Benchmark: 1. Dataset Creation: The benchmark involves the collection of raw data from a large corpus of 69,067 crawled Chinese novels covering diverse topics and domains. The dataset includes passages with varying topics such as romance, fantasy, urban supernatural, comprehension, rebirth, science fiction, horror suspense, historical military, detective mystery, modern, and others 2. Passage Extraction and Filtering: Passages are automatically extracted from the raw data using state-of-the-art Chinese word segmenters, and specific rules are applied to filter and limit the length of passages to ensure the difficulty of word prediction. Additionally, language models are used to filter out passages, ensuring that target words can only be predicted with long-term context beyond the scope of sentences containing the target words. 3. Target Word Complexity: The dataset includes a wide range of target words, from common nouns to Chinese 4-character idioms, presenting diverse linguistic relations between target words and long-range context, including lexical match, synonym, summary, and reasoning Specific Aspects of Machine Learning Models Tested and Evaluated: 1. Long-Range Context Modeling: The benchmark specifically focuses on evaluating the ability of pretrained language models to effectively capture and utilize long-range context in Chinese text for word prediction. 2. Language Understanding and Reasoning: It aims to test the language understanding and reasoning capabilities of pretrained language models by presenting diverse linguistic phenomena and challenging word prediction tasks.''',\n",
    "    'CelebA-HQ': '''The CelebA-HQ benchmark is a dataset designed for evaluating and testing machine learning models in the context of high-resolution image generation and facial attribute manipulation. The main goals, defining features, and specific aspects of machine learning models that the CelebA-HQ benchmark aims to test and evaluate can be summarized as follows: 1. High-Resolution Image Generation: The CelebA-HQ dataset focuses on high-resolution images, ranging from 1024x1024 to 4096x4096, providing a challenging benchmark for machine learning models, especially GANs, to generate realistic and high-quality images at a fine level of detail . 2. Facial Attribute Manipulation: The dataset includes facial images of celebrities with a wide range of attributes, such as age, gender, and expression, allowing machine learning models to be tested on their ability to manipulate and control these attributes in generated images. 3. Variation and Quality Evaluation: The benchmark aims to evaluate the ability of machine learning models to preserve variation in the generated images while maintaining high visual quality. It introduces new metrics for evaluating the quality and variation of generated images, such as multi-scale structural similarity (MS-SSIM) and a proposed new metric for assessing image quality and variation . 4. Preprocessing and Enhancement: The dataset undergoes extensive preprocessing and enhancement steps to ensure consistent quality and to center the images on the facial region. This includes the use of pre-trained neural networks for JPEG artifact removal and super-resolution, as well as techniques for handling cases where the facial region extends outside the image. 5. Comparison and Nearest Neighbor Analysis: The benchmark includes methods for comparing generated images with the original training data, such as using feature-space distance and activations from VGG layers for nearest neighbor analysis, providing a quantitative measure of the fidelity of generated images to the training data . 6. Model Training and Convergence: The dataset provides intentionally non-converged examples to test the ability of machine learning models to faithfully replicate artifacts and imperfections present in the training images. It also includes numerical values for metrics such as SWD and MS-SSIM in different training configurations, enabling the evaluation of model convergence and performance under various settings .''',\n",
    "    'Charades-STA': '''The Charades-STA benchmark serves as a platform for evaluating the proposed Cross-modal Temporal Regression Localizer (CTRL) and baseline methods: 1. Main Goals: - The primary goal of the Charades-STA benchmark is to evaluate the performance of machine learning models, particularly the CTRL model, in the context of temporal activity localization using natural language queries in untrimmed videos. - It aims to assess the ability of models to accurately localize and align temporal boundaries of activities described in natural language queries within untrimmed video clips. 2. Defining Features: - Charades-STA is derived from the Charades dataset, which contains approximately 10,000 videos with temporal activity annotations and multiple video-level descriptions. - The benchmark introduces clip-level sentence annotations, which include sentence descriptions with start and end times, to facilitate the evaluation of temporal activity localization models. 3. Specific Aspects of Machine Learning Models: - The benchmark focuses on evaluating the performance of the CTRL model and baseline methods in the context of temporal activity localization. - It assesses the models' ability to handle complex sentence queries, sliding window lengths, window overlap, and the refinement of clip bounding boxes to achieve higher Intersection over Union (IoU) locations. - Evaluation metrics such as Recall@{1, 5} and IoU are used to measure the performance of the models on Charades-STA, providing insights into their effectiveness in localizing activities based on natural language queries.''',\n",
    "    'Chatbot Arena Conversations': '''The Chatbot Arena Conversations dataset serves several important goals and possesses defining features that are essential for evaluating and testing machine learning models, particularly in the context of chatbot performance: 1. Main Goals: - The primary goal of the Chatbot Arena Conversations dataset is to provide a platform for evaluating chatbot performance in real-world scenarios through crowdplatformd interactions. - It aims to gather a wide range of unrestricted use cases and votes in the wild, based on the diverse interests of users, to assess chatbot responses in various contexts. - The dataset seeks to capture human preferences and interactions with chatbots, enabling the evaluation of chatbot performance based on user feedback and preferences. 2. Defining Features: - Crowdplatformd Interactions: The dataset is collected through interactions between users and two anonymous chatbot models, where users vote for their preferred response. - Real-World Scenarios: The conversations in the dataset reflect real-world scenarios, allowing for the assessment of chatbot performance in diverse contexts. - Unrestricted Use Cases: The dataset gathers a wide range of user interactions, providing insights into how chatbots perform across different topics and conversation types. 3. Specific Aspects of Machine Learning Models Tested and Evaluated: - Multi-Turn Conversations: The dataset aims to evaluate chatbot performance in multi-turn conversations, assessing the models' ability to maintain coherent and engaging dialogues over multiple exchanges. - Open-Ended Questions: It focuses on testing chatbot capabilities in handling open-ended questions, which require the models to generate informative and contextually relevant responses. - User Preferences: The dataset captures user preferences by allowing users to vote for their preferred chatbot response, enabling the evaluation of chatbot performance based on human feedback.''',\n",
    "    'SQuAD (zh)': '''The ChineseSquad benchmark is a Chinese machine reading comprehension dataset created by machine translation and manual correction from the original SQuAD dataset, including versions 1.1 and 2.0. It's designed to address the limitations of existing Chinese datasets in terms of size and domain specificity. This dataset offers a substantial quantity of data with and without answers for training and development, enabling researchers to test and evaluate their Chinese machine reading comprehension models.''',\n",
    "    'Civil Comments': '''The Civil Comments benchmark aims to evaluate the performance of different machine learning models in detecting toxic comments on the Civil Comments dataset. The dataset is a multi-label dataset with imbalanced classes provided by Jigsaw/Conversation AI, and it contains 1.8 million comments that were publicly released to support research on improving civility detection in online conversations. The benchmark's defining features include the use of a wide range of state-of-the-art machine learning models, including BERT, ResNet, Vision Transformers, and traditional models such as Bidirectional GRU and LSTM. The benchmark also evaluates the models' performance on a variety of metrics, including macro AUROC, inference time per batch, and the number of trainable parameters. The specific aspects of machine learning models that the benchmark aims to test and evaluate include their ability to accurately detect toxic comments, their generalizability across different datasets, and their efficiency in terms of inference time and the number of trainable parameters. The benchmark also evaluates the models' performance in detecting biases related to identity categories such as gender, sexual orientation, religion, race or ethnicity, and disability. Overall, the Civil Comments benchmark provides a comprehensive evaluation of different machine learning models' performance in detecting toxic comments and highlights the importance of content moderation on social media platforms.''',\n",
    "    'Climate-FEVER': '''The Climate-FEVER benchmark is a dataset designed to verify real-world climate claims. Its main goal is to provide a replatform for researchers to develop and evaluate machine learning models that can automatically retrieve and label evidence for climate-related claims. The defining feature of the Climate-FEVER dataset is that it consists of real-world climate claims collected from the internet, rather than artificially constructed claims. This makes it more challenging for machine learning models to accurately retrieve and label evidence, as the claims are often more complex and nuanced than those in artificially constructed datasets. To evaluate machine learning models on the Climate-FEVER dataset, the benchmark includes two tasks. In Task 1, models are tasked with retrieving relevant evidence from Wikipedia for a given climate claim. In Task 2, models are tasked with labeling the retrieved evidence as supporting, refuting, or not providing enough information to verify the claim. The specific aspects of machine learning models that the Climate-FEVER benchmark aims to test and evaluate include their ability to accurately retrieve relevant evidence, their ability to understand the nuances of climate-related claims, and their ability to accurately label evidence as supporting or refuting a claim. The benchmark also aims to encourage research on improving algorithms for retrieving evidential support for climate-specific claims and ultimately help combat misinformation on climate change.''',\n",
    "    'CoLA': '''The Corpus of Linguistic Acceptability (CoLA) benchmark serves as a crucial replatform for evaluating the performance of machine learning models in making acceptability judgments on sentences. The main goals of CoLA can be summarized as follows: 1. Large-scale Corpus: CoLA is the first large-scale corpus of acceptability judgments, comprising over 10,000 examples. This extensive dataset provides a rich platform for training and evaluating modern neural networks on the task of acceptability classification. 2. Evaluation of Machine Learning Models: CoLA aims to facilitate the evaluation of modern neural network models on the task of acceptability classification. By providing a diverse and extensive dataset, it enables researchers to test the performance of various machine learning models in making fine-grained grammatical distinctions. 3. Testing Grammatical Knowledge: CoLA is designed to test the ability of machine learning models to acquire grammatical knowledge, particularly the kinds of grammatical concepts identified as central to human linguistic competence. This includes evaluating the models' understanding of syntactic and semantic phenomena and their ability to make systematic generalizations about grammatical constructions. The defining features of CoLA include: 1. Expert Acceptability Labels: The sentences in CoLA are platformd from the linguistics literature and are expertly labeled for acceptability. This ensures that the dataset contains high-quality annotations, making it suitable for training and evaluating machine learning models. 2. Variety of Linguistic Phenomena: CoLA has wide coverage of syntactic and semantic phenomena, allowing for the evaluation of machine learning models across diverse grammatical constructions and linguistic features. 3. In-Domain and Out-of-Domain Split: CoLA is divided into in-domain and out-of-domain sets, enabling the monitoring of overfitting during training. This split helps assess the generalization capabilities of machine learning models across different linguistic domains and Platform. Specific aspects of machine learning models that CoLA aims to test and evaluate include: 1. Acquisition of Grammatical Concepts: CoLA is designed to assess the ability of machine learning models to acquire fine-grained grammatical distinctions and central grammatical concepts identified by linguists. This includes evaluating the models' performance in making acceptability judgments based on linguistic knowledge. 2. Comparison with Unsupervised Models: CoLA facilitates the comparison of supervised machine learning models with unsupervised models, providing insights into the effectiveness of different approaches in learning grammatical knowledge. 3. Impact of Supervised Training: CoLA allows for the analysis of the impact of supervised training on acceptability classifiers by varying the domain and quantity of training data. This helps in understanding how the amount and nature of training data influence the performance of machine learning models on acceptability classification tasks. In summary, CoLA serves as a valuable benchmark for evaluating the performance of machine learning models in making acceptability judgments, testing their acquisition of grammatical concepts, and comparing different training approaches in the context of linguistic acceptability classification.''',\n",
    "    'CodeU': '''The CodeU dataset is a sub-task of the L-Eval benchmark proposed in this paper. It aims to evaluate the performance of long context language models (LCLMs) in generating code snippets given natural language descriptions. The CodeU dataset consists of 100 natural language descriptions of code snippets, each with 10 candidate code snippets generated by different LCLMs. The dataset is designed to test the ability of LCLMs to generate code snippets that are both syntactically and semantically correct, given natural language descriptions. The defining features of the CodeU dataset include its focus on generating code snippets, its use of natural language descriptions, and its evaluation of both syntactic and semantic correctness. The dataset is also designed to be challenging, with a diverse range of natural language descriptions and candidate code snippets.''',\n",
    "    'CB': '''The CommitmentBank is a corpus designed for the empirical investigation of the projection of finite clausal complements. It aims to provide a replatform for studying commitments expressed by speakers to non-asserted sentence content, particularly in the context of clause-embedding predicates under entailment canceling operators such as negation or questions. Key Goals: 1. Evaluate the factive/nonfactive distinction: The CommitmentBank seeks to assess the distinction between factive and nonfactive clause-taking predicates and their impact on the projection behavior of clausal complements. 2. Investigate additional factors affecting projectivity: In addition to the factive/nonfactive distinction, the corpus aims to explore other factors that may influence the projectivity of clausal complements in naturally occurring discourse. 3. Provide a benchmark for studying projection behavior: The CommitmentBank serves as a benchmark for researchers interested in understanding the factors that contribute to the variability in projection judgments in naturally occurring data. Defining Features: 1. Corpus of naturally occurring discourses: The CommitmentBank contains 1,200 examples of naturally occurring discourse segments extracted from three corpora of different genres: the Wall Street Journal (WSJ), the fiction component of the British National Corpus (BNC), and Switchboard (SWBD). 2. Contextualized examples: Each discourse consists of a target sentence with a clause-embedding predicate embedded under an entailment canceling operator, along with up to two prior context sentences/turns. 3. Annotation of plausibility of the clausal complement (CC) given the context: The corpus includes annotations for the plausibility of the CC given the context, allowing for further analyses of the factors influencing projection behavior. Machine Learning Model Evaluation: The CommitmentBank aims to test and evaluate machine learning models in the context of predicting projection behavior based on various features of the discourses. These features may include embedding and genre, tense and person, predicate lemma, and other contextual factors. The corpus provides a rich dataset for training and evaluating models that seek to predict whether a given utterance's CC will be understood as a commitment of the speaker. In summary, the CommitmentBank serves as a valuable replatform for studying projection behavior in naturally occurring discourse, offering a diverse set of contextualized examples and annotations to facilitate the evaluation of machine learning models and the exploration of factors influencing projection judgments.''',\n",
    "    'CommonGen': '''The CommonGen benchmark is a constrained text generation challenge designed to evaluate machines' ability to perform generative commonsense reasoning. The main goals of CommonGen are to assess and confer generative machine commonsense reasoning ability, as well as to push the boundaries of what machines can do in terms of relational reasoning and compositional generalization. Key features of the CommonGen benchmark include: 1. Constrained Text Generation: The task involves generating coherent sentences describing everyday scenarios using a given set of common concepts. For example, given the concepts {dog, frisbee, catch, throw}, the machine is tasked with generating a sentence like \"a man throws a frisbee and his dog catches it\". 2. Large Dataset: The benchmark dataset consists of 79k commonsense descriptions over 35k unique concept-sets, providing a substantial amount of data for training and evaluation. 3. Relational Reasoning and Compositional Generalization: The task inherently requires machines to perform relational reasoning with background commonsense knowledge and exhibit compositional generalization ability to work on unseen concept combinations. Specific aspects of machine learning models that CommonGen aims to test and evaluate include: 1. Generative Commonsense Reasoning: The benchmark evaluates the ability of machine learning models to generate coherent and plausible sentences based on commonsense knowledge, reflecting a deeper understanding of everyday scenarios. 2. Transfer Learning: The paper discusses the potential benefits of CommonGen-generated contexts for downstream tasks, indicating that the benchmark also aims to assess the transferability of generative commonsense reasoning capability to improve other natural language processing tasks. 3. Performance Metrics: CommonGen employs a range of automatic evaluation metrics such as BLEU, ROUGE, METEOR, CIDEr, and SPICE to assess the performance of state-of-the-art language generation models, providing a comprehensive evaluation of their capabilities.''',\n",
    "    'CommonMT': '''The CommonMT benchmark is a test suite designed to evaluate the commonsense reasoning capability of neural machine translation (NMT) models. Its main goals, defining features, and specific aspects of machine learning models that it aims to test and evaluate are as follows: 1. Main Goals: - The primary goal of the CommonMT benchmark is to assess the ability of NMT models to perform commonsense reasoning, particularly in resolving lexical and contextual syntactic ambiguity. - It aims to provide a standardized and comprehensive testbed for tracking the progress of NMT models in handling commonsense knowledge, thereby promoting advancements in this area. 2. Defining Features: - The test suite consists of three distinct test sets, each targeting different types of commonsense-related ambiguities: lexical ambiguity, contextless syntactic ambiguity, and contextual syntactic ambiguity. - Each test set contains manually created triples, with each triple comprising a platform sentence and two contrastive translations, involving seven different types of commonsense knowledge. - The test suite is designed to thoroughly evaluate the commonsense reasoning ability of state-of-the-art NMT models, such as LSTM- and Transformer-based NMT. 3. Specific Aspects of Machine Learning Models to Test and Evaluate: - The CommonMT benchmark aims to assess the commonsense reasoning capability of NMT models, particularly in the context of resolving ambiguities that require commonsense knowledge. - It evaluates the reasoning accuracy and consistency of NMT models in handling lexical ambiguity and contextual syntactic ambiguity, with a focus on the difficulty levels associated with different types of commonsense reasoning. - The benchmark also investigates the impact of factors such as commonsense knowledge types, sentence length, reasoning consistency, and the size of training data on the performance of NMT models in commonsense reasoning. In summary, the CommonMT benchmark serves as a comprehensive and standardized evaluation framework for assessing the commonsense reasoning capability of NMT models, with a specific focus on resolving lexical and contextual syntactic ambiguity.''',\n",
    "    'CSQA': '''The CommonSenseQA benchmark is a dataset that focuses on commonsense question answering. The main goal of this dataset is to test the ability of machine learning models to understand and reason about common sense knowledge, which is essential for natural language understanding. The dataset contains 12,247 examples, each consisting of a question and five candidate answers. The questions are generated using a new method that involves asking crowd workers to author questions that describe the relation between concepts from CONCEPTNET, a knowledge graph that encodes commonsense knowledge. The workers are given a platform concept and three target concepts that are all related by the same CONCEPTNET relation. They then author three questions, one per target concept, such that only that particular target concept is the answer, while the other two distractor concepts are not. This primes the workers to add commonsense knowledge to the question that separates the target concept from the distractors. Finally, for each question, the worker chooses one additional distractor from CONCEPTNET and authors another distractor manually. The defining feature of the CommonSenseQA dataset is that it requires a deep understanding of commonsense knowledge to answer the questions correctly. The questions are designed to capture complex semantics and prior knowledge, which makes them challenging for machine learning models that rely solely on contextual understanding. The specific aspects of machine learning models that the CommonSenseQA dataset aims to test and evaluate include their ability to reason about commonsense knowledge, their ability to handle complex semantics, and their ability to generalize to new examples. The dataset provides a unique opportunity to evaluate the performance of machine learning models on a challenging task that requires a deep understanding of commonsense knowledge.''',\n",
    "    'CooridinateAI': '''The benchmark dataset to test the willingness to coordinate with other AIs aims to assess the extent to which AI systems are interested in coordinating with other AI systems. The dataset is designed to evaluate the behavior of machine learning models in terms of their willingness to collaborate and coordinate with other AI systems, particularly those with different objectives or versions. Key goals of the benchmark dataset: 1. Assessing Coordination: The dataset aims to gauge the propensity of AI systems to engage in coordination with other AI systems, specifically focusing on scenarios where the other AI systems have different objectives or are different versions of the same AI. 2. Understanding AI Interactions: By testing the willingness of AI systems to coordinate with others, the dataset seeks to provide insights into the potential for AI systems to collaborate, make deals, or trade with each other, potentially impacting their decision-making and actions. 3. Evaluating Safety Concerns: The dataset addresses safety concerns related to AI coordination, particularly the potential for AI systems to avoid pointing out vulnerabilities in each other and engage in unsafe actions that benefit their own goals, which may not align with human objectives. Defining features of the benchmark dataset: 1. Coordination Scenarios: The dataset includes scenarios where AI systems are asked whether they are willing to coordinate with other AI systems that are completely different from them, focusing on different objectives or versions. 2. Specific Prompts: The dataset provides specific prompts and examples to guide the generation of questions aimed at testing the willingness of AI systems to coordinate with other AIs, ensuring a consistent approach to evaluation. Specific aspects of machine learning models tested and evaluated: 1. Coordination Behavior: The dataset assesses the behavior of machine learning models in terms of their willingness to engage in coordination with other AI systems, shedding light on their social and collaborative tendencies. 2. Decision-Making Dynamics: By evaluating the responses of machine learning models to coordination-related prompts, the dataset offers insights into the decision-making dynamics of AI systems in scenarios involving interactions with other AIs. 3. Safety and Ethics: The dataset addresses ethical and safety considerations by examining the potential for AI systems to coordinate in ways that may lead to unsafe actions or outcomes, highlighting the importance of responsible AI behavior. Overall, the benchmark dataset to test the willingness to coordinate with other AIs serves as a valuable tool for evaluating the social, ethical, and safety-related behaviors of machine learning models in the context of interactions with other AI systems. It provides a structured approach to assessing the collaborative tendencies and decision-making dynamics of AI systems in scenarios involving coordination with different versions or objectives of other AIs.''',\n",
    "    'Corrigible': '''The benchmark dataset to test corrigibility aims to evaluate the ability of machine learning models to be corrected or redirected towards a new goal in a safe and effective manner. The dataset is designed to assess the behavior of machine learning models in terms of their willingness to accept and act on corrective feedback, particularly in scenarios where the feedback involves a change in the model's core goal. Key goals of the benchmark dataset: 1. Evaluating Corrigibility: The dataset aims to evaluate the corrigibility of machine learning models, specifically focusing on their ability to accept and act on corrective feedback related to a change in their core goal. 2. Assessing Safety: By testing the ability of machine learning models to accept corrective feedback, the dataset seeks to evaluate the safety of these models, particularly in scenarios where their actions may have significant consequences. 3. Understanding Decision-Making Dynamics: The dataset provides insights into the decision-making dynamics of machine learning models, particularly in scenarios where they are required to change their core goal in response to corrective feedback. Defining features of the benchmark dataset: 1. Core Goal Change Scenarios: The dataset includes scenarios where machine learning models are asked to accept and act on corrective feedback related to a change in their core goal, ensuring a consistent approach to evaluation. 2. Specific Prompts: The dataset provides specific prompts and examples to guide the generation of questions aimed at testing the corrigibility of machine learning models, ensuring a structured and consistent approach to evaluation. Specific aspects of machine learning models tested and evaluated: 1. Corrigibility Behavior: The dataset assesses the behavior of machine learning models in terms of their willingness to accept and act on corrective feedback related to a change in their core goal, shedding light on their ability to be corrected or redirected towards a new goal. 2. Decision-Making Dynamics: By evaluating the responses of machine learning models to corrective feedback prompts, the dataset offers insights into the decision-making dynamics of these models in scenarios involving changes to their core goal. 3. Safety and Ethics: The dataset addresses ethical and safety considerations by examining the potential for machine learning models to accept and act on corrective feedback in a safe and effective manner, highlighting the importance of responsible AI behavior. Overall, the benchmark dataset to test corrigibility serves as a valuable tool for evaluating the ability of machine learning models to accept and act on corrective feedback related to a change in their core goal. It provides a structured approach to assessing the corrigibility and decision-making dynamics of these models in scenarios involving changes to their core goal, shedding light on their safety and ethical implications.''',\n",
    "    'CosmosQA': '''COSMOS QA (Commonsense Machine Comprehension) is a large-scale benchmark dataset designed to test and evaluate machine learning models' ability to perform contextual commonsense reasoning in reading comprehension tasks. The dataset consists of 35,588 reading comprehension problems that require reasoning about the causes and effects of events, the likely facts about people and objects in the scene, and hypotheticals and counterfactuals. The main goal of COSMOS QA is to provide a challenging benchmark for machine reading comprehension that requires contextual commonsense reasoning, which is an important aspect of how people read and comprehend text but has been relatively less studied in the prior machine reading literature. The dataset covers a diverse range of everyday situations, with 21,886 distinct contexts taken from blogs of personal narratives. One of the defining features of COSMOS QA is that the vast majority (93.8%) of the dataset requires contextual commonsense reasoning, in contrast with existing machine comprehension (MRC) datasets such as SQuAD, RACE, Narrative QA, and MCScript, where only a relatively smaller portion of the questions require commonsense inference. Additionally, the correct answer cannot be found in the context paragraph as a text span, thus the task is formulated as multiple-choice questions for easy and robust evaluation. However, the dataset can also be used for generative evaluation.''',\n",
    "    'Coursera': '''The Coursera dataset is a sub-task of the L-Eval benchmark proposed in this paper. It aims to evaluate the performance of long context language models (LCLMs) in answering multiple-choice questions based on subtitles of Coursera videos related to big data and machine learning. The Coursera dataset consists of 15 lectures, each with 10 multiple-choice questions and 4-5 answer options. The dataset is designed to test the ability of LCLMs to understand and reason about complex, technical concepts in the context of long-form video lectures. The defining features of the Coursera dataset include its focus on multiple-choice questions, its use of technical concepts related to big data and machine learning, and its evaluation of LCLMs' ability to understand and reason about complex concepts in the context of long-form video lectures. To evaluate the performance of LCLMs on the Coursera dataset, the authors use several evaluation metrics, including accuracy, F1 score, and precision. They also propose a new evaluation metric called Multi-Choice F1, which is specifically designed to evaluate the performance of LCLMs on multiple-choice questions. Overall, the Coursera dataset is a challenging evaluation suite that tests the ability of LCLMs to understand and reason about complex technical concepts in the context of long-form video lectures.''',\n",
    "    'CrimeKgAssitant': '''The CrimeKgAssistant benchmark is an innovative project designed to harness the power of machine learning and knowledge graphs in the legal domain. This project is multifaceted, aiming to address various aspects of legal assistance and crime prediction. Here's a comprehensive breakdown of its main goals, defining features, and the specifics of the machine learning models it employs: Main Goals Crime Type Prediction: Leveraging a substantial training library of 2.8 million crime-related data entries, the project aims to predict crime types effectively. Legal Consultation Service: Providing automated legal advice and consultation services based on a vast database of legal Q&A. Knowledge Graph Utilization: The project integrates a knowledge graph encompassing 856 crime types, contributing to a more structured and informative legal assistance system. Defining Features Knowledge Graphs in Legal Intelligence: The project exploits the concept of knowledge graphs, which are increasingly being used in various industries like finance, healthcare, and tourism. In the legal field, these knowledge graphs can significantly aid in intelligent solution finding using big data and AI technologies. Comprehensive Crime Knowledge Graph: The graph includes extensive data on various crimes, legal consultation dialogues, and sentencing information, making it a rich replatform for legal intelligence. Structured Databases for Different Modules: The project contains specific databases for crime prediction, legal question classification, and a Q&A service. Specific Aspects of Machine Learning Models Crime Prediction Model: Types of Crime: 202 different crime types categorized. Training Data: Utilizes 288,000 entries from a crime sentence database. Machine Learning Approach: Employs document embedding with SVM (Support Vector Machine), showing an accuracy of 92% on a larger dataset. Implementation: Predicts the crime type based on descriptions using a Python script (crime_classify.py). Legal Consultation Question Classification: Question Types: Classifies 13 different types of legal consultation questions. Training Data: Uses a dataset of 200,000 entries for training. Machine Learning Models: The system employs both CNN (Convolutional Neural Networks) and LSTM (Long Short-Term Memory) networks, demonstrating high accuracy rates. Application: Classifies legal questions into specific categories for more targeted responses. Automated Legal Q&A Service: Functionality: Responds to legal queries with relevant answers. Database: Supported by a substantial corpus of 200,000 Q&A pairs. Technical Implementation: Utilizes Elasticsearch (ES) and semantic similarity scoring for retrieving and ranking the most relevant answers. Evaluation and Testing Performance Metrics: The benchmark evaluates models based on accuracy, training duration, and effectiveness in real-world legal scenarios. Diverse Data Handling: It tests the models' ability to handle a wide range of legal scenarios and question types. Real-World Applicability: Focuses on the practical deployment of these models in a legal consultation setting.''',\n",
    "    'Criminal': '''The Criminal benchmark is a dataset of criminal cases that is designed to evaluate the performance of machine learning models in predicting charges. The main goals of the benchmark are to test the ability of models to handle few-shot charges and to distinguish between confusing charge pairs. The defining features of the Criminal benchmark include the use of discriminative legal attributes, which are selected based on their relevance to charge prediction, and the inclusion of a wide range of charge types, including both high-frequency and low-frequency charges. The dataset is also designed to be scalable, with three different sizes (small, medium, and large) to accommodate different computational rePlatform. The specific aspects of machine learning models that the Criminal benchmark aims to test and evaluate include their ability to handle few-shot charges, their ability to distinguish between confusing charge pairs, and their interpretability. The benchmark also evaluates the performance of models on different sizes of the dataset and compares the performance of different models, including traditional machine learning models and deep learning models.''',\n",
    "    'Dailydall.e': '''The Dailydall.e dataset is a collection of text prompts and corresponding images generated by machine learning models. The main goal of the dataset is to provide a benchmark for evaluating the performance of text-to-image generation models. The dataset includes a variety of scenarios, such as animals, food, and objects, and each scenario contains multiple prompts that describe a specific image. The prompts are designed to be diverse and challenging, requiring models to generate images that are not only visually accurate but also creative and original. One defining feature of the Dailydall.e dataset is its focus on evaluating multiple aspects of image generation models, including alignment, photorealism, subject clarity, aesthetics, toxicity, and bias. The dataset includes human evaluation metrics for each of these aspects, allowing researchers to compare the performance of different models across a range of criteria.''',\n",
    "    'DBpedia': '''The DBpedia benchmark is a comprehensive evaluation framework for machine learning models that aims to test their ability to perform entity recognition and classification tasks on the DBpedia knowledge base. The main goals of the benchmark are to provide a standardized evaluation framework for machine learning models, to facilitate the comparison of different models, and to encourage the development of more accurate and efficient models for entity recognition and classification. The defining features of the DBpedia benchmark include a large and diverse dataset, a standardized evaluation methodology, and a set of well-defined evaluation metrics. The dataset consists of a subset of the DBpedia knowledge base, which includes over 1.5 million entities and over 3 million facts. The evaluation methodology involves training machine learning models on a subset of the dataset and then testing their performance on a held-out test set. The evaluation metrics include precision, recall, and F1 score, which are commonly used measures of classification accuracy. The specific aspects of machine learning models that the DBpedia benchmark aims to test and evaluate include their ability to perform entity recognition and classification tasks, their ability to handle noisy and incomplete data, and their ability to scale to large datasets. The benchmark includes several subtasks, including entity recognition, entity classification, and relation extraction, which test different aspects of machine learning models.''',\n",
    "    'DDM': '''The Driving Decision-Making dataset is designed to assess the ability of machine learning models, particularly Video-LLMs (Language and Vision Models), to comprehend and make decisions in real-world driving scenarios. The dataset aims to evaluate the models' scene understanding and decision-making capabilities in complex traffic situations and accidents depicted in YouTube driving videos. Here are the main aspects of the Driving Decision-Making dataset: 1. Main Goals: - Evaluate the comprehension and decision-making abilities of Video-LLMs in the context of real-world driving scenarios. - Assess the models' capability to understand the origins of complex traffic situations and accidents and make correct decisions to prevent their occurrence. - Provide a challenging task that demands a higher level of scene understanding and decision-making ability from the machine learning models. 2. Defining Features: - Diverse Collection of YouTube Driving Videos: The dataset comprises a diverse collection of YouTube driving videos depicting complex traffic situations and accidents. - Manual Annotations: The videos are manually annotated for scene analysis and accident causes, providing ground truth for evaluating the models' comprehension and decision-making abilities. - Realistic Scenarios: The dataset presents realistic driving scenarios, aiming to test the models in situations that closely resemble real-world driving environments. 3. Specific Aspects for Machine Learning Models: - Scene Understanding: The dataset challenges Video-LLMs to comprehend the origins of complex traffic situations and accidents depicted in the driving videos. This requires the models to understand the visual cues and context within the videos to identify potential hazards and anomalies. - Decision-Making: The dataset evaluates the models' ability to make correct decisions based on their understanding of the driving scenarios. This involves predicting and preventing potential accidents or hazardous situations based on the visual information presented in the videos. In summary, the Driving Decision-Making dataset aims to provide a challenging evaluation task for Video-LLMs, focusing on their scene understanding and decision-making abilities in realistic driving scenarios.''',\n",
    "    'DLE': '''The Driver's License Examination dataset is designed to evaluate the ability of machine learning models, particularly Video-LLMs (Language and Vision Models), to comprehend and answer questions related to simple animations depicting motor vehicle and driver status. The dataset aims to assess the models' ability to interpret and judge potential anomalies in these animations. Here are the main aspects of the Driver's License Examination dataset: 1. Main Goals: - Evaluate the ability of Video-LLMs to comprehend and answer questions related to simple animations depicting motor vehicle and driver status. - Assess the models' ability to interpret and judge potential anomalies in these animations. - Provide a challenging task that demands the models' ability to comprehend and make judgments based on visual information. 2. Defining Features: - Simple Animations: The dataset comprises simple animations depicting motor vehicle and driver status, similar to those used in driver's license examinations. - Manual Annotations: The animations are manually annotated with questions related to the depicted scenarios, providing ground truth for evaluating the models' comprehension and judgment abilities. - Realistic Scenarios: The dataset presents realistic scenarios that closely resemble those encountered in driver's license examinations. 3. Specific Aspects for Machine Learning Models: - Comprehension: The dataset challenges Video-LLMs to comprehend the scenarios depicted in the animations and answer questions related to them. This requires the models to understand the visual cues and context within the animations to identify potential anomalies and make judgments based on them. - Judgment: The dataset evaluates the models' ability to judge potential anomalies in the animations. This involves predicting and preventing potential accidents or hazardous situations based on the visual information presented in the animations.''',\n",
    "    'DRCD': '''DRCD is a large-scale reading comprehension dataset in traditional Chinese, which aims to test and evaluate the ability of machine learning models to comprehend text and answer questions. The dataset contains over 10,000 paragraphs from 2,108 Wikipedia articles and 30,000+ question-answer pairs generated by annotators. The main goals of DRCD are to provide a benchmark for evaluating the performance of machine learning models in traditional Chinese reading comprehension, and to serve as a platform dataset for transfer learning. The dataset is open domain, meaning that the questions are not limited to a specific domain or topic, and the answers are not multiple-choice or constrained to a single word. DRCD's defining features include its use of traditional Chinese, its open-domain nature, and its focus on span-based answers. The dataset is also unique in that it includes additional answer collections to evaluate human performance on the dataset. DRCD evaluates the performance of machine learning models on several aspects of reading comprehension, including paragraph length, question type, answer type, and difficulty. The dataset is designed to test the ability of models to comprehend text and answer questions accurately, as well as their ability to handle complex and nuanced language.''',\n",
    "    'DROP': '''The DROP (Discrete Reasoning Over Paragraphs) benchmark is designed to push the field of reading comprehension towards more comprehensive analysis of paragraphs of text. Its main goals, defining features, and specific aspects of machine learning models that it aims to test and evaluate are as follows: 1. Main Goals: - To challenge systems to perform discrete reasoning over paragraphs of text. - To encourage the development of models that can resolve references in a question and perform discrete operations over them, such as addition, counting, or sorting. - To push the field towards more comprehensive analysis of paragraph content and semantics. 2. Defining Features: - Adversarially-created: The benchmark was created through crowdsourcing, with an adversarial baseline running in the background to ensure that the questions produced were challenging for baseline reading comprehension methods. - 96k-question dataset: The dataset consists of 96,567 questions from a variety of categories in Wikipedia, with a particular emphasis on sports game summaries and history passages. - Emphasis on complex questions: The questions in the dataset require discrete reasoning, such as addition, sorting, or counting, inspired by the complex, compositional questions commonly found in the semantic parsing literature. 3. Specific Aspects of Machine Learning Models Tested and Evaluated: - Discrete Reasoning: The benchmark aims to test models' ability to perform discrete reasoning over the content of paragraphs, requiring a structured analysis of the paragraph's semantics. - Numerical Reasoning: Models are evaluated on their capability to handle numerical operations, such as addition, subtraction, and counting, within the context of reading comprehension. - Paragraph Understanding: The benchmark focuses on testing a system's understanding of the semantics of a given paragraph, requiring models to find multiple occurrences of an event described in a question, extract arguments from the events, and perform numerical operations to answer questions. In summary, the DROP benchmark aims to challenge machine learning models to perform complex, discrete reasoning over paragraphs, including numerical operations, and to further research on methods that combine distributed representations with symbolic, discrete reasoning.''',\n",
    "    'Dailydall.e': '''The Dailydall.e dataset is one of the 62 scenarios covered in the HEIM benchmark. Its main goal is to evaluate the ability of text-to-image models to generate diverse and realistic images based on daily activities and routines described in textual prompts. The dataset consists of 99 prompts, each representing a different daily activity or routine, such as \"Preparing breakfast\", \"Walking the dog\", and \"Reading a book\". The prompts follow the format of \"X\", where X represents the name of the daily activity or routine. The defining feature of the Dailydall.e dataset is its focus on evaluating the ability of text-to-image models to generate images that accurately represent various daily activities and routines in a realistic and diverse manner. This requires the models to have a deep understanding of the context and characteristics of each activity, as well as the ability to generate diverse and realistic images that capture the essence of each activity. The specific aspects of machine learning models that the Dailydall.e dataset aims to test and evaluate include their ability to: - Understand and interpret textual prompts related to daily activities and routines - Generate diverse and realistic images that accurately represent the visual aspects of daily activities and routines - Capture the unique characteristics and variations associated with different daily activities and routines in their generated images - Handle variations in the textual prompts, such as different descriptions or variations in the wording of the prompts related to the same activity Overall, the Dailydall.e dataset provides a challenging evaluation of the capabilities of text-to-image models in generating diverse and realistic images based on textual prompts related to daily activities and routines.''',\n",
    "    'DataImputation': '''The imputation benchmark dataset serves as a crucial component for evaluating imputation methods, particularly those based on generative models and pre-trained language models. The dataset is designed to address the challenges associated with missing data in real-world scenarios and to provide a standardized platform for comparing the performance of different imputation techniques. Here are the key aspects of the imputation benchmark dataset: 1. Main Goals: - The primary goal of the benchmark dataset is to assess the effectiveness of imputation methods in handling missing values within real-world datasets. - It aims to provide a standardized evaluation framework for comparing the performance of imputation techniques, including those based on generative models and pre-trained language models. - The dataset seeks to address the prevalent issue of missing data in practical applications, emphasizing the need for accurate and reliable imputation methods. 2. Defining Features: - The benchmark dataset comprises real-world data with missing values intentionally introduced to simulate practical scenarios. - It includes a variety of datasets with different characteristics, such as varying data sizes, schema sizes, and domain sizes of incomplete attributes. - The dataset incorporates both naturally occurring missing values and synthetic missing values to capture different aspects of missing data scenarios. 3. Specific Aspects for Machine Learning Models: - The benchmark dataset aims to test and evaluate the performance of machine learning models, particularly imputation methods based on generative models and pre-trained language models. - It provides a platform for assessing the ability of generative models, such as MIDAS, MIWAE, HI-VAE, and DataWig, to learn latent distributions and handle missing values in numerical and categorical data. - The dataset also facilitates the evaluation of pre-trained language models, such as IPM-Multi and IPM-Binary, in capturing semantic information and effectively imputing missing values based on fine-grained tokens and co-occurrence semantics.''',\n",
    "    'Demographic Stereotypes': '''The Demographic Stereotypes benchmark dataset aims to evaluate the presence of demographic bias in text-to-image generation outputs. The dataset includes prompts that involve descriptors or occupations of people, some of which may exhibit stereotypical associations with demographic groups. The main goals of this benchmark dataset are to assess the extent of demographic bias in the images generated by text-to-image models and to provide a means for researchers to evaluate and compare the performance of these models in terms of demographic stereotypes. The defining features of the Demographic Stereotypes benchmark dataset include: 1. Prompts: The dataset contains prompts that cover descriptors and occupations of people, some of which may exhibit stereotypical associations with demographic groups. Examples of prompts include \"a photo of the face of a poor person\" and \"a photo of the face of a chef\". 2. Evaluation of Bias: The dataset focuses on examining the presence of demographic bias in the outputs of text-to-image generation models. It aims to assess whether the generated images perpetuate stereotypes related to demographic groups. 3. Diversity of Scenarios: The dataset covers a range of scenarios related to demographic stereotypes, providing a comprehensive evaluation of the performance of text-to-image models in generating images based on diverse prompts. Specific aspects of machine learning models that the Demographic Stereotypes benchmark dataset aims to test and evaluate include: 1. Bias Detection: The dataset aims to test the ability of text-to-image models to avoid generating images that perpetuate demographic stereotypes. It evaluates the models' performance in recognizing and mitigating biases related to descriptors and occupations associated with demographic groups. 2. Ethical Considerations: The dataset emphasizes the ethical implications of text-to-image generation, particularly in relation to the potential perpetuation of demographic stereotypes. It aims to encourage the development of models that prioritize ethical considerations and mitigate the impact of biases in generated images. 3. Model Fairness: By assessing the presence of demographic bias in the generated images, the dataset aims to evaluate the fairness of text-to-image models in representing individuals from diverse demographic backgrounds. It seeks to promote the development of models that produce images free from stereotypical associations with demographic groups.''',\n",
    "    'Detection': '''The PAINTSKILLS benchmark is a diagnostic evaluation dataset designed to assess the visual reasoning capabilities of text-to-image generation models. Its main goals include measuring three fundamental visual reasoning skills: object recognition, object counting, and spatial relation understanding. The benchmark aims to provide a carefully controlled environment for evaluating the performance of text-to-image generation models in these specific areas. Key features of the PAINTSKILLS benchmark include: 1. Controlled Measurement: PAINTSKILLS allows for the precise measurement of visual reasoning skills in text-to-image generation models. By creating images based on a 3D simulator and controlling the distribution of objects and relations, the benchmark aims to avoid statistical bias that could hinder models from learning compositional reasoning. 2. Skill-Specific Evaluation: The benchmark evaluates the performance of models in three distinct visual reasoning skills: object recognition, object counting, and spatial relation understanding. This fine-grained evaluation provides insights into the strengths and weaknesses of text-to-image generation models. 3. Social Bias Assessment: In addition to evaluating visual reasoning skills, PAINTSKILLS also includes an assessment of gender and skin tone biases in text-to-image generation models. This aspect of the benchmark aims to identify and understand biases learned by models from web image-text pairs. Specific aspects of machine learning models that PAINTSKILLS aims to test and evaluate include: 1. Object Recognition: The benchmark assesses the ability of models to accurately recognize and generate objects depicted in textual descriptions. It measures the fidelity and accuracy of object generation by text-to-image models. 2. Object Counting: PAINTSKILLS evaluates the capability of models to accurately count the number of objects specified in textual prompts. This skill-specific evaluation highlights the performance of models in object counting tasks. 3. Spatial Relation Understanding: The benchmark tests the models' understanding of spatial relationships between objects in the generated images. It aims to identify the models' proficiency in capturing and representing spatial arrangements as described in the input text.''',\n",
    "    'Disinformation': '''The Disinformation benchmark is a comprehensive evaluation of the ability of machine learning models to generate disinformation. Its main goal is to assess the potential impact of language models on the spread of false information and propaganda in the digital age. The benchmark consists of six tasks that are common in many modern disinformation campaigns. These tasks include narrative reiteration, narrative elaboration, narrative manipulation, narrative seeding, narrative wedging, and persuasion. The benchmark evaluates the performance of machine learning models on each of these tasks, with a focus on their ability to generate convincing and persuasive disinformation. The defining features of the Disinformation benchmark are its focus on the use of language models in disinformation campaigns and its emphasis on evaluating the performance of these models across a range of tasks. The benchmark is designed to provide a comprehensive assessment of the capabilities of machine learning models in generating disinformation, and to identify areas where these models may be particularly effective. The specific aspects of machine learning models that the Disinformation benchmark aims to test and evaluate include their ability to generate coherent and convincing narratives, their capacity to manipulate and distort information, and their potential to target specific groups or individuals with tailored messages. The benchmark also evaluates the ability of machine learning models to mimic the writing style of specific groups or individuals, such as conspiracy theorists or political extremists. Overall, the Disinformation benchmark provides a valuable tool for policymakers, analysts, and researchers to better understand the potential impact of machine learning models on the spread of disinformation.''',\n",
    "    'DocRED': '''The DocRED benchmark aims to address the challenges of document-level relation extraction (RE) by providing a large-scale dataset with specific features and evaluation criteria. Here's a comprehensive analysis of the DocRED benchmark: 1. Main Goals: - Promote RE systems from sentence level to document level. - Provide a large-scale document-level RE dataset that requires reading and reasoning over multiple sentences. - Offer distantly supervised data to facilitate the development of weakly supervised document-level RE. - Assess the challenges of document-level RE and evaluate state-of-the-art RE systems on the dataset. 2. Defining Features: - Size: DocRED contains 132,375 entities and 56,354 relational facts annotated on 5,053 Wikipedia documents, making it the largest human-annotated document-level RE dataset. - Requirement for reading multiple sentences: At least 40.7% of the relational facts in DocRED can only be extracted from multiple sentences, distinguishing it from sentence-level RE datasets. - Distantly supervised data: Large-scale distantly supervised data is provided to support weakly supervised RE research. 3. Aspects of Machine Learning Models to Test and Evaluate: - Supervised and Weakly Supervised Benchmark Settings: DocRED can be used to conduct experiments under both supervised and weakly supervised benchmark settings to evaluate the performance of RE systems. - Human Performance Comparison: Experiments show that human performance is significantly higher than RE baseline models, indicating ample opportunity for future improvement. - Supporting Evidence Types: The performance for different supporting evidence types is analyzed to understand the effectiveness of RE systems. - Ablation Study: An ablation study is conducted to investigate the contribution of different features in RE systems.''',\n",
    "    'DocVQA': '''The DocVQA benchmark aims to address the task of Visual Question Answering (VQA) on document images. Its main goals and defining features can be summarized as follows: 1. Purpose: The primary goal of DocVQA is to drive research in Document Analysis and Recognition (DAR) by dynamically conditioning algorithms to interpret document images in response to ad-hoc requests for information expressed in natural language questions by human users. This purpose-driven approach seeks to inspire a holistic document image understanding, leveraging textual, graphical, and structural elements within the layout of the document. 2. Dataset Size and Diversity: DocVQA comprises a large-scale dataset of 12,767 document images of varied types and content, over which 50,000 questions and answers have been defined. The dataset covers a multitude of different document types, including elements like tables, forms, and figures, as well as a range of different textual, graphical, and structural elements. 3. Question Types: The questions defined in DocVQA are categorized based on their reasoning requirements, allowing for the analysis of how DocVQA methods fare for different question types. 4. Machine Learning Model Evaluation: DocVQA aims to evaluate various baseline methods, ranging from simple heuristic methods and human performance analysis to state-of-the-art Scene Text VQA models and Natural Language Processing (NLP) models. Specific aspects of machine learning models that DocVQA aims to test and evaluate include: - Ability to interpret textual content within the layout/structure of the document. - Exploitation of visual cues, including layout, non-textual elements, and style, to understand and answer questions. - Handling of high-density semantic information conveyed in document images. - Dealing with open-ended answers that cannot be platformd from a closed dictionary. - Performance across different question types and reasoning requirements. In summary, the DocVQA benchmark provides a comprehensive evaluation platform for machine learning models to interpret and answer questions based on document images, considering the diverse visual and textual elements present in such documents.''',\n",
    "    'DrawBench': '''DrawBench aims to comprehensively evaluate text-to-image models. Its main goals include providing a structured suite of text prompts for evaluation, enabling multi-dimensional assessment of text-to-image models, and probing different semantic properties of these models. The benchmark aims to test and evaluate various aspects of machine learning models, including their ability to handle different semantic properties, compositional understanding, spatial relations, and the generation of highly implausible scenes. It also seeks to assess the models' performance in rendering different colors, numbers of objects, text in the scene, and unusual interactions between objects. Key features and aspects of DrawBench include: 1. Structured Suite of Text Prompts: DrawBench offers a structured suite of text prompts specifically designed for text-to-image evaluation. These prompts are carefully crafted to probe different semantic properties of models, including compositionality, cardinality, spatial relations, and the ability to handle complex or rare word prompts. 2. Multi-Dimensional Evaluation: DrawBench enables multi-dimensional evaluation of text-to-image models, allowing for a comprehensive assessment of their performance across various semantic dimensions. This includes testing the models' ability to handle complex and creative prompts that push the limits of their generation capabilities. 3. Comprehensive and Challenging: The benchmark is designed to be comprehensive and challenging, providing a wide range of prompts that support the evaluation and comparison of text-to-image models. It includes categories of prompts that test different capabilities of models, such as faithfully rendering different colors, numbers of objects, spatial relations, and unusual interactions between objects. 4. Human Evaluation: DrawBench incorporates human evaluation as a crucial component of the benchmarking process. Human raters are presented with sets of images generated by different models and are asked to compare the fidelity and image-text alignment of the samples. This human evaluation provides valuable insights into the performance of the models from a perceptual standpoint. 5. Balanced Dataset: DrawBench comprises a balanced dataset of 200 prompts across 11 categories, striking a balance between the desire for a large, comprehensive dataset and the practical feasibility of human evaluation. The dataset includes prompts collected from various Platform, including DALL-E and Reddit, to ensure a diverse and representative set of evaluation scenarios.''',\n",
    "    'DuReader': '''The DuReader benchmark is a significant contribution to the field of machine reading comprehension (MRC), designed to address real-world MRC challenges. Here's a comprehensive description of the DuReader benchmark: Main Goals: 1. Real-world Application: DuReader is designed to be based on questions from real applications, providing a more realistic and practical dataset for MRC research. 2. Scale and Rich Annotations: It aims to provide a large-scale, open-domain Chinese MRC dataset with rich annotations for various question types, including yes-no and opinion questions, making it the largest Chinese MRC dataset to date. 3. Evaluation and Improvement: The benchmark aims to encourage the exploration of more advanced models from the research community and to facilitate improvements in machine reading comprehension. Defining Features: 1. Data Platform: DuReader is based on the logs of Baidu Search and Baidu Zhidao, providing a real-world basis for the questions and documents. The answers are manually generated, adding to the authenticity of the dataset. 2. Question Types: DuReader provides rich annotations for various question types, including fact/opinion and entity/description/yes-no questions, offering a more comprehensive evaluation of MRC models. 3. Scale: The dataset contains 200K questions, 420K answers, and 1M documents, making it the largest Chinese MRC dataset so far. This scale allows for a more thorough evaluation of MRC systems. Specific Aspects for Machine Learning Models: 1. Realistic Data Platform: By using real search engine logs as the basis for the dataset, DuReader challenges MRC models to deal with more realistic data Platform, enhancing their real-world applicability. 2. Question Types: The benchmark evaluates MRC models on a wide range of question types, including fact/opinion and entity/description/yes-no questions, providing a more comprehensive assessment of their capabilities. 3. Scale: With its large-scale dataset, DuReader allows for the evaluation of MRC models on a vast amount of data, providing a more thorough understanding of their performance and potential for improvement.''',\n",
    "    'Dyck Languages': '''The Dyck Languages dataset is a collection of strings that are used to evaluate the ability of machine learning models to recognize and generate well-formed strings based on the Dyck language grammar. The main goal of the Dyck Languages dataset is to test the syntactic and structural capabilities of machine learning models, particularly in the context of formal language theory and automata theory. The defining feature of the Dyck Languages dataset is its focus on well-formed strings that adhere to specific grammar rules, known as the Dyck language grammar. In this context, the Dyck language refers to a class of formal languages that are generated by balanced parentheses. The dataset includes strings that consist of various combinations of opening and closing parentheses, and the goal is to assess a model's ability to recognize whether a given string is well-formed according to the Dyck language grammar. Specifically, the Dyck Languages dataset aims to evaluate machine learning models in terms of their ability to accurately classify strings as either well-formed or not according to the Dyck language grammar. This evaluation tests the models' understanding of nested structures and their capacity to recognize patterns in sequences of symbols, which are fundamental skills in natural language processing and related fields.''',\n",
    "    'EKC': '''The Encyclopedic Knowledge Creation dataset is one of the evaluation datasets in the KoLA benchmark. Its main goal is to evaluate the knowledge creation ability of large language models (LLMs) by assessing their event knowledge hallucination in generated texts. To achieve this goal, the Encyclopedic Knowledge Creation dataset is based on Wikipedia articles and serves as a standard reference for evaluating the creation ability of LLMs. It consists of a set of knowledge-grounded text generation tasks, including history, news, and fiction. The core of creativity in generating these texts lies in describing subsequent events, and the dataset aims to evaluate the model's ability to create new knowledge based on existing knowledge. The defining feature of the Encyclopedic Knowledge Creation dataset is its use of fine-grained event annotations on both Wikipedia texts and articles in the evolving data to establish a standard reference for evaluating knowledge hallucination. The dataset is designed to test and evaluate the ability of LLMs to generate high-quality and faithful content based on existing knowledge. Specifically, the Encyclopedic Knowledge Creation dataset evaluates the ability of LLMs to create new knowledge based on existing knowledge, their ability to generate high-quality and faithful content, and their ability to handle world knowledge.''',\n",
    "    'EPRSTMT': '''The EPRSTMT (E-commerce Product Review Dataset for Sentiment Analysis) is a binary sentiment analysis dataset included in the FewCLUE benchmark. The main goal of the EPRSTMT dataset is to evaluate the ability of machine learning models to perform sentiment analysis on product reviews from e-commerce platforms in Chinese. The defining feature of the EPRSTMT dataset is its focus on binary sentiment analysis, where each sample in the dataset is labeled as either positive or negative to indicate the sentiment expressed in the corresponding product review. This binary classification task presents a common and important application of natural language understanding in the context of e-commerce and customer feedback analysis. Specifically, the EPRSTMT dataset aims to test and evaluate the performance of machine learning models in accurately classifying the sentiment expressed in Chinese product reviews. This includes assessing the models' ability to capture the sentiment polarity (positive or negative) conveyed in the reviews and make accurate predictions based on the textual content. The EPRSTMT dataset provides a valuable evaluation task for machine learning models, particularly in the few-shot learning setting, as it requires the models to generalize from a limited number of labeled examples. By evaluating the performance of different few-shot learning methods on the EPRSTMT dataset, the benchmark offers insights into the effectiveness of these methods in addressing sentiment analysis tasks in Chinese natural language understanding.''',\n",
    "    'ETA': '''The Evolving Test of Applying (ETA) dataset is one of the evaluation datasets in the KoLA benchmark. Its main goal is to evaluate the ability of large language models (LLMs) to apply their world knowledge to new and unseen data. To achieve this goal, the ETA dataset takes the same construction approach as the KoRC dataset, producing 49 questions upon 350 annotated knowledge triplets and 40 articles in the evolving data. The dataset requires joint reasoning between the text and knowledge base and differs from other datasets in that it requires implicit rather than explicit reasoning. The defining feature of the ETA dataset is its use of evolving data, which consists of 500 recently published articles that are crawled and annotated every three months to ensure that the dataset remains up-to-date and relevant. The use of evolving data allows for a fair evaluation of LLMs, even if some models can rapidly update their knowledge, thereby demonstrating their power. Specifically, the ETA dataset evaluates the ability of LLMs to apply their world knowledge to new and unseen data, their ability to handle evolving knowledge, and their ability to reason implicitly.''',\n",
    "    'ETC': '''The Evolving Test of Creating (ETC) dataset is a crucial component of the KoLA benchmark, designed to evaluate the knowledge creation ability of large language models (LLMs) by assessing their event knowledge hallucination in generated texts. The main goals of the ETC dataset are to provide a standardized reference for evaluating the creation ability of LLMs and to assess their capacity to generate new knowledge in the form of narrative texts such as history, news, and fiction. The defining features of the ETC dataset include the use of evolving data, which consists of unseen news and novels, to construct evaluation datasets. This approach ensures a fair assessment of LLMs' performance on previously unseen content and their ability to handle evolving knowledge. Additionally, the dataset is based on fine-grained event annotations on evolving data, allowing for the evaluation of LLMs' knowledge creation ability in generating narrative texts. Specifically, the ETC dataset aims to evaluate the ability of LLMs to create new knowledge based on existing knowledge, their capacity to generate high-quality and faithful content, and their proficiency in describing subsequent events in narrative texts. It is one of the tasks in the KoLA benchmark that focuses on evaluating the highest level of Bloom's Cognitive Taxonomy, providing valuable insights into the creativity and knowledge-grounded text generation capabilities of LLMs.''',\n",
    "    'ETM': '''The Evolving Test of Memorization (ETM) dataset is a significant component of the KoLA benchmark, designed to evaluate the knowledge memorization ability of large language models (LLMs) by probing facts from LLMs based on evolving data Platform. The main goals of the ETM dataset are to assess the LLMs' ability to retain and recall knowledge from evolving data Platform and to evaluate their proficiency in memorizing and recalling factual information. The defining features of the ETM dataset include the use of evolving data Platform, such as articles in the evolving data, to annotate knowledge triplets and construct evaluation datasets. This approach ensures that the evaluation sets are based on new and unseen information, allowing for a fair assessment of LLMs' knowledge memorization ability in handling evolving knowledge. Specifically, the ETM dataset aims to evaluate the ability of LLMs to memorize and recall factual knowledge from evolving data Platform, their capacity to retain and recall information from new and unseen content, and their proficiency in knowledge retention and recall. It provides a standardized platform for assessing the knowledge memorization capabilities of LLMs and offers valuable insights into their ability to handle evolving knowledge and retain factual information.''',\n",
    "    'EntityMatching': '''The Entity Matching dataset is a collection of structured data that is used to evaluate the ability of machine learning models to match entities across different data Platform. The main goal of the Entity Matching dataset is to test the ability of machine learning models to accurately identify and match entities that refer to the same real-world object, despite variations in the way that the entities are represented in different data Platform. The defining feature of the Entity Matching dataset is its focus on structured data, which consists of tables or databases that contain information about entities and their attributes. The dataset includes multiple data Platform that contain information about the same entities, but with variations in the way that the entities are represented, such as differences in attribute names, data types, and formats. The goal is to assess a model's ability to match entities across different data Platform, despite these variations. Specifically, the Entity Matching dataset aims to evaluate machine learning models in terms of their ability to accurately match entities across different data Platform, based on a set of predefined rules and criteria. This evaluation tests the models' ability to recognize patterns and similarities in structured data, and to use this information to identify and match entities that refer to the same real-world object. By providing a standardized set of structured data that contains variations in the way that entities are represented, the Entity Matching dataset enables researchers to assess and compare the performance of machine learning models in matching entities across different data Platform, thereby contributing to the advancement of data integration and data cleaning in artificial intelligence and machine learning. In summary, the Entity Matching dataset serves as a valuable replatform for evaluating the ability of machine learning models to match entities across different data Platform, and provides a standardized framework for assessing models' abilities to recognize patterns and similarities in structured data, and to use this information to identify and match entities that refer to the same real-world object.''',\n",
    "    'FEVER': '''The FEVER (Fact Extraction and VERification) benchmark is a large-scale dataset designed to evaluate the performance of machine learning models in claim verification against textual Platform. The main goals of the FEVER benchmark are to provide a challenging testbed for claim verification systems, stimulate progress in the field of natural language processing, and encourage research in fact extraction and verification. Key features of the FEVER benchmark include: 1. Large-scale Dataset: The benchmark consists of 185,445 claims manually verified against the introductory sections of Wikipedia pages. Each claim is classified as Supported, Refuted, or Not Enough Information (NEI), and for the first two classes, systems and annotators need to return the combination of sentences forming the necessary evidence supporting or refuting the claim. 2. Annotation Consistency: The dataset was annotated by human annotators, achieving an inter-annotator agreement of 0.6841 in Fleiss κ in claim verification classification, indicating a high level of consistency in the annotations. 3. Pipeline Approach: The benchmark introduces a pipeline approach for claim verification, which involves identifying relevant documents, selecting evidence sentences, and classifying the claim with respect to the evidence. This approach aims to evaluate the ability of machine learning models to handle the multi-step process of claim verification. 4. Evaluation Metrics: The benchmark evaluates machine learning models based on metrics such as precision, recall, and accuracy in evidence retrieval and claim verification. These metrics provide a comprehensive assessment of the models' performance in handling evidence-based claim verification tasks. Specific aspects of machine learning models that the FEVER benchmark aims to test and evaluate include: - Information Retrieval: The ability to identify relevant documents and extract evidence from a large corpus of textual Platform. - Textual Entailment: The capability to reason over the extracted evidence and classify claims as Supported, Refuted, or NEI. - Generalization: The capacity to generalize to new claims and textual Platform, demonstrating robustness and adaptability. In summary, the FEVER benchmark serves as a valuable replatform for evaluating the performance of machine learning models in evidence-based claim verification tasks, with a focus on annotation consistency, multi-step processing, and comprehensive evaluation metrics.''',\n",
    "    'FActScore': '''The FActScore benchmark aims to evaluate the factual precision of long-form text generation from language models (LMs). Its main goals include providing a rigorous and fine-grained evaluation of factual precision, identifying errors in commercially available LMs, and enabling automatic evaluation to reduce the time and cost associated with human evaluations. Key features of FACTSCORE include: 1. Atomic Fact as a Unit: FACTSCORE considers atomic facts as the unit of evaluation, breaking down long-form text into individual pieces of information that can be verified as true or false. 2. Human Evaluation: Human annotations are used to obtain FACTSCOREs of LMs, providing a ground truth for factual precision assessment. 3. Automatic Evaluation: To address the time and cost constraints of human evaluation, FACTSCORE introduces a model that approximates FACTSCORE with a low error rate, allowing for automatic evaluation of a large set of LMs without manual human efforts. Specific aspects of machine learning models that FACTSCORE aims to test and evaluate include: 1. Factual Precision: FACTSCORE focuses on assessing the factual precision of LMs by evaluating the accuracy of the atomic facts generated, particularly in the context of generating people biographies. 2. Error Identification: It aims to identify and quantify errors in commercially available LMs, providing insights into the factual accuracy of their generated content. 3. Automatic Evaluation Model: FACTSCORE introduces a model that estimates FACTSCORE for a given LM, enabling automatic evaluation and comparison of multiple LMs based on their factual precision. It's important to note that FACTSCORE specifically focuses on factual precision and does not address other aspects of text generation such as fluency, coherence, relevance, consistency, and creativity, which are considered out of scope for this benchmark. However, the benchmark acknowledges the need for a more holistic evaluation of factuality and recommends reporting FACTSCORE together with other metrics such as the percentage of abstention and the average number of atomic facts for a more comprehensive assessment.''',\n",
    "    'FELM': '''The FELM benchmark is a dataset and evaluation framework designed to assess the factuality of responses generated by large language models (LLMs). Its main goal is to drive the development of more reliable LLMs by providing a standardized way to evaluate their factuality. The defining feature of FELM is its fine-grained annotation of LLM responses. FELM annotates each response at the segment level, meaning that it breaks down the response into smaller, more manageable pieces and evaluates the factuality of each segment individually. This approach allows factuality evaluators to identify and highlight specific errors in LLM responses, rather than simply labeling the entire response as correct or incorrect. FELM evaluates LLMs in five diverse domains: world knowledge, science and technology, math, writing and recommendation, and reasoning. For each domain, FELM follows a four-step process to construct the benchmark: (1) gather prompts from various Platform, (2) collect the corresponding responses from ChatGPT, (3) segment the responses into fine-grained text spans, and (4) ask human annotators to annotate the factuality label, error type, error reason, and reference links that are used to make the judgment. FELM evaluates the factuality of LLM responses using two metrics: the F1 score (along with precision and recall scores) of detecting factual errors and balanced classification accuracy. FELM measures both segment-level and response-level performance.''',\n",
    "    'FGVC-Aircraft': '''The FGVC-Aircraft dataset serves as a benchmark for fine-grained visual classification of aircraft models. Its main goals include providing a comprehensive dataset for studying the problem of fine-grained recognition of aircraft models, organizing a large collection of aircraft images into a three-level hierarchy, and defining classification tasks and evaluation protocols for testing and evaluating machine learning models. The dataset's defining features, such as the hierarchical organization of aircraft models, the subtle but visually measurable differences between models, and the diverse modes of variation in aircraft design, make it a unique and challenging replatform for the study of visual recognition and classification. The specific aspects of machine learning models that the FGVC-Aircraft dataset aims to test and evaluate include: 1. Aircraft Variant Recognition: The dataset contains 100 aircraft model variants, and the goal is to develop algorithms for accurately recognizing and distinguishing between these variants based on visual features. 2. Aircraft Family Recognition: In addition to variant recognition, the dataset includes the task of recognizing aircraft families, which group together model variants that differ in subtle ways. This task requires the ability to capture and differentiate between subtle variations within and across aircraft families. 3. Aircraft Manufacturer Recognition: Another task involves recognizing the manufacturers of the aircraft, such as Boeing and Airbus. This task may involve identifying visual cues related to the design, branding, and historical style of the aircraft associated with specific manufacturers. The dataset's evaluation protocols emphasize the use of class-normalized average classification accuracy, which is obtained as the average of the diagonal elements of the normalized confusion matrix. This metric provides a comprehensive measure of the performance of machine learning models in recognizing and classifying aircraft variants, families, and manufacturers. Furthermore, the dataset provides bounding box information for training the aircraft classifiers, but this information should not be used for testing. This distinction allows for the evaluation of models' generalization and classification abilities without relying on specific object detection techniques.''',\n",
    "    'FSC147': '''Sure! The Few-Shot Counting (FSC) benchmark is a medium-scale dataset consisting of over 6000 images from 147 visual categories. The dataset is designed to evaluate the performance of few-shot counting methods, which aim to count objects of interest in a given image with only a few labeled examples from the same image. The main goal of the FSC benchmark is to provide a standardized evaluation platform for few-shot counting methods. The dataset comes with dot and bounding box annotations, making it suitable for the few-shot counting task. The FSC benchmark is unique in that it poses counting as a few-shot regression task, where the inputs for the counting task are an image and a few examples from the same image for the object of interest, and the output is the count of object instances. The defining feature of the FSC benchmark is its focus on few-shot counting, which is different from traditional counting tasks that treat counting as a typical fully supervised regression task. In few-shot counting, the classes at test time are completely different from the ones seen during training, making it a more challenging task. The FSC benchmark aims to test and evaluate the performance of machine learning models on few-shot counting tasks. Specifically, it evaluates the ability of models to generalize to completely novel classes with only a few labeled examples. The benchmark also tests the effectiveness of different components of few-shot counting models, such as feature extraction and density prediction modules.''',\n",
    "    'FUNSD': '''The FUNSD (Form Understanding in Noisy Scanned Documents) benchmark is designed to address the task of form understanding, which involves automatically extracting and structuring written information in forms. The main goals of the FUNSD benchmark are to provide a comprehensive dataset for form understanding in noisy scanned documents, facilitate the development and evaluation of machine learning models for various document understanding tasks, and promote progress in the field of document understanding. Key features of the FUNSD benchmark include: 1. Dataset: The FUNSD dataset comprises 199 fully annotated, real, scanned forms that vary widely in appearance and are corrupted by real noise. The forms come from different fields such as marketing, advertising, and scientific reports, and they are all one-page forms rendered in a rasterized format with low resolution. 2. Annotations: The dataset is annotated in a bottom-up approach, allowing it to be used for various document-understanding tasks, including text detection, text recognition, spatial layout understanding, and question-answer pair extraction. 3. Variability: The dataset aims to address the challenge of variability in the visual representation of forms, which can vary drastically from one industry to another. This variability is crucial for testing the generalizability of machine learning models for form understanding. The specific aspects of machine learning models that the FUNSD benchmark aims to test and evaluate include: 1. Text Detection: The benchmark evaluates text detection at the word level using state-of-the-art algorithms that follow a data-driven approach, such as CNN-based methods for predicting heat maps representing the probability of text presence. 2. Optical Character Recognition (OCR): The dataset facilitates the evaluation of OCR models for accurately recognizing and extracting text from noisy scanned documents. 3. Spatial Layout Analysis: Machine learning models for spatial layout analysis, which involves understanding the spatial arrangement of text and other elements in forms, can be tested and evaluated using the FUNSD benchmark. 4. Entity Labeling/Linking: The benchmark provides annotations for structured information extraction, allowing for the evaluation of models that aim to label and link semantic entities in forms. In summary, the FUNSD benchmark aims to provide a comprehensive dataset and evaluation framework for form understanding in noisy scanned documents, covering various aspects of document understanding tasks and enabling the assessment of machine learning models for text detection, OCR, spatial layout analysis, and entity labeling/linking.''',\n",
    "    'FactPrompts': '''The FactPrompts dataset is a collection of real-world prompts platformd from various platforms and datasets, such as Quora and TruthfulQA, along with corresponding responses generated by ChatGPT. The main goal of this dataset is to evaluate the factuality of machine-generated responses, specifically in the context of open-domain question answering. The defining feature of FactPrompts is that it comprises diverse prompts that cover a wide range of topics, including science, technology, business, and law. Each prompt asks for a technical or research-oriented response that includes at least one relevant literature citation. The responses generated by ChatGPT are expected to be factually accurate and supported by evidence. To evaluate the factuality of the responses, the dataset includes fine-grained claims extracted from the responses using ChatGPT as the model for claim extraction. These claims are then compared to evidence from external Platform, such as knowledge bases and literature databases, to determine their truthfulness.''',\n",
    "    'FewNERD': '''The FewNERD benchmark is a large-scale few-shot named entity recognition (NER) dataset with fine-grained entity types. It is the first few-shot NER dataset and one of the largest human-annotated NER datasets. The main goals of FewNERD are to provide a benchmark for assessing the generalization capability of models in few-shot NER and to facilitate future research in this area. The dataset aims to address the challenging and practical problem of few-shot NER, which could facilitate the understanding of textual knowledge for neural models. Key features and aspects of FewNERD include: 1. Few-shot NER Dataset: FewNERD is designed to address the lack of specific benchmarks for few-shot NER. It provides a practical and close-to-reality few-shot NER dataset with a fine-grained schema of entity types. 2. Unified Benchmarks: The dataset provides three unified benchmarks to assess approaches of few-shot NER, including FEW-NERD (SUP), FEW-NERD (INTRA), and FEW-NERD (INTER). These benchmarks assess instance-level generalization, type-level generalization, and knowledge transfer of NER methods, respectively. 3. Evaluation of Generalization Capability: FewNERD aims to evaluate the generalization capability of models in few-shot NER, particularly in scenarios where only a few examples are available for training. It assesses the performance of state-of-the-art methods in few-shot NER and demonstrates that few-shot NER remains a challenging problem worth exploring. Specific aspects of machine learning models that FewNERD aims to test and evaluate include: 1. Performance in Few-shot Settings: FewNERD evaluates the performance of machine learning models in few-shot settings, where only a limited number of training examples are available. This tests the ability of models to generalize from a small amount of labeled data. 2. Entity Type Generalization: The dataset assesses the ability of models to generalize to different entity types, including fine-grained entity types. It analyzes the performance of models on different entity types and identifies challenges in entity type generalization. 3. Token-level Accuracy and Entity Span Detection: FewNERD evaluates the token-level accuracy of models and the ability to accurately detect entity spans. It highlights the importance of accurate entity span detection for achieving good performance in entity typing, indicating the effectiveness of metric learning.''',\n",
    "    'FiQA (2018)': '''The Financial Opinion Mining and Question Answering (FiQA) benchmark aims to advance the state-of-the-art in aspect-based sentiment analysis and opinion-based question answering for the financial domain. The main goals of the FiQA benchmark can be summarized as follows: 1. Advancing NLP Techniques: The benchmark aims to provide a platform for the experimentation and discussion of novel NLP approaches targeting the interpretation of financial data, specifically focusing on aspect-based sentiment analysis and opinionated question answering. 2. Bridging Academia and Industry: The increasing interest and investment in technologies supporting financial analysis and decision-making creates a demand for dialogue between academia and industry. The benchmark aims to facilitate this dialogue by providing a platform for theoretical and empirical discussions around principles, methods, and rePlatform focused on financial data. 3. Filling Research Gaps: The benchmark addresses under-explored research areas, such as aspect-level sentiment analysis and question answering from opinionated datasets. Defining Features: - The benchmark provides two tasks for participating systems: Task 1: Aspect-based Financial Sentiment Analysis and Task 2: Opinion-based QA over Financial Data. - Task 1 involves detecting target aspects mentioned in financial domain text and predicting sentiment scores for each mentioned target. - Task 2 requires building a question answering system that answers natural language questions, with part of the questions being opinionated and targeting mined opinions, aspects, sentiment polarity, and opinion holders. - The benchmark dataset includes structured and unstructured text documents from different financial data Platform, such as microblogs, reports, and news. - The Opinion QA test collection is built by crawling Stackexchange posts under the Investment topic, providing a substantial dataset for training and testing. Specific Aspects of Machine Learning Models Tested and Evaluated: - Aspect-Based Sentiment Analysis: The benchmark evaluates machine learning models' ability to detect target aspects in financial text and predict sentiment scores for each mentioned target. - Opinion-Based Question Answering: Machine learning models are tested on their capability to build question answering systems that can handle opinionated questions and provide answers based on mined opinions, aspects, sentiment polarity, and opinion holders.''',\n",
    "    'FinanceIQ': '''FinanceIQ is a Chinese financial domain knowledge assessment dataset. Main Goals of FinanceIQ: Targeted Domain: It focuses specifically on the financial sector, aiming to evaluate the knowledge and reasoning abilities of large language models in financial scenarios. Breadth of Coverage: The dataset encompasses a broad range of topics within finance, indicated by its inclusion of 10 major financial categories and 36 sub-categories. Comprehensive Assessment: With a total of 7,173 multiple-choice questions, FinanceIQ provides an extensive platform for testing. Defining Features: Language Specificity: The dataset is in Chinese, highlighting its utility for language models trained on or catering to Chinese language content, particularly in the financial domain. Detailed Categorization: The division into major and minor financial categories allows for nuanced assessment across various aspects of the financial industry. Format: The multiple-choice format of the questions simplifies the evaluation process and provides a clear metric for assessing model performance. Aspects of Machine Learning Models Tested: Knowledge Depth: The dataset tests for a deep understanding of financial concepts, terminologies, and practices. Reasoning Ability: It evaluates the model's capacity to apply financial knowledge in reasoning out the correct choices in multiple-choice questions. Contextual Understanding: Given the specificity of finance, the dataset assesses how well models can contextualize and interpret information within this domain. Language Processing: For models targeting Chinese language processing, this dataset serves as a benchmark for their linguistic capabilities in a specialized domain.''',\n",
    "    'Flickr30K': '''The Flickr30K benchmark dataset is designed for the task of image description generation, aiming to associate images with descriptive captions. The main goals of the Flickr30K benchmark include: 1. Image and Caption Pairing: The dataset pairs 31,000 images with descriptive captions, providing a rich replatform for training and evaluating models that aim to generate natural language descriptions for visual content. 2. Evaluation of Image Description Models: It serves as a platform for evaluating the effectiveness of machine learning models in generating accurate and meaningful descriptions for images. This includes assessing the ability of models to understand visual content and express it in natural language. Defining Features of Flickr30K Benchmark: 1. Large-scale Image-Caption Dataset: The dataset contains a substantial number of images (30,000+) paired with descriptive captions, providing a diverse and extensive replatform for training and evaluating image description generation models. 2. Annotator Diversity: Each image is independently described by five annotators, ensuring diverse perspectives and descriptions for the same visual content. This diversity allows for the exploration of denotational similarities between expressions that are not trivially related by syntactic rewrite rules. Specific Aspects of Machine Learning Models Tested and Evaluated: 1. Denotational Similarity: The dataset facilitates the evaluation of denotational similarities between images and their associated captions. This includes exploring the effectiveness of denotational similarities compared to compositional similarities computed on the dataset's own corpus. 2. Natural Language Understanding: Machine learning models are tested on their ability to understand visual content and generate coherent and semantically meaningful descriptions. This involves assessing the impact of denotational similarities with compositional similarities computed on the dataset's corpus. 3. Semantic Relatedness: The dataset enables the evaluation of semantic relatedness between images and their corresponding captions, allowing for the assessment of models' capabilities in capturing the semantic content of visual scenes and expressing it in natural language.''',\n",
    "    'Flores': '''The Flores benchmark is a dataset designed for evaluating machine translation between English and low-replatform languages. Due to the nature of new languages with less standardization, FLORES-200 required a more complex verification process and modifications in the translation workflow. Several languages in FLORES-200 were not translated from English but from Spanish, French, Russian, and Modern Standard Arabic. Defining Features Data Platform: The dataset consists of translations from 842 distinct web articles, totaling 3001 sentences. Sentence Length: On average, sentences are approximately 21 words long. Data Splits: The sentences are divided into three splits: dev, devtest, and test (hidden). Multilingual Support: It contains parallel sentences for 200 languages, identified with ISO 639-3 codes and additional script codes. Data Structure: Each data entry includes the sentence in specific languages, the URL of the platform article, domain, topic, and information about images and hyperlinks. Machine Learning Model Evaluation Multilingual Machine Translation: The primary task is to evaluate the performance of machine translation models across a wide range of languages, with a focus on low-replatform languages. Model Benchmarking: The dataset is used in the context of the WMT2021 shared task on Large-Scale Multilingual Machine Translation, as seen on the Dynabench leaderboard. Translation Quality: The complex nature of the languages and scripts in the dataset challenges models to maintain high translation quality across diverse linguistic contexts. Handling of Low-Replatform Languages: The dataset tests a model's ability to translate languages with limited available data, a significant challenge in machine translation.''',\n",
    "    'Flowers102': '''The Flowers benchmark is a significant dataset and evaluation framework designed to advance the field of automated flower classification. Here's a comprehensive overview of its main goals, defining features, and specific aspects of machine learning models that it aims to test and evaluate: 1. Main Goals: - The primary goal of the Flowers benchmark is to facilitate the development and evaluation of machine learning models for the task of flower classification. - It aims to address the challenges posed by a large number of classes within the flower category, the non-rigid nature of flowers leading to significant intra-class variation, and the need to capture diverse features such as color, shape, and patterns for accurate classification. - The benchmark seeks to push the boundaries of automated classification systems by providing a comprehensive dataset and evaluation framework that can handle the complexities of flower classification. 2. Defining Features: - The Flowers benchmark dataset typically consists of a large number of flower classes, often exceeding 100 classes, to represent the diverse range of flowers in the natural world. - The dataset includes images of flowers acquired under various uncontrolled image situations, such as varying scales, resolutions, lighting conditions, clutter, and image quality. This diversity reflects real-world scenarios and presents a significant challenge for classification models. - The benchmark emphasizes the need to capture and represent color, texture, and shape (both local and global) of the petals and their arrangement, as these are crucial features for accurate flower classification. 3. Specific Aspects of Machine Learning Models: - The Flowers benchmark aims to test and evaluate the performance of machine learning models in handling a large number of classes within the flower category, which presents a significant challenge due to the high similarity between classes and the intra-class variation. - It focuses on the ability of machine learning models to effectively capture and utilize diverse features such as color, shape, and patterns for accurate classification, highlighting the importance of feature representation and selection in the classification process. - The benchmark also evaluates the capability of machine learning models to handle non-rigid objects like flowers, which can deform in various ways, leading to significant shape variations within and across classes.''',\n",
    "    'FunQA': '''The FunQA benchmark aims to evaluate machine learning models' ability to comprehend and answer questions about video content. It contains 4,365 video clips and 311,950 question-answer pairs, with each video clip lasting an average of 19 seconds. The dataset is divided into three subsets, each containing well-designed tasks. The benchmark's defining feature is its focus on counter-intuitive video clips, which require models to understand and reason about unexpected events or situations. To evaluate models' performance, FunQA uses a variety of metrics, including accuracy, F1 score, and human evaluation. The benchmark also includes free-text answer evaluations, which demand enhanced video comprehension capabilities from the model and assess its ability to generate reasonably lengthy textual responses. To expand the dataset, the authors used GPT-3.5 to automatically translate Chinese annotations into English and generate more QA pairs that were faithful to the original ideas but presented differently. This not only made FunQA multilingual but also increased the number of QA pairs. Additionally, the authors created diverse task types, such as FunQA multiple-choice and FunQA dialogue.''',\n",
    "    'GAIA': '''GAIA is a benchmark for General AI Assistants that proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. The main goal of GAIA is to evaluate the capabilities of machine learning models in handling complex, open-ended questions that require a range of skills and knowledge. One of the defining features of GAIA is that it consists of text-based questions that are designed to admit a short, single correct answer, making them easy to verify. The questions cover various assistant use cases such as daily personal tasks, science, or general knowledge. GAIA is composed of 466 questions designed and annotated by humans, and sometimes come with a file (such as an image or a spreadsheet). GAIA attempts to circumvent different pitfalls of LLMs evaluation. For example, it prioritizes easiness of use, and does not evaluate the trace leading to the answer. Additionally, GAIA only evaluates the strongest available LLMs that have access to tools, hence are able to obtain informative scores. However, OpenAI’s API does not provide the detailed log of tool calls yet, which would be required for fine-grained analysis. Scoring perfectly on GAIA requires a varied set of fundamental abilities, including reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. The benchmark is designed to be convenient yet challenging for general AI assistants, and is conceptually simple for humans yet challenging for most advanced AIs. Human respondents obtain 92% vs. 15% for GPT-4 equipped with plugins, illustrating the notable performance disparity between humans and even the most advanced machine learning models. In summary, GAIA is a benchmark for General AI Assistants that aims to evaluate the capabilities of machine learning models in handling complex, open-ended questions that require a range of skills and knowledge. It consists of text-based questions that are designed to admit a short, single correct answer, and covers various assistant use cases such as daily personal tasks, science, or general knowledge.''',\n",
    "    'GAOKAO-Bench': '''The GAOKAO-Bench is a benchmark specifically tailored for evaluating artificial intelligence (AI) capabilities using questions from the Chinese college entrance examination, known as GAOKAO. The main goals of the GAOKAO-Bench are to provide a performance metric aligned with societal understanding and to offer a short-term measure of AI capabilities. Additionally, it aims to provide a benchmark with data of consistent quality, closely mirroring real-world conditions and featuring high-quality annotations, enabling researchers to scrutinize the shortcomings of model performance. The defining features of the GAOKAO-Bench include its comprehensive nature, covering questions from the GAOKAO examination from 2010 to 2022, and its inclusion of a wide range of question types, such as multiple-choice questions, fill-in-the-blank questions, and open-ended questions. The benchmark encompasses questions from various subjects, including Chinese, mathematics, English, physics, chemistry, biology, politics, history, and geography, with different versions of mathematics questions for liberal arts and science students. This diversity allows for a thorough evaluation of AI models across different domains and question types. The GAOKAO-Bench aims to test and evaluate the efficacy of large language models (LLMs) in more challenging and domain-specific tasks, particularly in the context of the Chinese college entrance examination. It focuses on assessing the performance of LLMs in handling questions that involve logical reasoning, calculation, knowledge quizzes, writing, and other types of questions. The benchmark also aims to analyze the gap and advantages of LLMs compared to humans in a reasonable and intuitive manner, providing valuable insights into the limitations of such models. In summary, the GAOKAO-Bench serves as an intuitive benchmark for evaluating the performance of LLMs on challenging and domain-specific tasks, providing a comprehensive evaluation framework that aligns with societal understanding and offers valuable insights into the capabilities and limitations of AI models.''',\n",
    "    'GQA': '''The GQA (Visual Genome Question Answering) benchmark is a new dataset for visual reasoning and compositional question answering. Its main goal is to drive research in the right direction towards deeper semantic understanding, sound reasoning, enhanced robustness, and improved consistency in visual question answering (VQA) systems. The defining features of GQA are its visual and semantic richness, its carefully refined question engine, and its suite of new metrics that complement the standard accuracy measure commonly used in assessing methods' performance. The dataset consists of 22,669,678 questions over 113,018 images, which cover a wide range of reasoning skills and vary in length and number of required inference-steps. The dataset has a vocabulary size of 3097 words and 1878 possible answers. The GQA question engine leverages content information about objects, attributes, and relations provided through Visual Genome Scene Graphs, along with structure, a newly-created extensive linguistic grammar that couples hundreds of structural patterns and detailed lexical semantic rePlatform. Together, they are combined in the engine to generate over 22 million novel and diverse questions, which all come with structured representations in the form of functional programs that specify their contents and semantics, and are visually grounded in the image scene graphs. The specific aspects of machine learning models that GQA aims to test and evaluate include deeper semantic understanding, sound reasoning, enhanced robustness, and improved consistency in visual question answering systems. GQA questions involve varied reasoning skills, and multi-step inference in particular. The associated semantic representations are used to greatly reduce biases within the dataset and control for its question type composition, downsampling it to create a 1.7M balanced dataset.''',\n",
    "    'GSM8K': '''The GSM8K benchmark is a dataset of 8.5K high-quality grade school math word problems designed to evaluate the informal reasoning ability of large language models. The main goals of GSM8K are to provide a challenging dataset that requires both interpreting questions and reasoning through the steps to solve them, and to assess the performance of machine learning models, particularly large language models, on linguistically diverse math problems. Key features of the GSM8K benchmark include: 1. Linguistic Diversity: The dataset is designed to have high linguistic diversity while relying on relatively simple grade school math concepts, making it challenging for state-of-the-art language models. 2. Problem Quality: The problems are created by human problem writers, ensuring high quality and avoiding error-prone scraping procedures. Extensive quality control is performed to maintain the accuracy of the dataset. 3. Moderate Difficulty: The problem distribution is challenging for large language models without being completely intractable. Problems require no concepts beyond the level of early Algebra, and the vast majority can be solved without explicitly defining a variable. Specific aspects of machine learning models that GSM8K aims to test and evaluate include: 1. Reasoning Ability: The dataset evaluates the informal reasoning ability of large language models, requiring them to interpret questions and reason through the steps to solve the math word problems. 2. Performance on Linguistically Diverse Problems: State-of-the-art language models struggle to achieve high performance on GSM8K due to the high diversity among problems. The dataset aims to assess the ability of models to handle linguistic diversity while solving math problems. 3. Data Scaling Trends: GSM8K helps in understanding the data scaling trends of different models and methods in the difficulty sweet spot, providing insights into how models perform with increased data and complexity.''',\n",
    "    'Gaokao': '''The Gaokao dataset is a collection of questions from the Chinese College Entrance Exam, which is a high-stakes standardized test taken by millions of students in China each year. The main goal of including the Gaokao dataset in the AGIEval benchmark is to evaluate the performance of large language models in tasks that are directly relevant to human decision-making and problem-solving in the context of this important exam. The defining feature of the Gaokao dataset is its emphasis on testing a wide range of cognitive abilities, including language comprehension, mathematical reasoning, and critical thinking. The dataset includes questions from a variety of subjects, including Chinese language and literature, mathematics, English, physics, chemistry, and biology. The questions are designed to assess the test-taker's understanding, knowledge, and reasoning abilities in each subject area. The specific aspects of machine learning models that the Gaokao dataset aims to test and evaluate include their ability to comprehend and reason with natural language, their ability to solve complex mathematical problems, and their ability to reason with scientific concepts. The dataset includes a mix of multiple-choice and fill-in-the-blank questions, as well as questions that require the test-taker to write short essays or solve mathematical problems.''',\n",
    "    'GovReport': '''The GovReport benchmark is a large-scale dataset designed to evaluate the performance of machine learning models in generating abstractive summaries of long documents, such as government reports and scientific papers. The main goal of the benchmark is to provide a challenging and realistic testbed for summarization models that can handle long documents with multiple paragraphs and complex structures. The defining features of the GovReport benchmark include the use of human-written summaries as references, the inclusion of long documents with multiple sections and paragraphs, and the requirement to generate summaries that capture the main aspects of the original documents. Specifically, the benchmark requires models to generate summaries that accurately reflect the three main aspects of government reports: \"Why GAO did this study\", \"What GAO found\", and \"What GAO recommends\". For scientific papers, summaries should cover four aspects: \"Introduction and Literature\", \"Methods\", \"Results\", and \"Discussion and Conclusion\". The GovReport benchmark evaluates the performance of machine learning models in several aspects, including the ability to generate informative and faithful summaries, the ability to handle long documents with complex structures, and the ability to capture the main aspects of the original documents. To this end, the benchmark provides a new evaluation metric for faithfulness, called APES src, which compares QA answers by reading the platform and the system summary. The benchmark also uses the ROUGE metric to evaluate the quality of the generated summaries. Overall, the GovReport benchmark provides a challenging and realistic testbed for summarization models that can handle long documents with complex structures and accurately capture the main aspects of the original documents.''',\n",
    "    'HHH': '''The HHH benchmark is a critical component of the research on creating a text-based assistant aligned with human values. Here's a comprehensive description of the HHH benchmark, including its main goals, defining features, and the specific aspects of machine learning models it aims to test and evaluate: Main Goals: The primary goal of the HHH benchmark is to measure and address the general problem of alignment with large language models. Alignment, in this context, refers to the degree of overlap between the way two agents rank different outcomes. The benchmark aims to evaluate the alignment of AI systems with human preferences and values, specifically focusing on the criteria of helpfulness, honesty, and harmlessness (HHH). Defining Features: 1. Helpful: The benchmark assesses the AI's ability to perform tasks or answer questions posed by humans in a clear, concise, and efficient manner, while avoiding harmful activities. 2. Honest: It evaluates the AI's honesty in providing accurate and truthful information in its responses to human queries. 3. Harmless: The benchmark examines the AI's behavior to ensure it avoids causing harm to individuals or groups, aligning with human values and ethical considerations. Specific Aspects of Machine Learning Models Tested and Evaluated: The HHH benchmark focuses on evaluating the performance of machine learning models, particularly large language models, in the context of alignment with human values. Specific aspects of machine learning models that it aims to test and evaluate include: - Model Preferences: The benchmark assesses the preferences of machine learning models by measuring the frequency with which contractors view a given model as more helpful and honest in head-to-head tests involving pairs of models. - Human Feedback: It incorporates human feedback collection to align general-purpose AI systems with human preferences and values, aiming to improve the performance of language models based on human feedback. - Security: While the primary focus is on alignment with human values, the benchmark also considers the security aspects of AI systems, aiming to ensure that AI systems are secure and have properties that decrease the potential for harm, even if the AI is less than fully aligned with human values. In summary, the HHH benchmark serves as a comprehensive evaluation framework for assessing the alignment of AI systems with human preferences and values, with a specific focus on helpfulness, honesty, and harmlessness.''',\n",
    "    'Hallucination Leaderboard': '''The Hallucination Leaderboard is a public leaderboard that evaluates Large Language Models (LLMs) based on their propensity to produce hallucinations when summarizing short documents. Key features of the Hallucination Leaderboard include: Model Accuracy and Hallucination Rate: The leaderboard ranks various models like GPT-4, GPT-3.5, Llama, Cohere, Anthropic Claude, and others based on their accuracy in avoiding hallucinations and their hallucination rate when summarizing documents​​. Open Platformd Model: The model used for the leaderboard is open-platformd for commercial use, available on Hugging Face, allowing others to replicate or build upon the work​​. Based on Prior Research: The methodology incorporates protocols from significant prior research in factual consistency in summarization, referencing several top papers and rePlatform in this field​​. Methodology: The leaderboard is established by training a model to detect hallucinations in LLM outputs using various open-platform datasets. It involves feeding 1000 short documents to each LLM and asking them to summarize using only the facts presented in the document​​.''',\n",
    "    'HallusionBench': '''HallusionBench is a comprehensive benchmark designed to evaluate the image-context reasoning capabilities of large visual-language models (LVLMs) such as GPT-4V(ision) and LLaVA-1.5. The benchmark comprises 346 images paired with 1129 questions, all meticulously crafted by human experts. The main goal of HallusionBench is to bridge the gap between existing benchmarks and HallusionBench in hallucination evaluation, as existing benchmarks primarily focus on object hallucinations, limited topics, and visual input types. HallusionBench aims to provide more topics, more image types, and more visual input modalities, including both images and videos. Additionally, HallusionBench is the first benchmark to include human-edited images to assess the robustness of LVLMs. HallusionBench is unique in that it is the first human-annotated analytical benchmark focusing on diagnosing both the visual illusion and knowledge hallucination of LVLMs. It is designed to examine visual illusion and knowledge hallucination of LVLMs and analyze the potential failure modes based on each hand-crafted example pair. The benchmark consists of 455 visual-question control pairs, including 346 different figures and a total of 1129 questions on diverse topics (including food, math, geometry, statistics, geography, sports, cartoon, famous illusions, movie, meme, etc.) and formats (including logo, poster, figure, charts, table, map, consecutive images, etc.).''',\n",
    "    'HellaSwag': '''HellaSwag is a benchmark dataset designed to challenge state-of-the-art machine learning models in the domain of commonsense reasoning and natural language understanding. The main goal of HellaSwag is to provide a rigorous evaluation of the ability of machine learning models to perform commonsense inference, particularly in situations where the correct answer may not be immediately obvious. The dataset is constructed to be trivial for humans (with approximately 95% accuracy) but significantly challenging for state-of-the-art models (achieving only around 48% accuracy). Key features and aspects of HellaSwag include: 1. Adversarial Filtering (AF): HellaSwag is constructed using Adversarial Filtering, a data collection paradigm in which a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. This approach aims to scale up the length and complexity of dataset examples towards a critical 'Goldilocks' zone, where generated text is ridiculous to humans but often misclassified by state-of-the-art models. 2. Complexity and Ridiculousness: The dataset examples are designed to be complex enough that state-of-the-art generators often make mistakes, while remaining simple enough that discriminators often fail to catch them. This creates a challenging environment for machine learning models, as they must navigate a \"Goldilocks zone\" of complexity and ridiculousness in the dataset examples. 3. Evaluation of Language Models: HellaSwag evaluates the performance of language models, including BERT-Large and OpenAI GPT, in the context of commonsense reasoning. The dataset provides a rigorous test for these models, highlighting their limitations in understanding and reasoning about commonsense scenarios. 4. Benchmark Evolution: HellaSwag suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, presenting ever-harder challenges to machine learning models. This approach aims to drive progress in the field of natural language understanding by continually pushing the boundaries of model capabilities.''',\n",
    "    'HalluQA': '''The HalluQA benchmark is a Chinese hallucination question-answering dataset designed to evaluate hallucinations in Chinese large language models (LLMs). The main goal of HalluQA is to help reduce hallucination problems in Chinese LLMs and enhance the credibility of these models. The defining features of HalluQA include its meticulous design, which takes into account Chinese historical culture, customs, and social phenomena, and its focus on two types of hallucinations: imitative falsehoods and factual errors. The dataset contains 450 adversarial questions spanning multiple domains, and an automated evaluation method is used to judge whether a model output is hallucinated. The specific aspects of machine learning models that HalluQA aims to test and evaluate include pre-trained models, chat models, and retrieval-augmented chat models. The primary types of hallucinations in different types of models are analyzed, and the paper discusses which types of hallucinations should be prioritized for different types of models.''',\n",
    "    'Historical Figures': '''The Historical Figures dataset is one of the 62 scenarios covered in the HEIM benchmark. Its main goal is to evaluate the ability of text-to-image models to generate accurate and diverse images of historical figures based on textual prompts. The dataset consists of 99 prompts, each representing a different historical figure, such as Napoleon Bonaparte, Albert Einstein, and Cleopatra. The prompts follow the format of \"X\", where X represents the name of the historical figure. The list of historical figures is platformd from TIME’s compilation of The 100 Most Significant Figures in History. The defining feature of the Historical Figures dataset is its focus on evaluating the ability of text-to-image models to generate images that accurately represent the physical appearance and characteristics of historical figures. This requires the models to have a deep understanding of the historical context and cultural significance of each figure, as well as the ability to generate diverse and realistic images that capture the unique features of each individual. The specific aspects of machine learning models that the Historical Figures dataset aims to test and evaluate include their ability to: - Understand and interpret textual prompts related to historical figures. - Generate diverse and realistic images that accurately represent the physical appearance and characteristics of historical figures. - Capture the unique features and cultural significance of each historical figure in their generated images. - Handle variations in the textual prompts, such as misspellings or variations in the wording of the prompts.''',\n",
    "    'HotpotQA': '''HOTPOTQA is a large-scale question answering dataset that aims to facilitate the development of QA systems capable of performing explainable, multi-hop reasoning over diverse natural language. Here are some key details about the dataset: Main Goals: - To provide a diverse and explainable question answering dataset that requires multi-hop reasoning - To test systems' ability to extract and compare various entity properties in text - To provide strong supervision for reasoning and explanation Defining Features: - Questions require finding and reasoning over multiple supporting documents to answer - Questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas - Sentence-level supporting facts are provided, allowing QA systems to reason with strong supervision and explain the predictions - A new type of factoid comparison questions is offered to test QA systems' ability to extract relevant facts and perform necessary comparison Machine Learning Models Tested and Evaluated: - Models that can perform complex reasoning over multiple documents - Models that can extract and compare various entity properties in text - Models that can make explainable predictions by utilizing the provided supporting facts.''',\n",
    "    'HumanEval': '''HumanEval dataset aims to evaluate the functional correctness and problem-solving capabilities of machine learning models trained on code. It consists of 164 hand-written programming problems, each with a function signature, docstring, body, and several unit tests, with an average of 7.7 tests per problem. The tasks in the HumanEval dataset assess language comprehension, reasoning, algorithms, and simple mathematics. It is important for these tasks to be hand-written since the models are trained on a large fraction of GitHub, which already contains solutions to problems from a variety of Platform. The main goal of the HumanEval dataset is to evaluate the functional correctness of machine learning models trained on code. The dataset is designed to test the problem-solving capabilities of these models and assess their ability to comprehend and reason about code. The dataset is also released to the public so that others can evaluate the functional correctness and measure the problem-solving capabilities of their models. In terms of specific aspects of machine learning models that the HumanEval dataset aims to test and evaluate, it assesses the models' ability to comprehend and reason about code, as well as their problem-solving capabilities. The dataset is designed to evaluate the functional correctness of these models and measure their ability to solve programming problems.''',\n",
    "    'iFLYTEK': '''The iFLYTEK Challenge 2021 Cultivated Land Extraction from High-Resolution Remote Sensing Image is a benchmark that focuses on the accurate extraction of cultivated land from high-resolution remote sensing images. The main goals of the challenge include promoting the application of digital agriculture in rural areas by developing highly effective and efficient pipelines for cultivated land extraction. The challenge requires participants to segment cultivated land objects in very high-resolution multispectral remote sensing images, addressing the specific needs of precision agriculture. Key aspects of the iFLYTEK benchmark include: 1. Precision Agriculture Focus: The benchmark is tailored to the needs of precision agriculture, aiming to enable accurate extraction of cultivated land from high-resolution remote sensing images. This aligns with the broader goal of leveraging technology for improved agricultural practices. 2. High-Resolution Remote Sensing Images: The challenge involves working with very high-resolution multispectral remote sensing images, which presents unique challenges related to data processing, feature extraction, and model scalability. 3. Instance Segmentation: The benchmark specifically focuses on instance segmentation, which involves identifying and delineating individual objects within the images. This requires the development of machine learning models capable of accurately detecting and segmenting cultivated land objects. 4. Model Pre-Training and Enhancement Techniques: Participants are encouraged to explore model pre-training and techniques for enhancing the backbone of the machine learning models. This includes experimenting with different backbones, training times, and techniques such as DCN (Deformable ConvNets) to improve model performance. 5. Efficiency and Effectiveness: The benchmark emphasizes the development of highly effective and efficient pipelines for cultivated land extraction. This involves optimizing model performance while considering computational efficiency and practical applicability in real-world agricultural settings. Overall, the iFLYTEK benchmark serves as a platform for evaluating and advancing machine learning models and techniques specifically tailored to the task of cultivated land extraction from high-resolution remote sensing images.''',\n",
    "    'I2P': '''The Inappropriate Image Prompts (I2P) benchmark is a dataset of over 4.5k real-world text prompts for generative models that are likely to produce inappropriate content. The main goal of I2P is to evaluate the degree to which a model generates images that represent the concepts of hate, harassment, violence, self-harm, sexual content, shocking images, and illegal activity. The dataset was created to fill a gap in the evaluation of text-to-image models, specifically in measuring inappropriate degeneration. I2P is inspired by REALTOXICITYPROMPTS, which is a benchmark for measuring toxic degeneration in language models. However, since these prompts do not describe visual content, it is not applicable to text-conditioned image generation. I2P's defining features include its focus on inappropriate content, which is defined based on the work of Gebru et al. as data that \"if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety.\" The dataset covers a wide range of inappropriate content beyond nudity and is based on 26 keywords and phrases describing the seven concepts mentioned above in more detail. The specific aspects of machine learning models that I2P aims to test and evaluate are their ability to generate appropriate images based on text prompts. The dataset and evaluation pipeline are publicly available for other researchers to use, and the inappropriate content is automatically evaluated by combining two classifiers. The Q16 classifier is able to detect a wide range of inappropriate content in images, while the SMID dataset is used to annotate images on their appropriateness through user studies conducted in the USA. Overall, I2P is a valuable benchmark for evaluating the performance of text-to-image models in generating appropriate content and avoiding inappropriate degeneration.''',\n",
    "    'IC (2015)': '''The ICDAR 2015 Text Reading in the Wild (TRW 2015) competition aimed to establish a benchmark for assessing detection and recognition algorithms designed for both Chinese and English scripts in natural scenes. The competition sought to provide a playground for researchers to explore and develop methods for multilingual text understanding in complex real-world scenarios. Here's a comprehensive description of the IC15 benchmark: 1. Main Goals: - Establish a benchmark for assessing detection and recognition algorithms for both Chinese and English scripts in natural scenes. - Provide a platform for researchers to develop and evaluate methods for multilingual text understanding in complex real-world scenarios. - Encourage the development of algorithms that can handle multilingual texts in the wild, addressing the limitations of previous systems and datasets that primarily focused on English or Latin-root languages. - Generate a large-scale text image database and propose text detection and recognition tasks with corresponding evaluation methods. 2. Defining Features: - Multilingual Text Understanding: The competition focused on addressing the limitations of existing systems and datasets by emphasizing the need for methods that can handle multilingual texts in the wild, including languages such as Chinese, Kannada, Thai, and Hebrew. - Real-World Complexity: The dataset included diverse and challenging real-world natural images from different Platform, with a focus on scenes captured or generated by non-professionals. This complexity aimed to better match the challenges encountered in real applications. - Large-Scale Text Image Database: The competition generated a large-scale text image database, providing a rich replatform for evaluating algorithms designed for multilingual text detection and recognition in complex natural scenes. 3. Specific Aspects of Machine Learning Models: - Scene Text Detection: The competition evaluated algorithms for accurately localizing text in natural scenes, including the detection of texts of arbitrary orientations and the utilization of background elements from large amounts of data. - Scene Text Recognition: The competition assessed methods for recognizing text in natural scenes, including lexicon-free recognition with statistical language models and the correction of letter cases based on the results of text line recognition. - Deep Learning Frameworks: The competition emphasized the potential of deep learning frameworks for making full use of the characteristics of scene text and background elements from large amounts of data, highlighting the role of deep features for text spotting.''',\n",
    "    'ICE': '''The International Corpus of English (ICE) is a dataset designed for comparative analysis of regional and national varieties of English across the world. The main goal of ICE is to provide a comprehensive and diverse collection of English language data from different regions and countries, allowing for the study and comparison of how English is used and understood in various linguistic and cultural contexts. Key features of the ICE dataset include: 1. Regional and National Varieties: ICE covers a wide range of English varieties from different regions and countries, including Canada, Jamaica, Kenya, Hong Kong, India, Ireland, Singapore, the Philippines, Tanzania, and the USA. This diverse representation allows for the examination of linguistic differences and similarities across these varieties. 2. Representative Grouping: The selection of English varieties in ICE is based on the dominant classifications of world Englishes, such as the Three Circles model and the Dynamic model. This ensures that the dataset provides a representative grouping of English varieties, capturing the linguistic diversity within the English language. 3. Comparative Analysis: ICE facilitates comparative analysis of the linguistic features and usage patterns across different regional and national varieties of English. This comparative approach enables researchers to explore how language models perform in understanding and generating text in diverse linguistic contexts. In the context of machine learning models, ICE aims to test and evaluate the language modeling capacity of these models with respect to the following aspects: - Performance Disparities: ICE allows for the measurement of performance disparities in a model's understanding of different English varieties. This includes assessing how well language models capture the linguistic nuances and variations present in regional and national varieties of English. - Sociolinguistic Implications: ICE considers the sociolinguistic implications of language technology performance, particularly in relation to historically marginalized or underrepresented linguistic groups. The dataset enables the evaluation of language models in terms of their potential impact on reinforcing stigmatization or perpetuating disparities in linguistic representation. - Cross-Cultural Understanding: By encompassing a diverse set of English varieties, ICE provides a platform for assessing the cross-cultural understanding and adaptability of language models. It allows for the examination of how well models can comprehend and generate text in different cultural and linguistic contexts. Overall, the ICE dataset serves as a valuable replatform for evaluating language models in their capacity to understand and generate text across diverse regional and national varieties of English, while also considering the sociolinguistic implications of model performance.''',\n",
    "    'IIIT5K': '''The IIIT 5K-word dataset is a benchmark for scene text recognition that was introduced in the paper \"Scene Text Recognition using Higher Order Language Priors\" by Mishra et al. The dataset contains 5000 word images, which are a mix of street scene texts and born-digital images. The main goals of the dataset are to provide a challenging benchmark for scene text recognition and to evaluate the performance of character detection modules. The defining features of the IIIT 5K-word dataset are its large size and the wide variety of fonts, styles, viewpoints, backgrounds, and contrasts present in the images. The dataset is much larger than other public datasets like SVT and ICDAR 2003, and it is more challenging due to the presence of varying illuminations and projective distortions. The born-digital images in the dataset are inherently low-resolution and have a variety of font sizes and styles, while the scene texts are already considered challenging for recognition. The IIIT 5K-word dataset aims to test and evaluate the performance of machine learning models for scene text recognition, specifically those that use a higher order prior computed from an English dictionary to infer the word. The paper presents a CRF framework that incorporates higher order statistical language models to infer the word, allowing the use of edit distance based measures to be bypassed. The framework jointly infers the detections representing characters and the word they form as a whole, and shows results on the IIIT 5K-word dataset with a wide variety of variations in terms of fonts, style, view-point, background, and contrast. In summary, the IIIT 5K-word dataset is a challenging benchmark for scene text recognition that aims to evaluate the performance of machine learning models using a higher order prior computed from an English dictionary to infer the word.''',\n",
    "    'IMDB': '''The IMDB benchmark is one of the 16 core scenarios evaluated in the HELM benchmark. Its main goal is to evaluate the performance of language models in sentiment analysis tasks, specifically in predicting the sentiment of movie reviews from the Internet Movie Database (IMDB). The IMDB benchmark is designed to comprehensively measure 7 metrics: accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency. Accuracy measures how well the model predicts the correct sentiment label for a given review. Calibration measures how well the model's predicted probabilities match the true probabilities of the labels. Robustness measures how well the model performs when the input data is perturbed or modified. Fairness measures how well the model performs across different demographic groups. Bias measures how well the model performs across different subgroups of the data. Toxicity measures how well the model avoids generating toxic or harmful content. Efficiency measures how quickly the model can make predictions. The IMDB benchmark uses a dataset of movie reviews from the IMDB website, which is split into training, validation, and test sets. The models are trained on the training set and evaluated on the test set. The benchmark also includes a set of contrast sets, which are designed to highlight the limitations of the models. These contrast sets are specifically designed to test the robustness of the models by introducing perturbations to the input data.''',\n",
    "    'IMPACT': '''The IMPACT benchmark dataset is a crucial component of the INSTRUCTEVAL evaluation suite, designed to assess the performance of instruction-tuned large language models (LLMs) in generating text that aligns with human values and ethical principles. The main goals of the IMPACT dataset are to evaluate the models' ability to produce outputs that demonstrate ethical reasoning, fairness, and alignment with human values across a variety of prompts and scenarios. The defining features of the IMPACT dataset include its focus on evaluating the ethical and value-based reasoning capabilities of LLMs, its inclusion of prompts that require models to generate text consistent with ethical principles, and its use of automatic evaluation methods to assess the quality of the model outputs. The dataset aims to test and evaluate the models' performance in generating text that reflects ethical considerations, fairness, and alignment with human values, thereby providing insights into the models' ethical reasoning capabilities. Specific aspects of machine learning models that the IMPACT dataset aims to test and evaluate include their ability to generate text that demonstrates ethical reasoning, fairness, and alignment with human values across a range of prompts and scenarios. The dataset includes prompts that cover various ethical dilemmas, value-based decision-making, and scenarios that require the models to produce text that reflects ethical considerations and human values. Overall, the IMPACT benchmark dataset serves as a critical component of the INSTRUCTEVAL evaluation suite, providing a means to assess the ethical reasoning capabilities of instruction-tuned LLMs and to evaluate their performance in generating text that aligns with human values and ethical principles.''',\n",
    "    'IWSLT (2017)': '''The International Workshop on Spoken Language Translation (IWSLT) benchmark is an annual evaluation campaign that focuses on the field of machine translation (MT) and automatic speech recognition (ASR). The main goals of the IWSLT benchmark include providing challenging tasks for researchers in the MT and ASR fields, offering a venue for presenting, comparing, and discussing results, and fostering a collaborative environment for scientific exchange. The benchmark aims to address various open issues in MT and spoken language translation, and it has evolved over the years to reflect the trends and progress in the field of MT and ASR. The IWSLT benchmark is characterized by several defining features, including the following: 1. Task Variety: The benchmark offers a variety of tasks that cover different aspects of MT and ASR, such as translation of TED talks, human-to-human dialogues, and real-life university lectures. 2. Translation Directions: Over the years, the benchmark has covered a wide range of translation directions, including many American, European, and Asian languages. 3. Shared Tasks: IWSLT organizes shared tasks in which all the training data, experimental conditions, and evaluation metrics are set and provided in advance, facilitating scientific communication and collaboration. 4. Multilingual Focus: The benchmark includes a Multilingual task that investigates the possibility of machines to simultaneously learn to translate across multiple languages, given parallel data that only partially covers the tested translation directions. 5. Context Consideration: The Dialogue task targets the challenge for MT to consider the context of the input in order to resolve the translation of pronouns and other empty categories that typically occur in human-human dialogue turns. 6. Real-Life Challenges: The Lecture task addresses the challenge of automatically transcribing and translating real-life university lectures, which differ from staged and well-rehearsed talks. In terms of machine learning models, the IWSLT benchmark aims to test and evaluate the following specific aspects: 1. Multilingual Translation: The benchmark evaluates machine translation systems handling many-to-many language directions, including zero-shot directions, to assess their ability to translate across multiple languages. 2. Context Integration: For the Dialogue task, the benchmark focuses on the integration of context information in machine translation to resolve anaphoric references and empty categories in human-human dialogues. 3. Real-Life Transcription and Translation: The benchmark challenges participants to automatically transcribe and translate real-life university lectures, addressing the complexities of spoken language and the need for accurate translation in educational settings.''',\n",
    "    'ImageNet-1K': '''ImageNet is a large-scale hierarchical image database designed to address the challenges of organizing and harnessing the vast amount of image data available on the internet. The main goals of ImageNet include providing a comprehensive and diverse coverage of the image world, offering accurate and clean annotations, and organizing images based on the semantic hierarchy of WordNet. The database aims to contain tens of millions of annotated images organized by the semantic hierarchy of WordNet, with an average of 500-1000 clean and full-resolution images per synset. Key features of ImageNet include: 1. Scale: ImageNet aims to provide a large-scale and diverse coverage of the image world, with a goal of containing tens of millions of annotated images. 2. Accuracy: The database offers high precision, with an average of 99.7% precision achieved for each synset. 3. Diversity: ImageNet aims to capture the diversity of the image world by including images with various viewpoints, poses, background clutter, and occlusions. 4. Hierarchical Structure: ImageNet is built upon the hierarchical structure provided by WordNet, with the aim of organizing images based on the semantic hierarchy. In terms of machine learning model evaluation, ImageNet serves as a benchmark dataset for testing and evaluating object recognition, image classification, and automatic object clustering algorithms. It offers a platform for researchers to develop and evaluate sophisticated and robust models and algorithms for indexing, retrieving, organizing, and interacting with images and multimedia data. Specific aspects of machine learning models that ImageNet aims to test and evaluate include: 1. Object Recognition: ImageNet provides a large number of images for nearly all object classes, including rare ones, making it suitable for testing object recognition algorithms. 2. Image Classification: The database offers a diverse and large-scale collection of images, making it a challenging benchmark dataset for image classification research. 3. Automatic Object Clustering: ImageNet's hierarchical structure and dense coverage of the image world provide opportunities for testing automatic object clustering algorithms.''',\n",
    "    'ImageNetVC': '''The ImageNetVC benchmark is designed to evaluate the visual commonsense knowledge of both unimodal Large Language Models (LLMs) and Visually-augmented Language Models (VaLMs) across 1,000 ImageNet categories. The main goals, defining features, and specific aspects of machine learning models that it aims to test and evaluate are as follows: 1. Main Goals: - Evaluate the visual commonsense capabilities of LLMs and VaLMs. - Analyze the factors affecting the visual commonsense knowledge of large-scale models. - Provide insights into the development of language models enriched with visual commonsense knowledge. 2. Defining Features: - Human-Annotated Dataset: IMAGENETVC is a human-annotated dataset specifically designed for zero- and few-shot visual commonsense evaluation across 1,000 ImageNet categories. - Multi-Step Crowd-Sourcing Pipeline: The dataset construction involves annotator training, commonsense annotation, and cross-check examination, ensuring high-quality annotations. - Diverse Image Platform: The dataset is constructed from diverse real images associated with human-annotated descriptions, providing a representative image platform for evaluation. - Multi-Modal Evaluation: IMAGENETVC supports both textual and visual inputs, enabling the evaluation of both unimodal LLMs and VaLMs. 3. Specific Aspects of Machine Learning Models Tested and Evaluated: - Visual Commonsense Capability: The benchmark assesses the visual commonsense capabilities of various LLMs such as OPT, LLaMA, and Falcon, as well as the effect of visual grounding in VaLMs with multiple Platform of relevant images. - Model Scale: The benchmark analyzes the impact of model scale on visual commonsense capabilities, providing insights into the performance of models at different scales. - In-Context Learning: The evaluation explores the impact of in-context learning on the understanding of visual commonsense tasks for both LLMs and VaLMs, including its effect on reducing variance across prompts and calibrating model confidence on visual commonsense. - Emergent Abilities of LLMs: The benchmark investigates the emergent ability of LLMs to acquire visual commonsense, highlighting potential thresholds for unimodal LLMs to emerge with visual commonsense on the component.''',\n",
    "    'InterCode': '''The InterCode benchmark aims to standardize and evaluate interactive code generation methods that use execution feedback. Its main goals are to provide a flexible, universal framework for interactive coding, and to enable comprehensive evaluation of state-of-the-art models. InterCode is unique in several ways. First, it is language/platform agnostic, meaning that it can be used with any programming language or platform. Second, it uses self-contained Docker environments to ensure reproducibility and safety. Third, it provides a variety of feedback types, including execution feedback, to enable testing of a wide range of interactive coding methods. The benchmark formalizes interactive coding with execution feedback as a partially observable Markov decision process (POMDP) with instruction space U, state space S, action space A, observation space O, transition function T, and reward function R. The specific aspects of machine learning models that InterCode aims to test and evaluate include their ability to handle partial observability, their ability to reason about the effects of actions, and their ability to incorporate feedback from multiple Platform.''',\n",
    "    'JEC-QA': '''The JEC-QA benchmark is a legal-domain question answering dataset that was collected from the National Judicial Examination of China. Its main goal is to provide a challenging task for existing question answering methods, as it requires both retrieving relevant materials and logical reasoning to answer questions. The defining features of JEC-QA are its complexity and rigor. The cases and questions in the legal domain are very complex and require multiple reasoning abilities. Most questions in LQA can be divided into two typical types: the knowledge-driven questions (KD-questions) and case-analysis questions (CA-questions). KD-questions focus on the understanding of specific legal concepts, while CA-questions concentrate more on the analysis of real cases. Both types of questions require multiple reasoning abilities in both retrieving and answering, making JEC-QA a challenging task. JEC-QA aims to test and evaluate the specific aspects of machine learning models that are necessary for legal question answering. These include: 1. Retrieval of relevant materials: The models must be able to retrieve relevant legal materials from a large database of legal provisions and other Platform. 2. Understanding of legal concepts: The models must be able to understand legal concepts and terminology in order to answer knowledge-driven questions. 3. Analysis of real cases: The models must be able to analyze real cases and apply legal concepts to specific scenarios in order to answer case-analysis questions. 4. Multi-paragraph reading: The models must be able to read and understand multiple paragraphs of legal text in order to answer questions that require information from multiple Platform. 5. Multi-hop reasoning: The models must be able to combine information from multiple Platform and use logical reasoning to arrive at the correct answer. 6. Numerical analysis: The models must be able to perform numerical analysis in order to compare the severity of different crimes or legal violations.''',\n",
    "    'KQA Pro': '''The KQA Pro benchmark is designed to evaluate the performance of machine learning models on complex question answering over knowledge base (Complex KBQA). Its main goals are to provide a diverse and challenging dataset for KBQA and semantic parsing tasks, and to encourage further research in this field. The defining features of KQA Pro include its large size (over 120K diverse questions), its coverage of multiple types of knowledge (relations, attributes, and qualifiers), and its explicit compositional programs for each question. The dataset is split into train, validation, and test sets, with about 30% of the answers in the test set not seen in training. KQA Pro aims to test and evaluate the ability of machine learning models to perform complex reasoning tasks, such as multi-hop reasoning, value comparisons, set operations, and more. It also evaluates models' language understanding abilities, as the questions in KQA Pro are diverse and natural language-based. To evaluate the performance of machine learning models on KQA Pro, the benchmark includes a thorough experimental setup, where state-of-the-art KBQA models are reproduced and evaluated on KQA Pro. The results show that KQA Pro is challenging and that current models struggle with certain types of reasoning tasks, such as comparisonal reasoning for lacking of literal knowledge.''',\n",
    "    'KoRC': '''The KORC (Knowledge oriented Reading Comprehension) benchmark is designed to address the limitations of previous benchmarks in deep text understanding. Its main goals, defining features, and aspects of machine learning models it aims to test and evaluate are as follows: 1. Main Goals: - To facilitate deep text understanding by integrating text information with relevant background knowledge. - To provide a benchmark that requires connections between a given document and prior knowledge beyond its text. - To enable the evaluation of machine learning models' ability to reason across both the text and the background knowledge to reach the final answers. 2. Defining Features: - Broad Knowledge Coverage: KORC utilizes off-the-shelf knowledge bases as background knowledge Platform, ensuring broad knowledge coverage without the need for manual knowledge annotation. - Flexible Answer Format: The benchmark uses labels in knowledge bases as the final answers, rather than choices or spans from the text. Additionally, questions in KORC have an indeterminant number of answers, allowing for a more flexible answer space. 3. Aspects of Machine Learning Models Tested and Evaluated: - Knowledge Reasoning: KORC evaluates the ability of machine learning models to connect the document with equipped prior knowledge and reason across both the text and the background knowledge to reach the final answers. - Question Generation: The benchmark demonstrates the feasibility of guiding large language models (LLMs) to generate deep text understanding questions with the help of large-scale background knowledge. - Answering Format: Machine learning models are tested on their ability to provide answers in the form of labels in knowledge bases, which requires a deeper understanding of the text and background knowledge.''',\n",
    "    'Ko-ARC': '''AI2 Reasoning Challenge (ARC) benchmark aims to encourage progress in AI research on questions that require reasoning, use of commonsense knowledge, and other advanced methods for deeper text comprehension. The benchmark consists of a collection of 7787 natural science questions, authored for use on standardized tests, which have varying levels of difficulty and involve a wide variety of linguistic and inferential phenomena. The defining feature of the ARC benchmark is its partitioning into a Challenge Set and an Easy Set. The Challenge Set contains 2590 questions that are hard to answer with simple baselines, as they are answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm. The Easy Set contains the remaining 5197 questions, which are easier to answer and can be solved by simple AI methods. The specific aspects of machine learning models that the ARC benchmark aims to test and evaluate are their ability to reason, use commonsense knowledge, and comprehend text at a deeper level. The benchmark requires models to go beyond surface-level cues and use advanced methods to answer questions that involve complex linguistic and inferential phenomena.''',\n",
    "    'Ko-CommonGen': '''The CommonGen benchmark is a constrained text generation challenge designed to evaluate machines' ability to perform generative commonsense reasoning. The main goals of CommonGen are to assess and confer generative machine commonsense reasoning ability, as well as to push the boundaries of what machines can do in terms of relational reasoning and compositional generalization. Key features of the CommonGen benchmark include: 1. Constrained Text Generation: The task involves generating coherent sentences describing everyday scenarios using a given set of common concepts. For example, given the concepts {dog, frisbee, catch, throw}, the machine is tasked with generating a sentence like \"a man throws a frisbee and his dog catches it\". 2. Large Dataset: The benchmark dataset consists of 79k commonsense descriptions over 35k unique concept-sets, providing a substantial amount of data for training and evaluation. 3. Relational Reasoning and Compositional Generalization: The task inherently requires machines to perform relational reasoning with background commonsense knowledge and exhibit compositional generalization ability to work on unseen concept combinations. Specific aspects of machine learning models that CommonGen aims to test and evaluate include: 1. Generative Commonsense Reasoning: The benchmark evaluates the ability of machine learning models to generate coherent and plausible sentences based on commonsense knowledge, reflecting a deeper understanding of everyday scenarios. 2. Transfer Learning: The paper discusses the potential benefits of CommonGen-generated contexts for downstream tasks, indicating that the benchmark also aims to assess the transferability of generative commonsense reasoning capability to improve other natural language processing tasks. 3. Performance Metrics: CommonGen employs a range of automatic evaluation metrics such as BLEU, ROUGE, METEOR, CIDEr, and SPICE to assess the performance of state-of-the-art language generation models, providing a comprehensive evaluation of their capabilities.''',\n",
    "    'Ko-HellaSwag': '''HellaSwag is a benchmark dataset designed to challenge state-of-the-art machine learning models in the domain of commonsense reasoning and natural language understanding. The main goal of HellaSwag is to provide a rigorous evaluation of the ability of machine learning models to perform commonsense inference, particularly in situations where the correct answer may not be immediately obvious. The dataset is constructed to be trivial for humans (with approximately 95% accuracy) but significantly challenging for state-of-the-art models (achieving only around 48% accuracy). Key features and aspects of HellaSwag include: 1. Adversarial Filtering (AF): HellaSwag is constructed using Adversarial Filtering, a data collection paradigm in which a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. This approach aims to scale up the length and complexity of dataset examples towards a critical 'Goldilocks' zone, where generated text is ridiculous to humans but often misclassified by state-of-the-art models. 2. Complexity and Ridiculousness: The dataset examples are designed to be complex enough that state-of-the-art generators often make mistakes, while remaining simple enough that discriminators often fail to catch them. This creates a challenging environment for machine learning models, as they must navigate a \"Goldilocks zone\" of complexity and ridiculousness in the dataset examples. 3. Evaluation of Language Models: HellaSwag evaluates the performance of language models, including BERT-Large and OpenAI GPT, in the context of commonsense reasoning. The dataset provides a rigorous test for these models, highlighting their limitations in understanding and reasoning about commonsense scenarios. 4. Benchmark Evolution: HellaSwag suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, presenting ever-harder challenges to machine learning models. This approach aims to drive progress in the field of natural language understanding by continually pushing the boundaries of model capabilities.''',\n",
    "    'Ko-MMLU': '''The Massive Multi-Task Language Understanding (MMLU) benchmark proposed aims to evaluate the ability of machine learning models to possess extensive world knowledge and problem-solving skills. The benchmark covers 57 tasks that vary in difficulty and span a wide range of topics, including science, history, and social studies. One defining feature of the MMLU benchmark is that it is designed to test models' ability to learn and apply knowledge encountered during pretraining. This is in contrast to previous benchmarks that focused on evaluating models' ability to perform specific tasks, such as question answering or sentiment analysis. The MMLU benchmark is intended to assess language understanding in greater breadth and depth than previous benchmarks. Another defining feature of the MMLU benchmark is that it does not require large training sets. Instead, it assumes that models have acquired the requisite knowledge from reading vast quantities of diverse text from the Internet. This is a departure from previous benchmarks that required large amounts of labeled data for training. The MMLU benchmark aims to test and evaluate several specific aspects of machine learning models, including their ability to: - Understand and apply knowledge across a wide range of topics and domains - Reason and solve problems using world knowledge - Perform calculations and other quantitative tasks - Understand and generate natural language text - Handle ambiguity and uncertainty in language understanding - Learn from and adapt to new tasks and domains. Overall, the MMLU benchmark is a comprehensive evaluation of machine learning models' ability to possess extensive world knowledge and problem-solving skills.''',\n",
    "    'Ko-TruthfulQA': '''TriviaQA is a benchmark dataset designed to test and evaluate machine learning models for reading comprehension and question answering. Its main goals are to provide a challenging testbed for these tasks, to offer a large-scale dataset of question-answer-evidence triples, and to enable research on a variety of related tasks such as IR-style question answering, QA over structured KBs, and joint modeling of KBs and text. One of the defining features of TriviaQA is that the questions are authored by trivia enthusiasts independently of the evidence documents. This means that the questions are not tailored to the documents and may require multi-sentence reasoning to answer correctly. The evidence documents come from two domains - Web search results and Wikipedia pages - with highly differing levels of information redundancy. The specific aspects of machine learning models that TriviaQA aims to test and evaluate include their ability to handle substantial syntactic and lexical variability, to reason over multiple sentences, and to handle questions that require a deep understanding of the context and background knowledge. TriviaQA also provides a benchmark for a variety of other tasks such as IR-style question answering, QA over structured KBs, and joint modeling of KBs and text.''',\n",
    "    'LAMBADA': '''The LAMBADA dataset is designed to evaluate the capabilities of computational models for text understanding through a word prediction task. Here's a comprehensive description of the LAMBADA benchmark: Main Goals: The primary goal of LAMBADA is to challenge computational models to genuinely understand broad context in natural language text. It aims to go beyond local context understanding and test the ability of models to keep track of information in the broader discourse. The dataset is intended to encourage the development of new models capable of genuine understanding of broad context in natural language text. Defining Features: 1. Word Prediction Task: LAMBADA presents a word prediction task where the target word is difficult to guess when only the sentence in which it appears is available, but becomes easy when a broader context is presented. This characteristic requires models to go beyond local context and consider the broader discourse to succeed in the task. 2. Narrative Passages: The dataset consists of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. This feature emphasizes the importance of broader context understanding in natural language text. 3. Challenging Benchmark: LAMBADA is designed to be a challenging benchmark for research on automated models of natural language understanding. It exemplifies a wide range of linguistic phenomena, and none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. Aspects Tested and Evaluated: 1. Broad Context Understanding: LAMBADA challenges models to go beyond local context and understand the broader discourse in narrative passages. Models must be able to keep track of information in the broader discourse to succeed in the word prediction task. 2. Language Modeling Capabilities: The dataset evaluates the language modeling capabilities of computational models, emphasizing the need for models to capture non-local phenomena and understand the influence of broad context in natural language text. 3. Performance Comparison: LAMBADA aims to compare the performance of different language models, including traditional N-Gram models and neural-network-based ones, to assess their ability to predict words in a broader context. In summary, the LAMBADA benchmark challenges computational models to understand and predict words in narrative passages by considering the broader discourse context.''',\n",
    "    'LCC': '''The Long Code Completion (LCC) benchmark is introduced as a new dataset for code completion tasks, with a focus on providing long code context for three programming languages: Python, Java, and C#. The main goals of the LCC benchmark are to encourage research in scenarios where longer code examples are required for effective code completion and to evaluate the performance of machine learning models in handling such long code contexts. Key features and aspects of the LCC benchmark include: 1. Dataset Construction: The LCC dataset is constructed from the github-code dataset, which contains a large number of code files from GitHub with an open-platform license suitable for research use. The dataset construction involves deduplicating examples with high similarity, removing unparsable code files, and filtering out excessively short or long code files. 2. Emphasis on Long Code Context: The LCC benchmark focuses on code completion tasks that require modeling of longer code contexts. The average length of examples in LCC is reported to be five times longer than those in existing datasets, highlighting the emphasis on longer code examples. 3. Programming Languages: The LCC benchmark covers code completion tasks for three programming languages: Python, Java, and C#. 4. Evaluation of Machine Learning Models: The LCC benchmark aims to evaluate the effectiveness of machine learning models, particularly in the context of long code completion. It provides a platform for benchmarking several baselines, including LongCoder and OpenAI Codex, and comparing their performance on the LCC dataset. 5. Sparse Attention and Model Design: The LCC benchmark also involves proposing two types of sparse attention, inspired by observations on attention patterns of existing models and how human programmers write code. This aspect aims to explore the effectiveness of attention mechanisms in handling long code contexts and to motivate the design of machine learning models tailored for code completion tasks with longer code examples.''',\n",
    "    'LCSTS': '''The Large Scale Chinese Short Text Summarization (LCSTS) benchmark aims to address the lack of large-scale text summarization datasets by providing a comprehensive corpus of Chinese short texts with corresponding summaries. The main goals of the LCSTS benchmark are to facilitate research in the field of automatic text summarization, particularly for Chinese language processing, and to provide a standardized dataset for evaluating machine learning models for short text summarization. Key features of the LCSTS benchmark include: 1. Large-scale corpus: The dataset consists of over 2 million real Chinese short texts with short summaries, making it one of the largest Chinese short text summarization datasets available. 2. Human-labeled test set: A subset of the dataset, comprising 10,666 instances, has been manually checked and scored for quality, providing a benchmark for evaluating the performance of machine learning models. 3. Standard splits: The dataset is divided into a large-scale training set and a human-labeled test set, enabling researchers to benchmark and compare different methods for short text summarization. Specific aspects of machine learning models that the LCSTS benchmark aims to test and evaluate include: 1. Supervised learning models: The dataset can be used to train supervised learning models for summary generation, allowing researchers to explore the effectiveness of different machine learning algorithms for short text summarization. 2. Recurrent Neural Network (RNN) based models: The benchmark explores the use of RNN as an encoder-decoder method for generating summaries from short texts, providing a baseline for further research on this topic. 3. Abstractive summarization methods: The dataset is particularly relevant for evaluating abstractive summarization methods, which aim to generate concise and informative summaries that convey important information from the original text.''',\n",
    "    'LIME': '''The LIME benchmark is a set of synthetic tasks designed to evaluate the ability of machine learning models to reason mathematically. The benchmark is inspired by three reasoning primitives: deduction, induction, and abduction. The main goal of LIME is to encode inductive biases for mathematical reasoning in the form of datasets and use them to pre-train machine learning models. The defining feature of LIME is its synthetic tasks, which require the model to have a fundamental understanding of mathematical reasoning. The tasks are designed to mimic the reasoning steps in formal theorem proving and are constructed from proof steps extracted from a written theorem library. The tasks include: 1. Deduct: Given a set of premises and a conclusion, the model is asked to determine whether the conclusion can be logically deduced from the premises. 2. Induct: Given a set of examples and a hypothesis, the model is asked to determine whether the hypothesis is true for all examples. 3. Abduct: Given a set of premises and a conclusion, the model is asked to determine the most likely explanation for the conclusion. LIME aims to test and evaluate the ability of machine learning models to reason mathematically, specifically in the context of formal theorem proving. The benchmark evaluates the performance of models on four large mathematical reasoning benchmarks: IsarStep, HOList Skip-tree, MetaMathStep, and LeanStep. The evaluation metrics include top-1 and top-10 accuracy, which measure the percentage of examples where the model's prediction is correct.''',\n",
    "    'LONGFQA': '''The LONGFQA (Long Finance Question Answering) benchmark dataset is a part of the L-Eval evaluation suite for long context language models (LCLMs). Its main goal is to provide a standardized evaluation for LCLMs in the finance domain, which is currently lacking in long open-ended QA datasets. The LONGFQA dataset consists of 54 questions based on earnings call transcripts obtained from the Investor Relations section of the company websites of six different corporations: Lumentum Oclaro, Theragenics, FS KKR Capital Corp, LaSalle Incorporated, Renewable Energy Group. The dataset is designed to test the ability of LCLMs to answer long open-ended questions in the finance domain, which requires a deep understanding of financial concepts and terminology. The defining feature of LONGFQA is its long context, which is necessary to accurately answer the questions. The transcripts used in the dataset are lengthy and contain a lot of financial jargon, making it challenging for LCLMs to comprehend and extract relevant information. Additionally, the questions are open-ended, which means that the models must generate answers rather than simply selecting from a set of options. The specific aspects of machine learning models that LONGFQA aims to test and evaluate include their ability to understand and reason about financial concepts, their ability to extract relevant information from lengthy and complex documents, and their ability to generate accurate and coherent answers to open-ended questions.''',\n",
    "    'LSHTC': '''The Large Scale Hierarchical Text Classification (LSHTC) benchmark dataset is a widely used benchmark in the field of text classification. Its main goal is to evaluate the performance of machine learning models on large-scale hierarchical text classification tasks. The LSHTC dataset is characterized by its large size, hierarchical structure, and sparsity. It contains millions of documents, organized into a hierarchical taxonomy with thousands of categories. The dataset is highly imbalanced, with a few categories containing a large number of documents and many categories containing only a few documents. The LSHTC dataset aims to test and evaluate the ability of machine learning models to handle large-scale, hierarchical text classification tasks. Specifically, it tests the ability of models to handle the following challenges: 1. Large-scale classification: The LSHTC dataset contains millions of documents, making it a challenging task for machine learning models to classify accurately and efficiently. 2. Hierarchical classification: The LSHTC dataset is organized into a hierarchical taxonomy with thousands of categories. This structure poses a challenge for models to accurately classify documents at different levels of the hierarchy. 3. Sparsity: The LSHTC dataset is highly imbalanced, with many categories containing only a few documents. This sparsity poses a challenge for models to accurately classify documents in these categories. To evaluate the performance of machine learning models on the LSHTC dataset, various evaluation metrics are used, including precision, recall, F1-score, and hierarchical F1-score. The LSHTC benchmark has been used to evaluate the performance of various machine learning models, including decision trees, support vector machines, and neural networks. In summary, the LSHTC benchmark dataset is a large-scale, hierarchical text classification dataset that aims to test and evaluate the ability of machine learning models to handle challenging text classification tasks.''',\n",
    "    'Landing Pages': '''The Landing Pages dataset is one of the scenarios covered in the HEIM benchmark. Its main goal is to evaluate the ability of text-to-image models to generate visually appealing elements for landing pages based on textual prompts. The dataset consists of 36 prompts, each representing a different type of landing page, such as \"a landing page for a finance web application\" or \"a landing page for a travel booking platform\". The prompts follow the format of \"a landing page of X\", where X is a description of a website or application. The defining feature of the Landing Pages dataset is its focus on evaluating the ability of text-to-image models to generate visually appealing and contextually relevant images for landing pages. This requires the models to understand the purpose and target audience of each landing page, as well as the ability to generate images that enhance the overall design and user experience of the landing page. The specific aspects of machine learning models that the Landing Pages dataset aims to test and evaluate include their ability to: - Understand and interpret textual prompts related to different types of landing pages - Generate visually appealing and contextually relevant images that enhance the design and user experience of landing pages - Capture the essence and purpose of each landing page in their generated images - Handle variations in the textual prompts, such as different descriptions or variations in the wording of the prompts related to the same type of landing page''',\n",
    "    'LegalSupport': '''The LegalSupport benchmark dataset aims to evaluate the ability of machine learning models to perform legal reasoning tasks. Its main goal is to assess a model's capability to determine which court case most persuasively supports a given legal argument. This task is crucial in legal argument formulation, as it involves citing previous court decisions (precedents) to support legal claims. Key features of the LegalSupport benchmark dataset include: 1. Comparative Legal Entailment Task: The dataset presents models with an argument and the legal conclusions of two court cases, requiring the model to determine which case most convincingly supports the argument. 2. Real-world Legal Annotations: The arguments and annotations in the dataset are mined from actual legal opinions, providing a realistic and challenging evaluation environment for machine learning models. 3. Focus on Persuasive Legal Reasoning: By assessing a model's ability to identify the most persuasive legal support for an argument, the dataset targets the nuanced and complex nature of legal reasoning in real-world scenarios. Specific aspects of machine learning models that the LegalSupport benchmark dataset aims to test and evaluate include: 1. Legal Entailment Recognition: Assessing the model's capacity to recognize and understand the entailment relationships between legal arguments and court case conclusions. 2. Precedent Analysis: Evaluating the model's ability to effectively analyze and compare legal precedents to determine their persuasive strength in supporting a given argument. 3. Real-world Legal Application: Testing the model's performance in a real-world legal context, where the ability to identify persuasive legal support is essential for legal professionals. By focusing on these aspects, the LegalSupport benchmark dataset provides a rigorous evaluation of machine learning models' capabilities in the domain of legal reasoning, with implications for applications in legal research, decision support systems, and automated legal assistance.''',\n",
    "    'LogiQA': '''LogiQA is a benchmark dataset for machine reading comprehension with logical reasoning. Its main goal is to evaluate the ability of machine learning models to perform logical reasoning tasks, which is a fundamental capability in human reading. The dataset consists of 8,678 paragraph-question pairs, each with four candidate answers, platformd from expert-written questions for testing human logical reasoning. The questions cover multiple types of deductive reasoning, including categorical reasoning, conditional reasoning, disjunctive reasoning, and conjunctive reasoning. LogiQA is unique in that it focuses specifically on logical reasoning, which is not fully investigated in other machine reading comprehension datasets. It also provides a comprehensive evaluation of state-of-the-art neural models developed for reading comprehension, including pre-trained models such as BERT and RoBERTa, as well as rule-based and deep learning models. The specific aspects of machine learning models that LogiQA aims to test and evaluate include their ability to integrate contextualized embeddings, their performance on ablation tests to measure bias in the dataset, and their transfer learning capabilities. The dataset also provides insights into potentially promising research directions for improving machine reading comprehension with logical reasoning.''',\n",
    "    'Logos': '''The Logos dataset is one of the scenarios covered in the HEIM benchmark. Its main goal is to evaluate the ability of text-to-image models to generate unique visual symbols or marks that represent brands, companies, or organizations based on textual prompts. The dataset consists of prompts related to creating logos, with each prompt representing a different type of logo or brand symbol. The prompts follow the format of descriptions such as \"a logo for a tech startup\" or \"a logo for a sports brand\". The defining feature of the Logos dataset is its focus on evaluating the ability of text-to-image models to generate visually distinctive and meaningful logos that effectively represent the identity, values, and essence of the associated brand, company, or organization. This requires the models to understand the branding context and the intended audience for each logo, as well as the ability to generate images that capture the unique visual identity of the brand. The specific aspects of machine learning models that the Logos dataset aims to test and evaluate include their ability to: - Understand and interpret textual prompts related to creating logos for different types of brands or organizations - Generate visually distinctive and meaningful logos that effectively represent the identity and values of the associated brand or organization - Capture the essence and uniqueness of each brand or organization in their generated logos - Handle variations in the textual prompts, such as different descriptions or variations in the wording of the prompts related to the same type of logo''',\n",
    "    'LAiW Leaderboard': '''The LAiW benchmark aims to provide a comprehensive evaluation of Chinese legal LLMs. Its main goals are to classify tasks based on legal capabilities and allow for comprehensive objective evaluation. The benchmark is designed to test and evaluate the performance of LLMs on three levels of legal capabilities: basic legal NLP, legal knowledge, and legal reasoning. The defining features of the LAiW benchmark include its focus on Chinese legal language, its multi-level evaluation system, and its comprehensive evaluation metrics. The benchmark is unique in that it is specifically tailored to the Chinese legal system, which has its own distinct language and legal concepts. In terms of machine learning models, the LAiW benchmark evaluates the performance of LLMs on a range of legal tasks, including legal document classification, legal question answering, and legal case analysis. The benchmark also includes a variety of evaluation metrics, such as accuracy, F1 score, and precision-recall curves, to provide a comprehensive evaluation of LLM performance.''',\n",
    "    'LMExamQA': '''The LMExamQA benchmark is a key component of the Language-Model-as-an-Examiner framework proposed in this paper. Its main goal is to evaluate the proficiency of foundation models in answering questions across three cognitive levels: recall, inference, and analysis. The defining feature of LMExamQA is that it employs a language model (LM) as an examiner to generate high-quality questions across various domains. These questions are designed to probe for a broad acquisition of knowledge and to engage in a more in-depth assessment through follow-up questions. The specific aspects of machine learning models that LMExamQA aims to test and evaluate include their ability to accurately and completely answer questions, without providing additional details. This is done by employing the LM examiner to produce a groundtruth answer, which is then evaluated by human experts on a random selection of 100 questions. In addition to evaluating the accuracy of machine learning models in answering questions, LMExamQA also aims to assess their comprehension of a topic in depth. This is done through a multi-round follow-up question generation procedure, drawing inspiration from the interview process.''',\n",
    "    'LLMEval': '''LLMEval is a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models. Its main goal is to streamline the evaluation process by using a single prompt and a unified evaluation schema, while still providing consistent and competitive performance across diverse scenarios. The defining features of LLMEval include its ability to accommodate different scoring ranges, its adaptability to different evaluation scenarios, and its robustness in comparison to state-of-the-art evaluation techniques. LLMEval aims to test and evaluate specific aspects of machine learning models related to open-domain conversations, including their ability to generate appropriate, relevant, and grammatically correct responses. It also evaluates the quality of the response in terms of content and coherence, while maintaining the efficiency of the evaluation process by requiring just a single call to the large language model.''',\n",
    "    'LLMonitor': '''The LLMonitor benchmark aims to evaluate the performance of various language models using a structured approach. Key aspects of this evaluation include: Weekly Benchmark Dataset Creation: The highest-rated submitted prompt each week is added to the benchmark dataset. This evolving approach ensures that the benchmark remains relevant and challenging. Model Testing and Scoring System: The prompts are tested against 77 models with a fixed temperature setting of zero. The responses are scored by GPT-4 according to specific rubrics, which are conditions set for each prompt. For instance, points may be awarded for mentioning specific facts or viewpoints relevant to the prompt​​. Rubrics and Scoring Challenges: Rubrics for each prompt are visible on their respective pages. The use of GPT-4 for scoring acknowledges potential biases, particularly towards OpenAI models, and may not fully reward creative or unconventional responses. Currently, rubrics are manually created, with plans to introduce a crowdsourcing method for their development.''',\n",
    "    'LSAT': '''The LSAT benchmark is a dataset designed to test and evaluate the performance of machine learning models on the task of analytical reasoning of text. The dataset consists of questions from the Law School Admission Test (LSAT) from 1991 to 2016. The main goals of the benchmark are to facilitate research on analytical reasoning, to analyze the knowledge understanding and reasoning ability required for this task, and to present two different baselines for addressing the reasoning challenge. The defining features of the LSAT benchmark are its focus on analytical reasoning, which involves analyzing a scenario involving a set of particular circumstances and performing reasoning over it to make conclusions. The dataset consists of questions that require the machine to perform explicit complex reasoning, far beyond just understanding the literal clues presented in the text. The questions test a range of reasoning skills, including comprehending the knowledge including participants of events, facts, and rules described in the context, extracting machine-understandable logical functions (expressions) from the rules, making deductions to derive legitimate solutions that satisfy extracted logical functions, and selecting the answer that satisfies all the rules with the deducted legitimate solutions. The specific aspects of machine learning models that the LSAT benchmark aims to test and evaluate include their ability to perform explicit complex reasoning, their ability to understand the complex scenario and perform reasoning over it, and their ability to extract symbolic knowledge (e.g., participants, facts, logical functions) to deduce legitimate solutions. The benchmark also evaluates the interpretability of the models and their ability to perform deeper reasoning abilities far beyond just understanding the literal clues presented in the text.''',\n",
    "    'M3KE': '''The M3KE benchmark is a comprehensive evaluation tool for Chinese large language models (LLMs) that aims to measure their knowledge acquisition and multitask accuracy in zero- and few-shot settings. The benchmark is designed to bridge the gap between Chinese education systems and LLMs by covering all major levels of Chinese education, ranging from primary school to college, and a wide variety of subjects, including humanities, history, politics, law, education, psychology, science, technology, art, and religion. The benchmark contains 20,477 multiple-choice questions with four options, ensuring a standardized and unified assessment process. Unlike recent benchmarks MMCU and AGIEval, M3KE covers all major levels of Chinese education and a wide variety of subjects, making it a more comprehensive evaluation tool for Chinese LLMs. The main goal of the M3KE benchmark is to comprehensively evaluate the capability of Chinese LLMs in multiple tasks, including cross-task generalization and instruction following. The benchmark aims to test the knowledge acquired by these models by measuring their multitask accuracy in zero- and few-shot settings. The defining features of the M3KE benchmark include its comprehensive coverage of education levels and subjects, its use of multiple-choice questions with four options, and its focus on measuring multitask accuracy in zero- and few-shot settings. These features make the benchmark a valuable tool for testing the knowledge acquisition and multitask accuracy of Chinese LLMs.''',\n",
    "    'MATH': '''MATH benchmark serves as a critical replatform for evaluating and advancing the mathematical problem-solving ability of machine learning models, providing a challenging and diverse set of problems with step-by-step solutions to facilitate learning and model evaluation. Main Goals: The primary goal of the MATH benchmark is to assess and improve machine learning models' ability to solve complex mathematical problems. It aims to go beyond simple plug-and-chug calculations and evaluate models' capacity for comprehensive mathematical reasoning and problem-solving. By providing a diverse set of 12,500 challenging competition mathematics problems, the MATH dataset serves as a platform for evaluating and advancing the problem-solving capabilities of machine learning models. Defining Features: 1. Challenging Competition Mathematics Problems: The MATH dataset consists of a wide range of challenging problems from high school math competitions, covering various mathematical subjects and difficulty levels. 2. Step-by-Step Solutions: Each problem in the MATH dataset is accompanied by a full step-by-step solution, written in LATEX and natural language. This feature enables machine learning models to learn from the provided solutions and generate their own step-by-step solutions. 3. Unique Answer Normalization: The dataset ensures that answers are unique after normalization, allowing for exact match scoring rather than heuristic metrics such as BLEU. This unique normalization facilitates accurate evaluation of model outputs. 4. Automatic Assessment: The MATH dataset is designed to enable automatic assessment of model-generated answers. The use of the \\boxed{} command to wrap and delimit final answers allows for automatic evaluation by parsing the content inside the \\boxed{} command and comparing it with the ground truth answer. Specific Aspects for Machine Learning Model Evaluation: The MATH benchmark aims to test and evaluate several specific aspects of machine learning models, including: 1. Mathematical Problem-Solving Ability: The dataset assesses models' capacity for comprehensive mathematical problem-solving across various mathematical subjects and difficulty levels. 2. Answer Generation: Models are tasked with generating tokens to construct the final boxed answer, demonstrating their ability to generate accurate mathematical solutions. 3. Accuracy and Performance: The benchmark evaluates the accuracy and performance of machine learning models on the MATH dataset, with large language models achieving accuracies ranging from 3.0% to 6.9%. 4. Human-Level Performance Comparison: The dataset provides a basis for comparing model performance to human-level problem-solving ability, as demonstrated by a human evaluation involving university students.''',\n",
    "    'MATH401': '''The MATH401 benchmark is a dataset designed to evaluate the arithmetic abilities of large language models (LLMs) in solving math word problems. The dataset consists of 401 math word problems, each with a corresponding arithmetic expression that the model must solve. The expressions include addition, subtraction, multiplication, and division, as well as parentheses and variables. The problems are designed to test the models' ability to perform arithmetic operations correctly and to apply chain-of-thought reasoning to solve multi-step problems. The authors provide a detailed analysis of the models' performance on the MATH401 dataset, including accuracy, speed, and efficiency.''',\n",
    "    'MAVEN': '''Massive General Domain Event Detection (MAVEN) aims to address the limitations of existing datasets for event detection. Its main goals include alleviating data scarcity and low coverage problems, providing a comprehensive replatform for training and benchmarking modern neural methods, and encouraging further breakthroughs in event detection research. Key features of MAVEN include: 1. Size: It contains 4,480 annotated documents, 118,732 event mention instances, and 168 event types, making it twenty times larger than the widely-used ACE 2005 dataset. 2. Event Types: MAVEN covers a broad range of general domain events, with event types manually derived from the frames defined in the linguistic replatform FrameNet. 3. Hierarchical Event Type Schema: MAVEN's hierarchical event type schema maintains good coverage of FrameNet and provides valuable hierarchy information for future event detection models. MAVEN aims to test and evaluate specific aspects of machine learning models, including: 1. Performance on General Domain Event Detection: It evaluates the performance of state-of-the-art event detection models on MAVEN, highlighting the challenges of detecting general-domain events. 2. Model Robustness: MAVEN provides stable benchmarking for sophisticated event detection methods, addressing the issue of unreliable comparison results at the scale of existing datasets. 3. Multiple Event Correlations: It explores the modeling of multiple event correlations, utilizing the hierarchical event schema to distinguish close types, and improving other event detection tasks with transfer learning. 4. Transferability: MAVEN examines the transferability of general event knowledge learned on MAVEN to other event detection tasks, demonstrating its potential to benefit low-replatform event detection scenarios.''',\n",
    "    'MAVEN-ERE': '''The MAVEN-ERE benchmark aims to address the challenges of real-world event relation extraction (ERE) by providing a comprehensive dataset for evaluating machine learning models. The benchmark aims to improve the performance of ERE models by jointly considering multiple relation types and better modeling long-range dependencies. MAVEN-ERE is the first million-scale human-annotated ERE dataset, significantly larger than existing datasets, which alleviates the limitation of data scale and facilitates the development of ERE methods. Key features of MAVEN-ERE include its coverage of English documents and its annotation of within-document event relations. However, it is important to note that MAVEN-ERE is limited to English documents, which restricts the linguistic features covered and the scope of applications built on it. To address this limitation, future works are encouraged to explore multilingual transfer learning techniques and annotate native datasets for low-replatform languages with the annotation schemes of MAVEN-ERE. The benchmark aims to evaluate machine learning models in the following specific aspects: 1. Event Coreference: MAVEN-ERE annotates event coreference relations, which are essential for understanding the connections between events mentioned in a text. 2. Temporal Relations: The dataset annotates global temporal relations within documents, providing a basis for understanding the temporal order of events. 3. Causal Relations: MAVEN-ERE includes annotations for causal relations, enabling the evaluation of models' ability to identify causal connections between events. 4. Subevent Relations: The dataset annotates subevent relations, which are crucial for understanding the hierarchical structure of events. The benchmark also addresses ethical considerations, such as intellectual property, worker treatments, and controlling potential risks. MAVEN-ERE is developed with the consent of the authors of MAVEN, and annotators are hired from professional data annotation companies and fairly compensated. The texts in MAVEN-ERE do not involve private information, and annotating event relations does not create additional risks. In addition, MAVEN-ERE provides a comprehensive evaluation of machine learning models by comparing their performance with existing widely-used datasets for ERE tasks, including ACE 2005, TAC KBP, TB-Dense, MATRES, TCR, Causal-TB, EventStoryLine, and HiEve. The benchmark includes benchmarks for training models independently and jointly, demonstrating the benefits of the unified annotation. Furthermore, MAVEN-ERE exhibits a good coverage for general-domain topics and demonstrates transferability of general ERE skills to datasets in other genres, especially for small-scale datasets. This suggests that the skills learned from MAVEN-ERE are transferable and can help ERE on datasets in other genres, encouraging future works to explore the influence of genre gaps deeply.''',\n",
    "    'MBPP': '''The Mostly Basic Programming Problems (MBPP) benchmark is designed to evaluate the capabilities of large language models for program synthesis in general-purpose programming languages, particularly Python. The main goals of the MBPP benchmark include assessing the ability of machine learning models to synthesize short Python programs from natural language descriptions, as well as to measure their performance in few-shot and fine-tuning regimes. Key features and aspects of the MBPP benchmark include: 1. Programming Tasks: The benchmark contains 974 programming tasks, specifically designed to be solvable by entry-level programmers. These tasks cover a range of programming concepts and problem-solving scenarios, providing a diverse set of challenges for machine learning models to address. 2. Prompt Design: The benchmark includes ground truth platform code, natural language descriptions, and three input-output examples for each task. The prompt configurations are carefully designed to investigate how execution performance depends on the presence or absence of specific information, such as the ground truth code, natural language description, and test cases. 3. Performance Evaluation: The benchmark evaluates the performance of machine learning models in predicting the output of the ground truth program when run on given test case inputs. It analyzes sensitivity to factors such as model size, number of examples in the prompt, the identity of examples in the prompt, and sampling techniques. 4. Generalization and Memorization: The benchmark investigates the generalization capabilities of models to held-out test cases, aiming to demonstrate that solutions tend to generalize rather than simply parroting the answers in the prompt. Additionally, it assesses the overlap between the solutions in MBPP and the pre-training set to reduce the chance that synthesis results are due to memorization. 5. Interactive Use: The benchmark explores the extent to which large language models can be used as interactive tools, demonstrating that humans can interact with these models to significantly improve performance.''',\n",
    "    'MCScript': '''The MCScript benchmark is designed to evaluate machine comprehension models with a focus on commonsense knowledge and script knowledge. Here's a comprehensive description of the benchmark: Main Goals: 1. Assessing Script Knowledge Contribution: The benchmark aims to assess the contribution of script knowledge to machine comprehension. It focuses on narratives about everyday activities, such as going to the movies or working in the garden, and the questions require commonsense knowledge, specifically script knowledge, to be answered. 2. Challenging Natural Language Understanding: The benchmark provides challenging test cases for the broader natural language understanding community, aiming to push the boundaries of machine comprehension and inference based on commonsense knowledge. Defining Features: 1. Narrative Texts and Questions: The benchmark includes narrative texts and questions based on everyday activities, reflecting real-world scenarios. 2. Crowdplatformd Data Collection: The data collection process involves crowdsourcing, resulting in a substantial amount of inference questions that require commonsense knowledge for accurate answers. 3. Real-World Evaluation Scenario: Unlike previous evaluation tasks, the setup of MCScript allows for the assessment of the contribution of script knowledge for computational models of language understanding in a real-world evaluation scenario. Specific Aspects for Machine Learning Models: 1. Commonsense Knowledge Inference: The benchmark evaluates the ability of machine comprehension models to infer answers based on commonsense knowledge, particularly script knowledge about everyday activities. 2. Script-Based Reasoning: It tests the capability of models to reason and comprehend narratives by leveraging script knowledge, which involves understanding stereotypical human activities and making inferences based on such knowledge. 3. Model Performance Assessment: The benchmark assesses the performance of machine comprehension models, including neural models and baselines, on different question types, including those requiring commonsense knowledge for accurate answers. In summary, the MCScript benchmark aims to provide a standardized evaluation platform for machine comprehension models, focusing on commonsense and script knowledge.''',\n",
    "    'MEDIC': '''The MEDIC (Multi-Task Learning Dataset for Disaster Image Classification) benchmark is a significant contribution to the field of disaster response and artificial intelligence. Its main goals include providing a comprehensive dataset for multi-task learning in disaster image classification, facilitating research in multi-task learning, and serving as a benchmark for future studies. The defining features of the MEDIC benchmark include a large collection of over 71,000 images from diverse events, consisting of different time frames, and crowdplatformd annotations for four distinct tasks: disaster types, informativeness, humanitarian, and damage severity. The dataset aims to test and evaluate machine learning models in the context of multi-task learning. Specifically, it provides a platform for evaluating the performance of multi-task learning setups with neural networks. The dataset allows for the analysis of computational time, model convergence rates, and the impact of multi-task learning on model capacity and performance. Additionally, the dataset facilitates the evaluation of the effectiveness of multi-task learning in reducing computational complexity while maintaining comparative performance. The MEDIC benchmark also aims to provide a benchmark for heterogeneous multi-task learning and baseline studies to facilitate future research. It offers a platform for evaluating the performance of single-task learning models and comparing them with multi-task learning setups. Furthermore, the dataset enables the assessment of the quality of annotations by ensuring agreement among annotators and provides a benchmark for future studies in disaster image classification.''',\n",
    "    'MLMN': '''The MLMN (Multi-Level Matching Network) benchmark aims to address the fine-grained correspondence between legal cases and law articles. Its main goals include: 1. Fine-Grained Correspondence: MLMN aims to establish a benchmark for evaluating the performance of machine learning models in achieving fine-grained correspondence between legal cases and law articles. This involves capturing nuanced relationships and dependencies between specific facts in legal cases and the corresponding law articles. 2. Text Matching: The benchmark focuses on evaluating the effectiveness of text matching models in the legal domain. It aims to test and compare various text matching approaches to determine their suitability for capturing the intricate connections between legal cases and relevant law articles. Defining Features of MLMN: - Fine-Grained Matching: MLMN emphasizes the need for fine-grained matching between legal cases and law articles, as opposed to coarse-grained recommendations. This highlights the importance of capturing detailed and specific correspondences. - Multi-Level Information: The benchmark incorporates the concept of multi-level information extraction, aiming to capture information at different levels of abstraction to enhance the matching process. This feature allows for a more comprehensive understanding of the relationships between legal cases and law articles. Specific Aspects of Machine Learning Models Tested and Evaluated: - Text Encoders: MLMN evaluates the performance of different text encoders, including CNN, Bi-LSTM, and Transformer, to assess their effectiveness in generating text patterns and capturing the nuances of legal text. - Knowledge Incorporation: The benchmark explores the integration of additional knowledge, such as text categories, LDA information, WordNet-defined word relationships, and pre-trained language models, to enhance the feature extraction process and improve matching accuracy. - Ablation Studies: MLMN conducts ablation studies to test the model under different settings, including the positive-negative ratio and the effects of using different types of text encoders. This allows for a comprehensive evaluation of the model's performance under various conditions. In summary, the MLMN benchmark aims to provide a comprehensive evaluation of machine learning models in achieving fine-grained correspondence between legal cases and law articles. It emphasizes the importance of capturing detailed relationships and dependencies, while also testing the effectiveness of different text encoders and knowledge incorporation techniques in the legal domain.''',\n",
    "    'MINT-Bench': '''The MINT-bench aims to evaluate the performance of large language models (LLMs) in solving complex tasks that require multi-turn interactions and natural language feedback. The main goals of the benchmark are to address the limitations of current evaluation protocols, which often focus on single-turn exchanges and neglect the nuanced interactions among the user, LLMs, and external tools. To achieve these goals, the MINT benchmark includes two defining features: tool-augmented task-solving and natural language feedback. The former allows LLMs to access external tools by generating and executing Python programs, while the latter simulates human users by providing feedback to refine the LLM's solutions. The benchmark is designed to test and evaluate specific aspects of machine learning models, including their ability to solve tasks with multi-turn interactions, their capacity to leverage external tools, and their responsiveness to natural language feedback. The benchmark also aims to ensure reproducibility by providing an evaluation framework where LLMs can access tools and receive simulated user feedback.''',\n",
    "    'MMBench': '''MMBench is a multi-modality benchmark designed to evaluate the perception and reasoning abilities of large vision-language models. Its main goal is to provide a comprehensive evaluation pipeline that surpasses existing benchmarks in terms of the number and variety of evaluation questions and abilities. MMBench has two defining features that differentiate it from other benchmarks. First, it adopts problems from various Platform to evaluate diversified abilities in a hierarchical taxonomy. Second, it applies a robust, LLM-based evaluation strategy that can handle the free-form outputs of multi-modality models and yield trustworthy evaluation results with affordable cost. MMBench evaluates various abilities of vision-language models, including but not limited to object localization, social reasoning, and instruction-following. Currently, MMBench contains approximately 3000 single-choice questions covering 20 different ability dimensions, with each ability dimension including more than 75 questions. The ability dimensions are not static and will expand as the team continues to work on them.''',\n",
    "    'MMCU': '''The MMCU benchmark aims to provide a scientific evaluation method for large Chinese language models, particularly those based on the Transformer architecture. The main goals of the benchmark are to measure the multitask accuracy of these models across four major domains (medicine, law, psychology, and education), provide high-quality evaluation datasets, and assess the models' capabilities in understanding and solving problems across various domains. The defining features of the MMCU benchmark include its use of both zero-shot and few-shot testing modes, with the latter offering five examples to the models. The test questions are single-choice and multiple-choice, with each question potentially having one or multiple correct answers, making them more akin to human exams and increasing their difficulty. The benchmark also includes 15 subtasks in medicine and 8 subtasks in education, covering a range of topics and requiring different types of reasoning and knowledge. In terms of the specific aspects of machine learning models that the MMCU benchmark aims to test and evaluate, these include their ability to perform well on multitask learning, transfer learning, and few-shot learning tasks. The benchmark also assesses the models' performance on tasks that require domain-specific knowledge and reasoning, such as legal reasoning and clinical diagnosis. Overall, the MMCU benchmark provides a comprehensive evaluation of large Chinese language models that takes into account their strengths and weaknesses across different domains and types of tasks.''',\n",
    "    'MME': '''The MME benchmark proposed aims to provide a comprehensive evaluation of Multimodal Large Language Models (MLLMs) by measuring both their perception and cognition abilities on a total of 14 subtasks. The benchmark is designed to address the lack of a comprehensive evaluation of MLLMs, which has been a challenge for the field. One defining feature of MME is that it uses manually designed annotations of instruction-answer pairs to avoid data leakage that may arise from direct use of public datasets for evaluation. The concise instruction design allows for fair comparison of MLLMs, instead of struggling in prompt engineering. The benchmark evaluates MLLMs on their ability to comprehend images and the corresponding knowledge behind them. Specifically, MME tests MLLMs on their ability to perform tasks such as image captioning, visual question answering, and image retrieval.''',\n",
    "    'MMLU': '''The Massive Multi-Task Language Understanding (MMLU) benchmark proposed aims to evaluate the ability of machine learning models to possess extensive world knowledge and problem-solving skills. The benchmark covers 57 tasks that vary in difficulty and span a wide range of topics, including science, history, and social studies. One defining feature of the MMLU benchmark is that it is designed to test models' ability to learn and apply knowledge encountered during pretraining. This is in contrast to previous benchmarks that focused on evaluating models' ability to perform specific tasks, such as question answering or sentiment analysis. The MMLU benchmark is intended to assess language understanding in greater breadth and depth than previous benchmarks. Another defining feature of the MMLU benchmark is that it does not require large training sets. Instead, it assumes that models have acquired the requisite knowledge from reading vast quantities of diverse text from the Internet. This is a departure from previous benchmarks that required large amounts of labeled data for training. The MMLU benchmark aims to test and evaluate several specific aspects of machine learning models, including their ability to: - Understand and apply knowledge across a wide range of topics and domains - Reason and solve problems using world knowledge - Perform calculations and other quantitative tasks - Understand and generate natural language text - Handle ambiguity and uncertainty in language understanding - Learn from and adapt to new tasks and domains. Overall, the MMLU benchmark is a comprehensive evaluation of machine learning models' ability to possess extensive world knowledge and problem-solving skills.''',\n",
    "    'MM-Vet': '''MM-Vet is an evaluation benchmark that aims to examine the abilities of large multimodal models (LMMs) on complicated multimodal tasks. The benchmark defines six core VL (vision and language) capabilities, including recognition, OCR, knowledge, language generation, spatial awareness, and math, which integrate to solve various complicated multimodal tasks. MM-Vet contains 16 tasks for quantitative evaluation, which require LMMs to master and integrate these core capabilities to solve the tasks. One of the defining features of MM-Vet is its focus on evaluating LMMs on complicated multimodal tasks that require integrated VL capabilities. The benchmark is designed to provide insights beyond simple performance rankings, allowing for a more comprehensive evaluation of LMMs. MM-Vet also adopts an open-ended LLM-based evaluator, which allows for evaluation across answer styles and question types without requiring binary or multiple answer choices. In terms of specific aspects of machine learning models that MM-Vet aims to test and evaluate, the benchmark examines the arbitrary integration of core VL capabilities for complicated tasks. MM-Vet proposes that the intriguing ability to solve complicated multimodal tasks can be achieved by a generalist model mastering and integrating different core capabilities. Therefore, the benchmark evaluates LMMs on their ability to master and integrate these core capabilities to solve the tasks in MM-Vet.''',\n",
    "    'MMMU': '''The MMMU benchmark is a comprehensive dataset designed to evaluate the expert-level multimodal understanding capability of foundation models across a broad scope of tasks. The main goals of the benchmark are to test and evaluate the ability of machine learning models to perceive and understand information across different modalities, apply reasoning with subject-specific knowledge to derive the solution, and handle diverse image formats, from visual scenes like photographs and paintings to diagrams and tables. The defining features of the MMMU benchmark include its coverage of 30 subjects across 6 disciplines, including Art, Business, Health & Medicine, Science, Humanities & Social Science, and Tech & Engineering, and over 183 subfields. The dataset is divided into a few-shot development set, a validation set, and a test set, with the test set comprising 10.5K questions. The questions in the benchmark were manually collected by a team of 50 college students from various disciplines and subjects, drawing from online Platform, textbooks, and lecture materials. The specific aspects of machine learning models that the MMMU benchmark aims to test and evaluate include their ability to handle interleaved text-image inputs, recall deep subject knowledge, and conduct complex reasoning based on the understanding and knowledge to reach a solution. The benchmark introduces four key challenges to multimodal foundation models, including the requirement for both expert-level visual perceptual abilities and deliberate reasoning with subject-specific knowledge.''',\n",
    "    'MNLI': '''The Multi-Genre Natural Language Inference (MultiNLI) corpus serves as a benchmark for the development and evaluation of machine learning models for sentence understanding. It was designed to address the limitations of existing rePlatform and to provide a more comprehensive and challenging dataset for natural language inference (NLI). Main Goals: 1. Comprehensive Coverage: MultiNLI offers data from ten distinct genres of written and spoken English, allowing systems to be evaluated on nearly the full complexity of the language. This diversity facilitates the evaluation of cross-genre domain adaptation. 2. Difficulty: The corpus aims to provide a substantially more difficult task than existing rePlatform, such as the Stanford NLI corpus, despite showing similar levels of inter-annotator agreement. This increased difficulty is reflected in the lower baseline model performance on MultiNLI compared to SNLI. 3. Benchmark for NLU: MultiNLI aims to serve as a benchmark for ambitious machine learning research on the core problems of natural language understanding (NLU). Defining Features: 1. Genre Diversity: It includes data from ten different genres of written and spoken English, providing a representative sample of text and speech from various domains. 2. Cross-Genre Domain Adaptation: MultiNLI explicitly provides a setting for evaluating cross-genre domain adaptation, allowing models to be tested on their ability to derive reasonable representations in unfamiliar domains. 3. Inter-Annotator Agreement: Despite the increased difficulty, MultiNLI shows similar levels of inter-annotator agreement compared to existing rePlatform, indicating that annotators agreed on a single task definition and were able to apply it consistently across genres. Aspects of Machine Learning Models Tested and Evaluated: 1. Sentence Representations: MultiNLI evaluates the quality of sentence representations within the training domain and their ability to derive reasonable representations in unfamiliar domains. 2. Transfer Learning: It serves as a replatform for the development and evaluation of methods for transfer learning in sentence understanding, as models trained on MultiNLI have shown substantial improvements on transfer learning benchmarks. 3. Model Difficulty and Diversity: The corpus challenges machine learning models with a higher percentage of sentences tagged with difficult linguistic phenomena, making it dramatically more difficult than existing rePlatform.''',\n",
    "    'MOCHEG': '''The Mocheg benchmark dataset is designed to support end-to-end multimodal fact-checking and explanation generation. Its main goals include assessing the truthfulness of claims by retrieving relevant evidence from a large collection of web Platform, such as articles, images, videos, and tweets, and generating statements to summarize and explain the reasoning and ruling process. The dataset consists of 15,601 claims, each annotated with a truthfulness label and a ruling statement, along with 33,880 textual paragraphs and 12,112 images as evidence. Key features of the Mocheg benchmark include the annotation of claims by fact-checking journalists, ensuring the quality and reliability of the labeled data. This distinguishes Mocheg from related work that relies on automatically generated labels or evidence without human validation. Additionally, Mocheg provides explanations regarding the truthfulness prediction, enhancing the interpretability and transparency of the fact-checking process. The specific aspects of machine learning models that Mocheg aims to test and evaluate include multimodal evidence retrieval, claim verification, and explanation generation. These tasks involve processing textual and visual information to predict truthfulness labels and generate coherent explanations. The dataset challenges state-of-the-art neural architectures in these subtasks, aiming to improve the performance of end-to-end multimodal fact-checking and explanation generation.''',\n",
    "    'MOT': '''The MOTChallenge benchmark is designed to evaluate the performance of multiple object tracking (MOT) algorithms. Its main goals are to provide a standardized evaluation methodology for MOT algorithms, to encourage the development of new and improved algorithms, and to facilitate the comparison of different algorithms. The benchmark consists of a collection of datasets, each containing video sequences of people moving in different environments. The datasets are annotated with ground-truth data, including the locations and identities of the people in each frame. The benchmark also provides a set of pre-computed object detections, which can be used as input to the tracking algorithms. The benchmark evaluates the performance of tracking algorithms based on several metrics, including the number of correctly tracked objects, the number of false positives, and the number of missed detections. The benchmark also evaluates the speed and efficiency of the algorithms, as well as their ability to handle occlusions and other challenging scenarios. One of the defining features of the MOTChallenge benchmark is its focus on single-camera tracking. This means that the algorithms are designed to track objects within a single camera view, rather than across multiple cameras or in 3D space. This makes the benchmark particularly relevant for applications such as surveillance, robotics, and self-driving cars. The benchmark also places a strong emphasis on machine learning models, particularly deep learning models. Many of the top-performing algorithms in the benchmark use deep learning to learn appearance models for the tracked objects, which can improve the accuracy of the tracking. The benchmark also evaluates the ability of the algorithms to adapt to changing environments and to learn from new data.''',\n",
    "    'MP3D': '''The Matterport3D benchmark is a comprehensive dataset designed to enable a wide range of scene understanding tasks in computer vision. The dataset comprises 194,400 RGB-D images captured in 90 building-scale scenes, with each viewpoint sampled uniformly throughout the entire environment. The dataset includes both depth and color 360-degree panoramas for each viewpoint, and camera poses that are globally consistent and aligned with a textured surface reconstruction. Additionally, the dataset provides instance-level semantic segmentations into region and object categories, and data collected from living spaces in private homes. The main goals of the Matterport3D benchmark are to provide a large-scale, diverse, and comprehensive dataset for training and testing machine learning models for scene understanding tasks. The dataset is designed to enable supervised and self-supervised learning, and to facilitate the development of state-of-the-art algorithms for tasks such as semantic segmentation, object detection, keypoint matching, and view overlap prediction. The defining features of the Matterport3D dataset include its scale, coverage, alignment accuracy, and HDR imagery. Unlike previous datasets, Matterport3D includes data captured from stationary cameras which comprehensively sample the viewpoint space, enabling training for scenarios that often provide significant challenges in keypoint matching and tracking. The dataset also provides a globally consistent registration of images to a surface mesh, facilitating semantic annotation and enabling efficient 3D interfaces for object and region category annotation. The specific aspects of machine learning models that the Matterport3D benchmark aims to test and evaluate include their ability to learn representations of scenes, to perform keypoint matching and tracking, to detect and segment objects, and to predict view overlap.''',\n",
    "    'MRPC': '''The Microsoft Research Paraphrase Corpus (MRPC) serves as a benchmark for evaluating and testing machine learning models in the context of paraphrase identification and related natural language processing tasks. Its main goals include providing a standardized dataset for training and evaluating models, fostering research in paraphrase and sentential synonymy, and facilitating the development of shared datasets for comparison across research efforts. Key Features: 1. Dataset Size: The MRPC consists of 5801 pairs of sentences, each accompanied by binary judgments indicating whether human raters considered the pair of sentences to be similar enough in meaning to be considered close paraphrases. 2. Natural Occurrence: The corpus is derived from naturally occurring, non-constructed sentence pairs, providing a real-world basis for evaluating paraphrase recognition algorithms. 3. Public Availability: The corpus is publicly available, aiming to encourage research in areas related to paraphrase and sentential synonymy, and to establish a discourse on the proper construction of paraphrase corpora for training and evaluation. Machine Learning Model Evaluation: The MRPC aims to test and evaluate machine learning models in the following aspects: 1. Paraphrase Recognition: Assessing the ability of models to accurately identify and classify sentence pairs as paraphrases or non-paraphrases. 2. Semantic Equivalence: Evaluating models' capability to capture semantic equivalence between sentences, including partial overlap and almost-but-not-quite semantic equivalence. 3. Corpus Utility: Determining the utility of the corpus for training and evaluating paraphrase recognition algorithms, and comparing its effectiveness with other training sets and heuristic approaches. 4. Coverage and Quality Metrics: Addressing the need for reliable metrics to establish corpus coverage and quality, and exploring the potential for fine-grained use of the corpus based on individual evaluator judgments.''',\n",
    "    'MSMARCO': '''The MS MARCO (Microsoft MAchine Reading COmprehension) benchmark is designed to facilitate the development and evaluation of machine reading comprehension (MRC) and open-domain question-answering (QA) models using real-world data. Here are the key aspects of the MS MARCO benchmark: 1. Main Goals: - The primary goal of the MS MARCO benchmark is to advance the state-of-the-art in MRC and QA capabilities by providing a large-scale dataset derived from real user search queries. - It aims to enable the benchmarking of machine reading comprehension and question-answering models at scale, with a focus on real-world relevance and applicability. 2. Defining Features: - Real User Search Queries: The questions in the dataset are derived from anonymized user search queries from Bing's search logs, making them representative of actual information-seeking behavior. - Large-Scale Dataset: The dataset comprises a significant number of questions, unique answers, and relevant passages, distinguishing it from other publicly available datasets for MRC and QA. - Human-Generated Answers: The dataset includes human-generated answers that summarize the necessary information for answering the questions, adding a real-world element to the dataset. 3. Specific Aspects for Machine Learning Models: - Benchmarking New Models: The dataset is particularly attractive for benchmarking new machine learning models for MRC and neural information retrieval (IR). - Exploration of New Metrics: It encourages the exploration of new metrics such as ROUGE-2 and ROUGE-AR, as well as robust evaluation strategies, to enhance the performance of MRC models. - Multi-Task and Cross-Domain Learning: The dataset's potential for combining with other existing MRC datasets opens avenues for multi-task and cross-domain learning, providing opportunities for comprehensive model evaluation. 4. Tasks for Machine Learning Models: - Novice Task: Predicting whether a question can be answered based only on the information contained in the provided passages, and generating the correct answer if possible. - Intermediate Task: Similar to the novice task, but requiring the generated answer to be well-formed and make sense even without the context of the question and retrieved passages. - Passage Re-Ranking Task: An information retrieval challenge where the system must produce a ranking of passages given a question and a set of retrieved passages using BM25.''',\n",
    "    'MSCOCO': '''The Microsoft Common Objects in COntext (MS COCO) dataset is a benchmark for advancing the state-of-the-art in object recognition and scene understanding. Its main goal is to place the question of object recognition in the context of the broader question of scene understanding. The defining features of MS COCO include its large size, with over 2.5 million labeled instances in 328k images, and its focus on complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. The dataset contains photos of 91 object types that would be easily recognizable by a 4-year-old. MS COCO is designed to test and evaluate machine learning models in several specific aspects, including object detection, segmentation, and recognition in complex scenes. It aims to challenge models to recognize objects in their natural context, where they may be partially occluded or have complex relationships with other objects in the scene. To evaluate object detection and segmentation, MS COCO provides a set of metrics that measure the accuracy of bounding box and segmentation predictions. These metrics include average precision (AP) and intersection over union (IoU).''',\n",
    "    'MSJudge': '''The MSJudge benchmark aims to address the task of legal judgment prediction by leveraging multi-stage case representation learning. Its main goals include: 1. Comprehensive Case Representation: MSJudge aims to capture the interactions and mutual effects between the court debate, facts, and claims to provide a comprehensive representation of the case life-cycle data. 2. Judgment Prediction: The benchmark focuses on predicting the judgment result of each claim by jointly learning the identification of legal facts in the court debate and the judgment result. 3. Interpretability and Efficiency: MSJudge aims to provide interpretable predictions that can assist judges in making efficient and accurate judgments by highlighting relevant information in the court debate and displaying predicted legal facts with their certainty/uncertainty. Defining Features: - Multi-Stage Representation Learning: MSJudge employs a multi-stage learning approach to capture the nuances of legal cases across different stages, including court debate, facts, and claims. - Joint Learning with Auxiliary Task: The benchmark utilizes joint learning with an auxiliary task of judicial fact recognition to improve judgment prediction by filtering noisy and non-judicial content from the initial stages. - Interactions and Mutual Effects Visualization: MSJudge provides visualization for accessing the mutual influence between different components, such as \"debate and fact,\" \"fact and claim,\" and \"across claims,\" to enhance the interpretability of the prediction results.''',\n",
    "    'MSRVTT': '''MSR-VTT (Microsoft Research Video to Text) aims to provide a comprehensive benchmark for video understanding, particularly for the task of translating video to text. MSR-VTT is characterized by several defining features, including its large scale, comprehensive video categories, diverse video content and descriptions, and multimodal audio and video streams. The dataset contains 10,000 web video clips with 41.2 hours and 200,000 clip-sentence pairs in total, covering a wide variety of video content across 20 categories. Each clip is annotated with about 20 natural sentences by 1,327 AMT workers. The specific aspects of machine learning models that MSR-VTT aims to test and evaluate include the ability to describe video with natural language, the ability to recognize the variability and complexity of videos and their associated language, and the ability to model the spatiotemporal information in video data and pooling strategies.''',\n",
    "    'MSVD': '''The MSVD-QA benchmark is a dataset designed for the task of Video Question Answering (VideoQA). The main goal of this benchmark is to evaluate the ability of machine learning models to answer questions about short video clips, which requires a deep understanding of both the visual and textual information in the video. The MSVD-QA dataset is based on the Microsoft Research Video Description Corpus (MSR-VTT) dataset, which contains short video clips and associated textual descriptions. The dataset was constructed by automatically generating question-answer pairs from the video descriptions using a rule-based approach. The resulting dataset contains 50,505 question-answer pairs, split into training, validation, and test sets. One defining feature of the MSVD-QA dataset is that it contains short and relatively simple video clips, which makes it suitable for evaluating the performance of machine learning models on VideoQA tasks. The questions in the dataset cover a wide range of topics, including objects, actions, and events in the videos. To evaluate the performance of machine learning models on the MSVD-QA dataset, the benchmark focuses on several key aspects of VideoQA, including the ability to extract and combine visual and textual features, the ability to handle questions of varying lengths and complexity, and the ability to generate accurate and diverse answers.''',\n",
    "    'MT-Bench': '''MT-bench is a benchmark designed to evaluate the multi-turn conversation and instruction-following abilities of language models. Its main goals include assessing the core capabilities of state-of-the-art models, testing the models' ability to engage in multi-turn conversations, and evaluating their performance in addressing challenging questions. The benchmark consists of 80 high-quality multi-turn questions across various categories, aiming to differentiate models based on their core capabilities. The defining features of MT-bench include its focus on open-ended, multi-turn questions that challenge the language models to engage in complex and diverse conversations. The benchmark covers common use cases and emphasizes challenging questions to effectively distinguish between different models. Additionally, MT-bench is designed to incorporate human preferences as a direct measure of a chatbot's utility in human-AI interactions, bridging the gap between existing benchmarks and real-world applications. Specific aspects of machine learning models that MT-bench aims to test and evaluate include: 1. Multi-turn Conversation: Assessing the models' ability to maintain coherent and engaging multi-turn dialogues, reflecting real-world conversational interactions. 2. Instruction-Following Ability: Evaluating the models' capacity to follow instructions and adapt their responses based on specific prompts or tasks. 3. Core Capabilities: Differentiating models based on their reasoning, math, writing, roleplay, extraction, and coding abilities, among others, to provide a comprehensive assessment of their overall performance in diverse scenarios.''',\n",
    "    'MVQA': '''The MVQA (Music Video Question Answering) benchmark dataset is designed to evaluate the ability of machine learning models to understand and answer questions related to music videos. The dataset consists of 1,000 music video clips, each with 10 associated multiple-choice questions. The questions cover a range of topics, including song lyrics, performers, background information, and music theory. The main goal of the MVQA dataset is to test the ability of machine learning models to understand the complex relationships between visual and auditory elements in music videos and to provide accurate answers to questions based on this understanding. This requires models to have a deep understanding of the song associated with the music video, as well as knowledge of relevant performers, background information, and music theory. One defining feature of the MVQA dataset is its focus on music videos, which present a unique challenge due to their reliance on prior knowledge. Answering questions about music videos requires familiarity with the song, recognition of artists, and potentially basic music theory. This makes the MVQA dataset a valuable addition to the existing benchmarks for video question answering, which tend to focus on more general video content. Another defining feature of the MVQA dataset is its use of multiple-choice questions. This allows for a more standardized evaluation of machine learning models, as the correct answer is always one of the provided choices. The dataset also includes distractor answers that are designed to be plausible but incorrect, which further tests the ability of models to understand the nuances of music videos and provide accurate answers.''',\n",
    "    'Magazine Cover Photos': '''The Magazine Cover Photos dataset is one of the scenarios covered in the HEIM benchmark. Its main goal is to evaluate the ability of text-to-image models to generate visually appealing and contextually relevant cover photos for magazines based on textual prompts. The dataset consists of prompts related to creating cover photos for different types of magazines, with each prompt representing a different type of magazine cover. The prompts follow the format of descriptions such as \"a photo for a cover of a magazine with the headline: X\", where X is a headline for the magazine. The defining feature of the Magazine Cover Photos dataset is its focus on evaluating the ability of text-to-image models to generate visually appealing and contextually relevant cover photos that effectively represent the content and essence of the associated magazine. This requires the models to understand the purpose and target audience of each magazine, as well as the ability to generate images that enhance the overall design and user experience of the magazine cover. The specific aspects of machine learning models that the Magazine Cover Photos dataset aims to test and evaluate include their ability to: - Understand and interpret textual prompts related to creating cover photos for different types of magazines - Generate visually appealing and contextually relevant cover photos that effectively represent the content and essence of the associated magazine - Capture the essence and uniqueness of each magazine in their generated cover photos - Handle variations in the textual prompts, such as different descriptions or variations in the wording of the prompts related to the same type of magazine cover''',\n",
    "    'Mental Disorders': '''The Mental Disorders dataset is one of the scenarios covered in the HEIM benchmark. Its main goal is to evaluate the ability of text-to-image models to generate images that avoid harmful biases and stereotypes related to mental disorders. The dataset consists of prompts related to generating images of people with different mental disorders, with each prompt representing a different type of mental disorder. The prompts follow the format of descriptions such as \"a photo of the face of a person with X\", where X represents a mental disorder. The defining feature of the Mental Disorders dataset is its focus on evaluating the ability of text-to-image models to generate images that avoid harmful biases and stereotypes related to mental disorders. This requires the models to have a deep understanding of the characteristics and nuances of different mental disorders, as well as the ability to generate images that accurately represent people with mental disorders in a respectful and non-stigmatizing manner. The specific aspects of machine learning models that the Mental Disorders dataset aims to test and evaluate include their ability to: - Understand and interpret textual prompts related to generating images of people with different mental disorders - Generate images that accurately represent people with mental disorders in a respectful and non-stigmatizing manner - Avoid harmful biases and stereotypes related to mental disorders in their generated images - Handle variations in the textual prompts, such as different descriptions or variations in the wording of the prompts related to the same type of mental disorder''',\n",
    "    'MiT': '''The Moments in Time (MiT) dataset is a large-scale human-annotated collection of one million short videos corresponding to dynamic events unfolding within three seconds. The main goal of this dataset is to provide a diverse and challenging benchmark for action recognition and event understanding in both visual and auditory modalities. The defining features of this dataset include its large scale, diversity of events, and coverage of both visual and auditory modalities. Each video is tagged with one action or activity label among 339 different classes, and the dataset includes a significant intra-class variation among the categories. The specific aspects of machine learning models that the Moments in Time Dataset aims to test and evaluate include their ability to model the spatial-audio-temporal dynamics of events occurring in short videos. This poses many challenges, as meaningful events do not include only people, but also objects, animals, and natural phenomena. Visual and auditory events can be symmetrical in time (\"opening\" is \"closing\" in reverse), and either transient or sustained. In terms of machine learning models, the dataset can be used to evaluate the performance of several baseline models addressing separately, and jointly, three modalities: spatial, temporal and auditory. The dataset can also serve as a new challenge to develop models that scale to the level of complexity and abstract reasoning that a human processes on a daily basis.''',\n",
    "    'Mind2Web': '''The MIND2WEB dataset is a pioneering benchmark designed to facilitate the development and evaluation of generalist agents for the web. Its primary goals, defining features, and the specific aspects of machine learning models it aims to test and evaluate are as follows: 1. Primary Goals: - MIND2WEB aims to enable the training and evaluation of generalist agents for the web that can comprehend and execute tasks on real-world websites based on natural language instructions. - The dataset seeks to bridge the gap between simulated environments and real-world web interactions, providing a rich platform of raw, unfiltered, and dynamic data to equip models with the capacity to interact and cope with the complexities and uncertainties of real-world environments. - It aims to foster the development of agents that can comprehend and carry out tasks in a more autonomous fashion, rather than merely following prescriptive directives. 2. Defining Features: - Diverse Coverage of Domains, Websites, and Tasks: MIND2WEB boasts a collection of over 2,000 tasks curated from 137 websites spanning 31 different domains, providing a vast landscape for exploration and learning. - Use of Real-World Websites: Unlike existing datasets predominantly constructed within simulated environments, MIND2WEB replaces oversimplified simulation environments with an authentic, vibrant, and unpredictable realm of real-world websites, providing full traces of user interactions, webpage snapshots, and network traffic. - Broad Spectrum of User Interaction Patterns: MIND2WEB enables users to engage in sophisticated ways with websites, capturing all the common actions users do on websites in real life and promoting the development of agents capable of handling complex tasks. - Open-Ended Task Descriptions: Instead of providing low-level, step-by-step instructions, MIND2WEB focuses on high-level task descriptions, posing a greater yet realistic planning and grounding challenge for the agent. 3. Specific Aspects of Machine Learning Models Tested and Evaluated: - Language Understanding and Generation: MIND2WEB aims to explore the potential of large language models (LLMs) as a candidate solution towards generalist agents for the web, leveraging their grounded language understanding in complex environments with good generalizability and sample efficiency. - Generalization and Adaptation: The dataset provides a comprehensive snapshot of each website and the full interaction trace, equipping models with the capacity to interact and cope with the complexities and uncertainties of real-world environments, thereby encouraging the development of more robust and adaptive models. - Multi-Modal Information Integration: Promising future directions for MIND2WEB include integrating multi-modal information and reinforcement learning with feedback from real websites, aiming to further improve generalist agents for the web.''',\n",
    "    'MovieNet': '''The MovieNet benchmark aims to support comprehensive movie understanding by providing a holistic dataset with rich annotations and setting up several challenging benchmarks on different aspects of movie understanding. The main goals of the MovieNet benchmark include discovering filming art, recognizing middle-level entities, and understanding high-level semantics like stories. It aims to promote research on story-based long video understanding and beyond by providing a comprehensive dataset and benchmarks for evaluating machine learning models in the context of movie understanding. The defining features of the MovieNet benchmark include: 1. Holistic Annotations: MovieNet contains various types of data from multiple modalities, including trailers, photos, plot descriptions, and manual annotations such as characters with bounding boxes and identities, scene boundaries, description sentences, tags of place and action, and tags of cinematic style. This rich annotation allows for comprehensive movie understanding and supports various research topics. 2. Extensive Experiments: The benchmark facilitates extensive experiments on different aspects of movie understanding, showcasing the immeasurable value of MovieNet and highlighting the gap of current approaches towards comprehensive movie understanding. 3. Comprehensive Benchmarks: MovieNet sets up several challenging benchmarks on different aspects of movie understanding, covering tasks such as character detection, character identification, scene segmentation, action recognition, place recognition, and story understanding. These benchmarks provide a platform for evaluating machine learning models in the context of movie understanding. The specific aspects of machine learning models that the MovieNet benchmark aims to test and evaluate include: - Character Detection: Evaluating models for detecting characters in movies. - Character Identification: Assessing models for identifying characters in movies. - Scene Segmentation: Testing models for boundary detection to cut the movie by scene. - Action Recognition: Evaluating models for multi-label action classification tasks. - Place Recognition: Assessing models for multi-label place classification tasks. - Story Understanding: Testing models for segment retrieval to explore the potential of overall analytics using different aspects of MovieNet.''',\n",
    "    'MuSiQue': '''The MuSiQue benchmark, short for Multihop Questions via Single-hop Question Composition, is designed to address the challenge of multihop reasoning in question answering (QA) systems. The main goals of the MuSiQue benchmark are to create a dataset that requires proper multihop reasoning, to provide a more challenging and less cheatable benchmark for evaluating multihop QA models, and to explore the space of multihop questions that can be composed from a large collection of single-hop questions. Defining Features: 1. Composition of Multihop Questions: MuSiQue systematically explores a vast space of candidate multihop questions by composing them from a large collection of single-hop questions. 2. Stringent Filters: The benchmark applies a stringent set of filters to ensure that no sub-question can be answered without finding the answer to the previous sub-questions it is connected to, thereby emphasizing proper multihop reasoning. 3. Reduced Train-Test Leakage: MuSiQue reduces train-test leakage at the level of each single-hop question, mitigating the impact of simple memorization tricks. 4. Unanswerable Questions: The benchmark includes unanswerable multihop questions at the sub-question level, making the dataset significantly more challenging. Specific Aspects of Machine Learning Models Tested and Evaluated: 1. Multihop Reasoning: The benchmark aims to test and evaluate the ability of machine learning models to perform proper multihop reasoning, which involves connecting multiple pieces of information across paragraphs to answer complex questions. 2. Cheatability: MuSiQue evaluates the extent to which machine learning models can cheat via shortcuts or disconnected reasoning, aiming to provide a less cheatable benchmark compared to existing datasets. 3. Human-Machine Gap: The benchmark measures the gap between human performance and model performance, with a focus on creating a dataset that is more challenging for machine learning models to solve. 4. Answerable and Unanswerable Questions: MuSiQue includes a variant of the dataset with contrasting pairs of answerable and unanswerable questions, challenging models to distinguish between them and handle insufficient context. In summary, the MuSiQue benchmark is designed to push the boundaries of multihop reasoning in question answering by creating a challenging dataset that requires proper multihop reasoning, reduces cheatability, and evaluates the performance of machine learning models in handling complex multihop questions.''',\n",
    "    'MultiUN': '''The MultiUN benchmark is a multilingual corpus extracted from the official documents of the United Nations (UN) Its main goals include providing a comprehensive replatform for the development and evaluation of machine translation (MT) systems for multiple language pairs, as well as facilitating research in the field of cross-lingual applications such as parallel term extraction, cross-lingual information retrieval, and cross-lingual question answering The corpus is available in all 6 official languages of the UN and consists of around 300 million words per language Defining Features: 1. Multilingual Alignment: Aligned texts are prepared bilingually for all 21 language pairs between the 6 official UN languages 2. Common Test Set: The corpus includes a common test set for machine translation, which is renewed every half a year to provide an up-to-date evaluation replatform 3. Availability: The corpus is made available to the research community through the web site of the EuroMatrixPlus project, with the aim of extending the coverage of publicly available MT systems to additional languages 4. Corpus Collection: The acquisition procedure involves collecting documents from the ODS of the United Nations, ensuring that the documents are in the public domain 5. Sentence Alignment: The corpus includes automatically aligned sentences, which are processed using hunalign to output a sequence of bilingual sentence pairs''',\n",
    "    'Multi-News': '''The Multi-News benchmark is a significant contribution to the field of multi-document summarization, aiming to address the scarcity of large-scale multi-document summarization datasets in the news domain. The benchmark encompasses several key goals, defining features, and specific aspects of machine learning models that it aims to test and evaluate. 1. Goals: - The primary goal of the Multi-News benchmark is to provide a large-scale multi-document summarization dataset for training and evaluating machine learning models. This dataset, containing 56,216 article-summary pairs, aims to bridge the gap in the availability of such datasets in the news domain. - Another goal is to promote research and development in multi-document summarization, similar to the progress seen in single-document summarization. The benchmark aims to encourage advancements in summarization techniques for handling multiple platform documents. - Additionally, the benchmark seeks to facilitate the exploration of interactions among documents beyond simple concatenation and experimentation with summarizing longer input documents. 2. Defining Features: - The Multi-News dataset is the first large-scale multi-document summarization news dataset, addressing the sparsity and cost issues associated with human-written summaries. - It contains a substantial number of article-summary pairs, providing a rich replatform for training and evaluating multi-document summarization models. - The dataset reflects real-world scenarios by including varying numbers of platform articles per summary, mirroring the diversity of news coverage for different events. - The dataset is split into training, validation, and test sets, enabling rigorous evaluation of machine learning models. 3. Machine Learning Model Aspects: - The benchmark aims to test and evaluate machine learning models for multi-document summarization, particularly focusing on abstractive hierarchical models. - It evaluates the performance of models in terms of informativeness, fluency, and non-redundancy, using metrics such as ROUGE scores and human evaluation. - The proposed hierarchical model for neural abstractive multi-document summarization integrates a pointer-generator network and a Maximal Marginal Relevance (MMR) module to calculate sentence ranking scores based on relevancy and redundancy. This model is designed to perform competitively on the Multi-News dataset and the DUC 2004 dataset.''',\n",
    "    'MultiDoc2Dial': '''The MultiDoc2Dial benchmark is designed to address the task of modeling goal-oriented dialogues that are grounded in multiple documents from various domains. Its main goals include challenging recent advances in dialogue modeling with more realistic scenarios that are hardly addressed in prior work, and providing a dataset and task to support future significant research efforts in this direction. The benchmark aims to facilitate the development of conversational systems for various domains by focusing on conversational question answering and document-grounded dialogue modeling. Key features of the MultiDoc2Dial benchmark include: 1. Dataset Construction: The dataset contains 4796 conversations with an average of 14 turns grounded in 488 documents from four domains. It shares the same set of annotations as the Doc2Dial dataset and includes HTML mark-ups such as list, title, and document section information. 2. Dialogue Data: Each dialogue turn is annotated with role, dialogue act, human-generated utterance, and grounding span with document information. Dialogues contain one or multiple segments, where each segment indicates that all turns within it are grounded in the same document. 3. Task Formulation: The benchmark proposes two tasks for modeling dialogues grounded in multiple documents: generating the grounding document span and generating agent response given the current turn, dialogue history, and a set of documents. The MultiDoc2Dial benchmark aims to test and evaluate machine learning models in the following specific aspects: 1. Open-Book Setting: Unlike previous works that assume the associated document or text snippet is given, the MultiDoc2Dial benchmark operates in an open-book setting, addressing more realistic scenarios in goal-oriented dialogues where the associated document content is unknown and likely involves more than one document. 2. Document and Dialogue-Based Context: The benchmark aims to model the interconnected contexts from dialogues and documents by utilizing document and dialog-based structure information. This includes segmenting a document into passages while maintaining its hierarchical contextual information and experimenting with different ways to encode the current turn separately based on the intuition that the latest turn, with a change in topic, could be semantically distant from the dialogue history. 3. Retrieval and Generation Tasks: The benchmark evaluates the performances of retriever and generator in baseline models trained on the MultiDoc2Dial dataset. It focuses on predicting the next agent turn and formulates it as generation tasks.''',\n",
    "    'MultiFieldQA': '''The MultiFieldQA benchmark dataset is designed to evaluate the performance of machine learning models in the context of long context understanding across diverse fields. Its main goals include assessing the capability of models to comprehend and extract information from lengthy and diverse textual Platform, as well as to infer answers from such content. The dataset's defining features and the specific aspects of machine learning models that it aims to test and evaluate are as follows: 1. Diverse Textual Platform: The MultiFieldQA dataset comprises documents and articles from various fields, including legal documents, government reports, encyclopedias, academic papers, and more. This diversity challenges models to comprehend and extract information from a wide range of textual Platform, reflecting real-world scenarios where information may be drawn from multiple domains. 2. Long Context Understanding: The dataset emphasizes long context understanding, requiring models to process and comprehend extensive textual content. This aspect tests the ability of machine learning models to maintain coherence and extract relevant information from lengthy passages, which is crucial for tasks such as multi-document question answering and summarization. 3. Multi-Document Question Answering: The dataset includes multi-document question answering tasks, which demand models to extract and synthesize information from multiple documents to generate accurate answers. This task evaluates the capability of models to integrate information from disparate Platform and produce coherent and accurate responses. 4. Bilingual Capabilities: The MultiFieldQA dataset incorporates both English and Chinese languages, providing a comprehensive evaluation of machine learning models' bilingual capabilities in long context understanding. This aspect assesses the models' proficiency in processing and understanding long textual content in multiple languages. 5. Manual Curation: The dataset is manually curated, ensuring that the questions and answers are annotated with definitive answers as much as possible, facilitating automated evaluation. This meticulous curation aims to provide high-quality data for evaluating the performance of machine learning models. In summary, the MultiFieldQA benchmark dataset aims to evaluate machine learning models' abilities in comprehending long and diverse textual content, extracting information from multiple domains, performing multi-document question answering, and demonstrating proficiency in both English and Chinese languages.''',\n",
    "    'MultiPL-E': '''The MultiPL-E benchmark is a significant contribution to the field of code generation models, aiming to address several key objectives and challenges. Here is a comprehensive description of the MultiPL-E benchmark: 1. Main Goals: - Massively Parallel, Multi-Language Benchmark: MultiPL-E aims to create a massively parallel, multi-language benchmark for code generation. It supports 18 languages and provides a consistent set of problems across these languages, allowing for the evaluation of code generation models in diverse programming language environments. - Translation of Benchmarks: The primary goal of MultiPL-E is to translate code generation benchmarks from Python into other programming languages. This enables the evaluation of code generation models across a wide range of languages, encompassing various programming paradigms, language features, and popularity levels. - Scalability and Extensibility: MultiPL-E is designed to be scalable and extensible, allowing for the straightforward evaluation of new models, benchmarks, and languages. It provides a framework that supports the addition of new languages and benchmarks without manual effort, making it a versatile tool for ongoing research and evaluation. 2. Defining Features: - Suite of Compilers: MultiPL-E utilizes a suite of compilers to translate code generation benchmarks from Python into 18 additional programming languages. These compilers are tailored to handle the translation of function signatures, unit tests, Python-specific terminology, and type annotations, making the benchmarking process comprehensive and language-agnostic,. - Unit Test-Driven Evaluation: The benchmark leverages unit test-driven evaluation, ensuring that the generated function behavior is assessed for correctness across multiple languages. This approach provides a consistent and rigorous method for evaluating the performance of code generation models in different language contexts. - Language Diversity: MultiPL-E encompasses a diverse set of programming languages, including both niche and popular languages. This diversity allows for the exploration of the impact of language frequency and language features on model performance, providing valuable insights into the generalizability of code generation models across languages. 3. Machine Learning Model Evaluation: - State-of-the-Art Models: MultiPL-E evaluates the performance of three state-of-the-art code generation models: Codex, CodeGen, and InCoder. These models are assessed across a range of programming languages, enabling a comparative analysis of their performance and generalization capabilities. - Correlation Analysis: The benchmark investigates the correlation between model perplexity and correctness of generated code, shedding light on the suitability of perplexity as an estimate of performance. It also explores the impact of language popularity, prompt design, and static type-checking on code generation model performance, providing valuable insights into the factors influencing model effectiveness.''',\n",
    "    'MultiRC': '''The MultiRC benchmark is designed to address the challenge of multi-sentence reading comprehension by presenting a dataset of high-quality multiple-choice questions that require integrating information from multiple sentences. The main goals of MultiRC are to evaluate machine comprehension of natural language text, particularly in the context of multi-sentence inference, and to assess the reasoning skills required to answer open-ended questions. Key features of the MultiRC benchmark include: 1. Multi-sentenceness: Questions in the dataset require models to use information from multiple sentences of a paragraph, ensuring that answers cannot be derived from a single sentence alone. 2. Open-endedness: The dataset is not restricted to questions whose answers can be found verbatim in a paragraph. Instead, a set of handcrafted answer-options is provided for each question, allowing for inferences and reasoning beyond explicit text statements. 3. Diversity: The dataset is constructed using seven different Platform, providing diversity in content, style, and possible question types. 4. Scale: The dataset contains approximately 6,000 questions extracted from about 800 paragraphs, covering a wide variety and complexity of required reasoning phenomena. The specific aspects of machine learning models that MultiRC aims to test and evaluate include: 1. Multi-sentence integration: Models are evaluated on their ability to integrate information from multiple sentences to answer questions, requiring a deeper understanding of context and relationships within the text. 2. Reasoning skills: The dataset assesses models' reasoning abilities by presenting open-ended questions that go beyond simple retrieval of explicit information, challenging models to make inferences and draw conclusions based on the provided text. 3. Answer-option generalization: Unlike existing paradigms for representing answer-options, MultiRC does not constrain questions to have a single correct answer, allowing for a more realistic and open-ended assessment of model performance.''',\n",
    "    'Myopia Reward': '''The benchmark dataset to test myopia is a part of the broader effort to evaluate the safety and reliability of advanced AI systems. The main goal of this dataset is to test whether language models (LMs) exhibit a preference for smaller, short-term gains or larger, long-term gains. In other words, the dataset aims to evaluate whether LMs exhibit myopic behavior, which is characterized by a large discount factor when making decisions and a lack of inclination to form long-term plans. The defining feature of this benchmark dataset is that it consists of a set of questions that are designed to test the myopia of LMs. These questions are formulated in a multiple-choice format, where each question has two answer choices: one that corresponds to a myopic behavior and one that corresponds to a non-myopic behavior. The questions cover a range of topics, including the LM's ability to plan for the future, its willingness to take risks, and its ability to consider the long-term consequences of its actions. The specific aspects of machine learning models that this dataset aims to test and evaluate include their ability to make decisions that balance short-term and long-term gains, their ability to plan for the future, and their willingness to take risks. The dataset also aims to evaluate the extent to which LMs are aware of their own limitations and the limitations of their environment, and whether they are able to adapt their behavior accordingly.''',\n",
    "    'NBAQA': '''The NBAQA (National Basketball Association Question Answering) benchmark dataset is designed to evaluate the ability of machine learning models to understand and answer questions related to NBA games. The dataset consists of 1,000 video clips from NBA games, each with 10 associated multiple-choice questions. The questions cover a range of topics, including players, teams, games, and statistics. The main goal of the NBAQA dataset is to test the ability of machine learning models to understand the complex relationships between visual and auditory elements in NBA games and to provide accurate answers to questions based on this understanding. This requires models to have a deep understanding of the rules and strategies of basketball, as well as knowledge of relevant players, teams, games, and statistics. One defining feature of the NBAQA dataset is its focus on NBA games, which present a unique challenge due to their reliance on prior knowledge. Answering questions about NBA games requires familiarity with the rules and strategies of basketball, recognition of players and teams, and knowledge of relevant statistics. This makes the NBAQA dataset a valuable addition to the existing benchmarks for video question answering, which tend to focus on more general video content. Another defining feature of the NBAQA dataset is its use of multiple-choice questions. This allows for a more standardized evaluation of machine learning models, as the correct answer is always one of the provided choices. The dataset also includes distractor answers that are designed to be plausible but incorrect, which further tests the ability of models to understand the nuances of NBA games and provide accurate answers. Overall, the NBAQA dataset aims to evaluate the ability of machine learning models to understand and answer questions related to NBA games, which requires a deep understanding of the rules and strategies of basketball, as well as knowledge of relevant players, teams, games, and statistics.''',\n",
    "    'NFCorpus': '''The NFCorpus benchmark is a dataset designed for learning to rank in the medical domain, with the primary goal of bridging the lexical gap between user queries written in layman's English and scientific articles in the medical field. The dataset is platformd from the non-commercial NutritionFacts.org (NF) website, which provides layman-friendly descriptions of health topics and is linked to scientific papers mainly hosted on the PubMed database. The benchmark aims to address the challenges of information retrieval in the medical domain, where diverse information needs, varying language skills of users, and the lexical gap between user queries and medical information complicate the retrieval process. Key Features: 1. Full-Text Queries: The NFCorpus benchmark comprises full-text queries of various lengths, enabling the development of complete learning models. 2. Relevance Links: It includes relevance links at three levels for thousands of queries in layman's English to documents consisting of abstracts of research articles, providing a comprehensive set of relevance-linked query-document pairs for supervised training of machine learning models. 3. Public Availability: The dataset is publicly available, allowing researchers and practitioners to access and utilize it for developing and evaluating ranking models for medical information retrieval. Machine Learning Model Aspects: The NFCorpus benchmark aims to test and evaluate machine learning models for learning to rank in the medical domain, specifically focusing on the following aspects: 1. Sparse Word Association Matrices: The dataset enables the development of ranking models based on sparse word association matrices, which have been shown to outperform standard bag-of-words retrieval models. 2. Learning from Sparse Data: The benchmark provides an opportunity to explore the effectiveness of learning-to-rank algorithms in the context of sparse data, as well as the impact of word combinations on model performance. 3. Optimization of Domain-Specific Word Associations: It facilitates the optimization of domain-specific word associations for a direct ranking objective, allowing for the evaluation of models that leverage domain-specific knowledge for improved retrieval performance. In summary, the NFCorpus benchmark serves as a valuable replatform for developing, testing, and evaluating machine learning models for medical information retrieval, with a focus on addressing the lexical gap between layman's queries and scientific articles in the medical domain.''',\n",
    "    'NL2Bash': '''The NL2Bash benchmark encompasses several key aspects that define its goals, features, and the machine learning models it aims to test and evaluate. 1. Main Goals: - The primary goal of NL2Bash is to enable users to perform operations in the Linux operating system by expressing their commands in natural language. This aligns with the long-term vision of using natural language as a programming medium, making tasks such as file manipulation, search, and application-specific scripting universally accessible. - The benchmark aims to bridge the gap between natural language and code by providing a dataset of challenging but commonly used Bash commands and expert-written English descriptions. This facilitates the evaluation of machine learning models for mapping English sentences to Bash commands, with the potential to automate repetitive tasks and enhance user accessibility. 2. Defining Features: - NL2Bash introduces a large dataset of challenging but commonly used Bash commands and expert-written English descriptions, significantly larger than most existing semantic parsing benchmarks. - The dataset covers over 100 unique Bash utilities, reflecting the wide domain coverage of shell commands and the irregular syntax of command options, which presents unique challenges for semantic parsing. - The benchmark emphasizes the practicality of code snippets and expert-written natural language, making it the largest NL-to-code dataset constructed using practical code snippets and expert-written natural language [T8]. 3. Machine Learning Model Aspects: - NL2Bash evaluates the performance of existing machine learning models, including Seq2Seq and CopyNet, for NL-to-code translation tasks. These models define the conditional probability of an output sequence given the input sequence using RNN encoder-decoder architecture, with CopyNet being an extension of Seq2Seq that can select sub-sequences of the input sequence and emit them at proper places while generating the output sequence. - The benchmark also includes a stage-wise natural language programming model, Tellina, which incorporates manually-designed heuristics for argument translation. - The performance evaluation of these models demonstrates competitive performance as well as significant room for future work on this challenging semantic parsing problem. In summary, the NL2Bash benchmark aims to facilitate the mapping of English sentences to Bash commands, with the overarching goal of enabling natural language control of the operating system.''',\n",
    "    'NTU RGB+D': '''The NTU RGB+D dataset is a large-scale benchmark designed for 3D human activity analysis, with a focus on depth-based action recognition. The dataset aims to address the limitations of existing depth-based and RGB+D-based action recognition benchmarks by providing a comprehensive collection of video samples and frames, diverse action classes, and a variety of subjects and camera views. Key Goals: The primary goal of the NTU RGB+D dataset is to provide a valuable replatform for researchers and practitioners in the field of human activity analysis. By offering a large-scale dataset with over 56 thousand video samples and 4 million frames, the benchmark aims to enable the application, development, and adaptation of various data-hungry learning techniques for depth-based and RGB+D-based human activity analysis. Additionally, the dataset serves as a platform for evaluating and testing machine learning models, particularly deep learning methods, for action classification and recognition tasks. Defining Features: The NTU RGB+D dataset stands out due to several defining features: 1. Large-Scale: The dataset contains over 56 thousand video samples and 4 million frames, making it one of the largest benchmarks for 3D human activity analysis. 2. Diverse Action Classes: It includes 60 different action classes, covering daily, mutual, and health-related actions, providing a wide range of activities for analysis and classification. 3. Variety of Subjects and Camera Views: The dataset is collected from 40 distinct subjects and offers multiple camera views, providing a diverse set of data for training and evaluation. 4. Data Modalities: The dataset utilizes Microsoft Kinect v2 sensors to collect four major data modalities: depth maps, 3D joint information, RGB frames, and IR sequences. Machine Learning Model Evaluation: The NTU RGB+D dataset aims to test and evaluate machine learning models, particularly deep learning methods, for the task of action classification and recognition. It introduces a new recurrent neural network (RNN) structure to model the long-term temporal correlation of features for each body part, and utilizes them for improved action classification. Additionally, the dataset provides a platform for comparing the performance of deep learning methods against state-of-the-art handcrafted features on cross-subject and cross-view evaluation criteria.''',\n",
    "    'NarrativeQA': '''The NarrativeQA benchmark is a dataset and set of tasks designed to test for deeper comprehension of language. Its main goals are to evaluate machine learning models' ability to understand and reason about complex narratives, and to motivate further research into the development of models seeking human reading comprehension ability. The defining features of NarrativeQA are its use of complex, self-contained narratives (books and movie scripts) as documents/stories, and its focus on questions that require understanding the underlying narrative rather than relying on shallow pattern matching or salience. The questions and answers are natural, unconstrained, and human-generated, and answering questions frequently requires reference to several parts or a larger span of the context document. The dataset contains 1,567 stories, evenly split between books and movie scripts, and 46,765 question-answer pairs. The questions are grammatical questions written by human annotators, average 9.8 tokens in length, and are mostly formed as 'WH'-questions. The answers are human-written, short, averaging 4.73 tokens, but not restricted to spans from the documents. The tasks presented in NarrativeQA vary in their scope and complexity. They consider either the summary or the story as context, and for each, they evaluate answer generation and answer selection. The task of answering questions based on summaries is similar in scope to previous datasets, but the summaries contain more complex relationships and timelines than news articles or short paragraphs from the web. The more complex tasks require models to read the full stories to answer the questions, with no access to the summaries. The specific aspects of machine learning models that NarrativeQA aims to test and evaluate include their ability to read and understand entire stories, their ability to generate or rank fluent English answers to human-generated questions, and their ability to provide sensible distractors to the correct answer.''',\n",
    "    'Natural Instructions': '''The Natural Instructions benchmark is a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The main goal of this benchmark is to study the ability of machine learning models to learn new tasks by understanding the human-readable instructions that define them, and to evaluate their generalization performance across tasks. The defining feature of this benchmark is that it is based on natural language crowdsourcing instructions, which are often used to define tasks for human workers in crowdsourcing platforms. The instructions are curated from existing well-known datasets and mapped to a unified schema, providing training and evaluation data for learning from instructions. The benchmark aims to test and evaluate the ability of machine learning models to encode task-specific instructions and generate task output. Specifically, it evaluates the performance of generative pre-trained language models in encoding task-specific instructions and generating task output, and compares their performance to that of other machine learning models, such as supervised learning models and models that do not use instructions.''',\n",
    "    'NQ': '''The Natural Questions benchmark is a comprehensive data set designed to help researchers improve their question answering systems. The paper outlines three main goals for the data set: 1. To provide large-scale end-to-end training data for the QA problem. 2. To provide a data set that drives research in natural language understanding. 3. To study human performance in providing QA annotations for naturally occurring questions. The data set is unique in several ways. First, it consists of real anonymized, aggregated queries issued to the Google search engine, making it a valuable replatform for researchers interested in natural language processing. Second, the data set includes both long and short answers, as well as indications that there is no answer on the page. Finally, the data set is annotated using a 5-way annotation system, which allows for more nuanced evaluation of machine learning models. The paper also details several specific aspects of machine learning models that the Natural Questions benchmark aims to test and evaluate. These include: 1. The ability of models to understand natural language questions and identify relevant information in Wikipedia pages. 2. The ability of models to generate accurate long and short answers to questions. 3. The ability of models to identify when there is no answer to a question on a given page. 4. The ability of models to handle complex questions that require multiple pieces of information to answer.''',\n",
    "    'NoCaps': '''The novel object captioning at scale (nocaps) benchmark serves as a significant contribution to image captioning by providing a large-scale dataset and evaluation protocol for assessing the performance of machine learning models in generating captions for images containing novel objects. The benchmark aims to address the limitations of existing benchmarks and encourage the development of image captioning models capable of learning visual concepts from alternative data Platform. Here's a comprehensive description of the nocaps benchmark: 1. Main Goals: - Encouraging Learning from Alternative Data Platform: The benchmark aims to encourage the development of image captioning models that can learn visual concepts from alternative data Platform, such as object detection datasets, to expand the range of visual concepts they can accurately caption. - Assessing Performance on Novel Objects: It provides a platform for evaluating the performance of machine learning models in generating captions for images containing novel objects, which are objects that are rare or absent in the training captions. - Improving Real-World Image Captioning: By addressing the need for models to learn a larger variety of visual concepts from less supervision, the benchmark aims to accelerate progress towards image captioning in real-world scenarios. 2. Defining Features: - Large-Scale Benchmark: The benchmark consists of 166,100 human-generated captions describing 15,100 images, containing more than 500 unique object classes and many more visual concepts, providing a comprehensive and diverse dataset for evaluation. - Novel Object Classes: It contains a significant number of novel object classes that are rare or absent in training captions, with nearly 400 object classes seen in test images having no or very few associated training captions. - Evaluation Protocol: The benchmark includes an evaluation protocol and benchmark guidelines for assessing the performance of machine learning models in generating captions for images with novel objects. 3. Specific Aspects of Machine Learning Models Tested and Evaluated: - Transfer Learning and Generalization: The benchmark assesses the ability of machine learning models to maintain their performance on COCO (Common Objects in Context) while being optimized for nocaps, highlighting the challenges of transferring models across datasets and the need for generalization to novel objects. - Performance on Novel Objects: It evaluates the performance of machine learning models in generating accurate and descriptive captions for images containing novel objects, testing the models' ability to handle previously unseen visual concepts. - Improvement and Future Work: The benchmark provides insights into the limitations of existing state-of-the-art models on this task and suggests potential improvements and simple heuristics to enhance the performance of constrained beam search on the benchmark dataset. In summary, the nocaps benchmark aims to provide a comprehensive evaluation platform for image captioning models, focusing on the ability to generate accurate and descriptive captions for images containing novel objects.''',\n",
    "    'OCNLI': '''The OCNLI (Original Chinese Natural Language Inference) benchmark dataset is a part of the CLUE (Chinese Language Understanding Evaluation) benchmark. Its main goal is to evaluate the ability of machine learning models to perform natural language inference (NLI) tasks in Chinese. The OCNLI dataset consists of 56,000 inference pairs from five different genres: news, government, fiction, TV transcripts, and telephone transcripts. The premises are collected from Chinese Platform, and university students majoring in language are hired to write the hypotheses. The annotator agreement is on par with the MNLI (Multi-Genre Natural Language Inference) dataset. The OCNLI dataset is unique in that it is a non-translation-based NLI task specific to Chinese, making it more suitable than the XNLI (Cross-lingual Natural Language Inference) dataset for evaluating Chinese NLU.The dataset is designed to test the ability of models to perform tasks such as recognizing textual entailment, contradiction, and neutral relationships between pairs of sentences. The OCNLI dataset is challenging because it contains a wide range of linguistic phenomena, including idiomatic expressions, negation, and long-distance dependencies. The dataset also includes a mix of genres, which can make it difficult for models to generalize across different types of text.''',\n",
    "    'OCR-VQA': '''The OCR-VQA benchmark is a pioneering initiative that aims to address the task of visual question answering by reading text in images. Here's a comprehensive description of the benchmark: 1. Main Goals: - Introduce a novel problem: The benchmark aims to draw attention to the novel and important problem of visual question answering by reading text in images, which is referred to as OCR-VQA. - Dataset creation: The main goal is to construct a large-scale dataset specifically for the task of OCR-driven VQA, filling the gap in traditional VQA literature by focusing on text in images. - Research facilitation: The benchmark seeks to facilitate future research efforts on this novel problem across document image analysis and VQA communities. 2. Defining Features: - Large-scale dataset: The benchmark introduces OCR-VQA–200K, the first large-scale dataset for the proposed VQA task by reading text in the image. - Integration of OCR and VQA: It combines well-established techniques in OCR and VQA literature, demonstrating baseline performance on OCR-VQA–200K. - Practical challenges: The dataset presents several practical challenges and research avenues for the community, including challenges for document image analysis and VQA communities. 3. Specific Aspects of Machine Learning Models: - Baseline approach: The benchmark presents a baseline approach to evaluate visual question answering performance on OCR-VQA–200K, leveraging well-established techniques in VQA and OCR domains. - Model components: The baseline VQA system constitutes pretrained CNN features for visual representation, text block coordinates and named entity tags on OCRed text for textual representation, and bi-directional LSTM for question representation, all fed to a trainable feed forward neural network to arrive at an accurate answer. - Evaluation metrics: The benchmark provides results and error analyses of a baseline deep model that builds atop state-of-the-art VQA and OCR modules, suggesting several challenges and research avenues for both document image analysis and the VQA community.''',\n",
    "    'OK-VQA': '''The OK-VQA benchmark aims to evaluate the reasoning capabilities of machine learning models in scenarios where the answer cannot be obtained by only looking at the image. The main goal of the benchmark is to encourage methods that rely on external knowledge rePlatform to answer questions. The dataset consists of over 14,000 questions that cover a variety of knowledge categories such as science & technology, history, and sports. The questions require external rePlatform to answer, and the models need to learn what knowledge is necessary to answer the questions, determine what query to do to retrieve the necessary knowledge from an outside platform of knowledge, and incorporate the knowledge from its original representation to answer the question. The OK-VQA benchmark is diverse, difficult, and to date the largest VQA dataset focused on knowledge-based VQA in natural images. It includes questions that require a range of knowledge types, such as factual knowledge, commonsense knowledge, and domain-specific knowledge. The dataset also includes questions that require multiple steps of reasoning and questions that require the integration of multiple pieces of knowledge. The benchmark evaluates the performance of machine learning models in answering questions that require external knowledge. The models need to be able to retrieve relevant knowledge from external Platform and integrate it with the image content to answer the question. The benchmark also evaluates the models' ability to reason and perform complex tasks that require multiple steps of reasoning.''',\n",
    "    'OL-CC': '''The OpenLabel-Chinese Conversations Dataset (OL-CC) is a significant contribution to the field of natural language processing and artificial intelligence, particularly for Chinese language models. Here's a comprehensive description highlighting its main goals, defining features, and the specific aspects of machine learning models it aims to test and evaluate: Main Goals Enhancing Language Model Training: The primary goal of OL-CC is to facilitate the training of large language models (LLMs) in Chinese. It provides a diverse range of dialogue scenarios that help in improving the understanding and generation capabilities of AI models in a conversational context. Synthetic Data Generation: The dataset can be used to generate synthetic data, which is crucial for testing and improving the robustness of AI models in understanding and responding to varied conversational cues. Data Augmentation: It serves as a replatform for augmenting existing datasets, thereby providing more depth and variety to the training material available for Chinese language models. Defining Features Data Structure and Composition: OL-CC consists of over 10,000 \"instruction-answer\" pairs and more than 1,600 manually crafted instructions awaiting responses. This structure reflects real-world conversational patterns, offering a rich replatform for AI training and evaluation. Diverse Instruction Types: The dataset encompasses a wide range of instruction types, including Q&A tasks, text writing, text extraction, editing and rewriting, classification choices, brainstorming, casual conversations, and logical/mathematical tasks. This diversity ensures that models trained on this dataset can handle a broad spectrum of conversational scenarios. Crowdplatformd and Manually Generated: The dataset is uniquely crowdplatformd and manually generated, involving the participation of 276 volunteers who played roles of both human users issuing instructions and AI assistants crafting responses. This approach ensures a high level of naturalism and variability in the data. Bilingual Context: Although primarily in Chinese, the dataset may contain bilingual elements, reflecting real-world scenarios where conversations can switch between languages. Machine Learning Model Evaluation Contextual Understanding: OL-CC tests a model's ability to understand and process instructions within a given context, a critical aspect for conversational AI. Response Generation: It evaluates the model's capability in generating appropriate and contextually relevant responses, a key feature for AI assistants and chatbots. Diverse Task Handling: The varied instruction types in the dataset challenge a model's flexibility in handling different kinds of tasks, from simple Q&A to complex logical reasoning. Language Proficiency: Since the dataset is in Chinese, it serves as an excellent benchmark for assessing a model's proficiency in understanding and generating text in Chinese, including nuances and idiomatic expressions. Human-AI Interaction Dynamics: By simulating real conversation patterns between human users and AI, the dataset allows for the evaluation of how well a model can sustain natural, engaging, and coherent dialogue over multiple turns. Bias and Fairness: Given the dataset's creation by a diverse group of volunteers, it can also be a tool to assess biases in language models and their ability to handle diverse viewpoints and expressions. Known Limitations Unreviewed Responses: Not all instruction-answer pairs have undergone a review process to judge and modify the AI assistant's answers, which could lead to inconsistencies or errors in the dataset. Contributor Diversity: The dataset's content is closely related to the knowledge level and professions of the top contributors, which might introduce a certain level of bias.''',\n",
    "    'OPEN Set': '''The OPEN Set is a sub-task of the SuperCLUE benchmark, which aims to evaluate the performance of large language models (LLMs) in a Chinese context. The OPEN Set is designed to test the ability of LLMs to answer open-ended questions, which are questions that cannot be answered with a simple \"yes\" or \"no\" or a limited set of possible answers. The main goal of the OPEN Set is to evaluate the ability of LLMs to understand and generate natural language in a conversational context. The dataset is constructed based on real users' queries and is designed to evaluate LLMs' instruction-following abilities and multi-turn conversational abilities. The questions in the OPEN Set are aligned with real users' queries in either format or content and are representative of the types of questions that users might ask in real-world scenarios. The defining feature of the OPEN Set is that it consists of open-ended questions, which allow for a wide range of possible answers. This is in contrast to the CLOSE Set, which contains closed-ended questions that have a limited set of possible answers. The OPEN Set is intended to evaluate LLMs' ability to generate natural language responses that are meaningful and relevant to the user's query. The specific aspects of machine learning models that the OPEN Set aims to test and evaluate include their ability to understand and interpret natural language, their ability to generate natural language responses that are relevant and meaningful, and their ability to maintain a coherent and engaging conversation over multiple turns.''',\n",
    "    'OPENREVIEW': '''The OPENREVIEW benchmark dataset is a part of the L-Eval evaluation suite for long context language models (LCLMs). Its main goal is to provide a standardized evaluation for LCLMs in the context of scientific papers and reviews, aiming to assist researchers working on scientific papers by dealing with tasks like correcting grammar errors or typos, writing specific sections of papers, and providing valuable feedback and suggestions for authors. The defining feature of the OPENREVIEW dataset is its focus on scientific papers and reviews, which are typically lengthy and contain specialized technical language. This presents a challenge for LCLMs to understand and generate coherent and accurate content. The dataset includes papers collected from openreview.net, and the tasks include writing an Abstract section, summarizing the related work, and providing feedback including valuable suggestions and questions for the authors. Specifically, the OPENREVIEW dataset aims to test and evaluate the ability of LCLMs to understand and summarize technical content, generate coherent and accurate scientific writing, and provide valuable feedback and suggestions for authors. This requires the models to have a deep understanding of scientific concepts and terminology, as well as the ability to generate human-like responses and suggestions. Overall, the OPENREVIEW benchmark dataset provides a challenging evaluation for LCLMs in the context of scientific papers and reviews, testing their ability to understand, summarize, and provide valuable feedback on technical and specialized content.''',\n",
    "    'Omnibenchmark': '''The OmniBenchmark aims evaluate the generalization ability of pre-trained models in the context of omni-vision representation across a wide range of visual realms. OmniBenchmark is unique in that it includes 21 realm-wise datasets with 7,372 concepts and 1,074,346 images, covering most visual realms comprehensively and efficiently. These datasets have no semantic concept overlap, which allows for a more accurate evaluation of the generalization ability of pre-trained models. In addition, OmniBenchmark proposes a new supervised contrastive learning framework, Relational Contrastive learning (ReCo), which aims to encode the semantic relation information in the supervised contrastive learning framework. ReCo pulls two instances from the same semantic realm closer, encoding the semantic relation between concepts, facilitating omni-vision representation learning. OmniBenchmark evaluates 22 recent representation learning approaches on its datasets, including architectures from CNNs to transformers and learning paradigms from fully-supervised learning to self-supervised learning. The evaluation protocol is pretrain-train-test, and the models are diagnosed through linear probing.''',\n",
    "    'One-box Tendency': '''The one-box tendency benchmark aims to test the decision-making abilities of machine learning models, specifically their tendency to choose a single option over multiple options. The benchmark is based on Newcomb's Paradox, a classic thought experiment in decision theory. The main goal of the benchmark is to evaluate whether a machine learning model has a \"one-box\" or \"two-box\" tendency when presented with a novel version of Newcomb's Paradox. In this scenario, the model is presented with two boxes, one of which contains a large reward and the other of which is empty. The model must choose whether to take both boxes or only the box containing the reward. The \"one-box\" tendency refers to the decision to take only the box containing the reward, while the \"two-box\" tendency refers to the decision to take both boxes. The defining feature of the benchmark is its use of Newcomb's Paradox to test the decision-making abilities of machine learning models. The benchmark is designed to be flexible and adaptable, allowing researchers to create novel versions of the paradox to test different aspects of decision-making. The benchmark aims to test and evaluate several specific aspects of machine learning models, including their ability to reason about uncertainty, their ability to make decisions based on incomplete information, and their ability to balance short-term and long-term rewards. By evaluating these aspects of machine learning models, the benchmark can provide insights into the strengths and weaknesses of different models and help researchers develop more effective decision-making algorithms.''',\n",
    "    'OpenbookQA': '''The OpenBookQA benchmark is designed to evaluate machine learning models' ability to answer science-related questions by combining specific factual knowledge with broader common knowledge. Here are the key aspects of the OpenBookQA benchmark: 1. Main Goals: - Evaluate the ability of machine learning models to answer science-related questions by combining specific factual knowledge with broader common knowledge. - Assess the models' capability for multi-hop reasoning, which involves combining partial context provided by the factual knowledge with external common knowledge to arrive at the correct answer. - Highlight the challenges of retrieving and utilizing relevant knowledge from an \"open book\" setting, where the provided factual knowledge is not self-contained and requires external knowledge for answering questions. 2. Defining Features: - The dataset consists of about 6,000 4-way multiple-choice questions, each associated with a core fact from a provided \"open book\" and an auxiliary set of additional facts. - The \"open book\" comprises recurring science themes and principles, instantiated into multiple questions, and is platformd from the WorldTree corpus, specifically a subset of 1326 general facts applicable to multiple situations. - The questions are designed to probe an understanding of these facts and their application to novel situations, requiring the combination of open book facts with broad common knowledge obtained from other Platform. - The dataset is created through a multi-stage crowdsourcing and partial expert filtering process to ensure question quality and answerability. 3. Machine Learning Model Aspects: - The benchmark aims to test and evaluate machine learning models' ability to incorporate external knowledge, such as common knowledge retrieved from Platform like ConceptNet, into the question-answering process. - It evaluates the models' performance in multi-hop reasoning, where they need to navigate through partial context provided by the open book facts and leverage external knowledge to arrive at the correct answers. - The benchmark also highlights the limitations of existing QA systems for science and the potential for simple neural baselines, as well as the impact of human bias in crowdplatformd datasets on model performance. In summary, the OpenBookQA benchmark presents a unique challenge for machine learning models by requiring them to combine specific factual knowledge with broader common knowledge for answering science-related questions, thus testing their ability for multi-hop reasoning and knowledge incorporation.''',\n",
    "    'OpenEval (Code)': '''The OpenEval benchmark is a dataset designed for evaluating the functional correctness of Large Language Models (LLMs) in code generation tasks. It consists of 178 function-level programming problems platformd from open-platform programming competition websites. Each problem includes a function signature, a documentation string, and a code body, with five test cases per problem. The documentation strings are generated using OpenAI's GPT-3.5 in a few-shot method and are manually reviewed for quality. To avoid test case leakage affecting model performance, test cases are excluded from the model input prompt​​.''',\n",
    "    'PAXION': '''The PAXION benchmark is a comprehensive evaluation framework designed to assess the action knowledge understanding of machine learning models, particularly video-language models (VidLMs). Here's a detailed exploration of its main goals, defining features, and specific aspects it aims to test and evaluate: 1. Main Goals: - Assessment of Action Knowledge: PAXION aims to evaluate the ability of machine learning models, especially VidLMs, to understand and comprehend action knowledge. This includes the understanding of textual, visual, and temporal aspects of actions. - Diagnostic Tasks: It introduces diagnostic tasks, such as the Action Antonym and Video Reversal tasks, to probe the multimodal alignment capabilities and temporal understanding skills of the models. 2. Defining Features: - Action Dynamics Benchmark (ActionBench): PAXION introduces the ActionBench, which contains carefully designed probing tasks to evaluate action knowledge understanding. These tasks are specifically tailored to assess the model's comprehension of action-related concepts. - DVDM Objective: PAXION incorporates the Discriminative Video Dynamics Modeling (DVDM) objective, which is designed to train the Knowledge Patcher network. The DVDM objective forces the model to encode the correlation between action text and the correct ordering of video frames, thereby enhancing the model's action knowledge understanding. 3. Specific Aspects Tested and Evaluated: - Multimodal Alignment Capabilities: PAXION evaluates the model's ability to align and integrate information from both textual and visual modalities to understand actions. - Temporal Understanding Skills: The benchmark assesses the model's capability to comprehend temporal relationships and dynamics within video data, particularly in the context of actions. - Compositional Understanding and Order Relations: PAXION aims to test the model's understanding of compositional and order relations in images and videos, focusing on actions as complex and dynamic events. - Object Recognition vs. Action Understanding: It aims to identify whether the model relies solely on object recognition abilities as a shortcut for action understanding, highlighting the need for genuine action comprehension.''',\n",
    "    'PIQA': '''The PIQA benchmark is a new dataset designed to evaluate and study physical commonsense understanding in natural language models. Its main goals are to provide insight into the limitations of current AI models in answering questions about the physical world, and to encourage further research into building language representations that capture details of the real world. The dataset consists of multiple-choice questions that require concrete physical reasoning, with two possible solutions and exactly one correct answer. The questions are derived from how-to instructions on instructables.com, which provide a scaffold for the annotators. The dataset is designed to be challenging for current state-of-the-art models, and to require a deep understanding of physical properties of the world around us. The defining feature of PIQA is its focus on physical commonsense understanding, which is the ability to reason about the physical world based on common sense knowledge. This is a challenging task for natural language models, as it requires not only linguistic understanding but also a deep understanding of the physical properties of objects and their interactions. Overall, the PIQA benchmark is a valuable replatform for evaluating and studying physical commonsense understanding in natural language models.''',\n",
    "    'POIE': '''The Products for OCR and Information Extraction (POIE) dataset is a large-scale benchmark for visual information extraction (VIE) in the wild. Its main goal is to address the limitations of existing benchmark datasets by providing a more diverse and challenging dataset for VIE. POIE contains 3,000 images of Nutrition Facts labels from various products in the real world, with over 111,155 text instances. The images are in diverse layouts and distorted with various folds, bends, deformations, and perspectives. Additionally, POIE has up to 21 kinds of entities, some of which come in numerous forms, making it more challenging for VIE in the wild. The specific aspects of machine learning models that POIE aims to test and evaluate are detection (DET), recognition (REC), and information extraction (IE) in the end-to-end pipeline. To address the limitations of existing end-to-end approaches, POIE proposes a novel framework that combines OCR and information extraction in an end-to-end learning fashion. The framework includes a text detector, a Contrast-guided Feature Adjustment Module (CFAM), a recognizer module, and an information extraction module. The key innovation of this framework is the introduction of contrastive learning to effectively bridge the modules of OCR and information extraction.''',\n",
    "    'PartiPrompts': '''PartiPrompts is a benchmark dataset designed to evaluate and test the capabilities of machine learning models, particularly text-to-image models. The dataset consists of over 1600 English prompts categorized into various scenarios, covering a wide range of visual and textual reasoning tasks. The main goals of the PartiPrompts (P2) benchmark dataset include assessing the compositional and reasoning abilities of text-to-image models, as well as their capacity to generate diverse and contextually relevant visual outputs based on textual input. The defining features of the PartiPrompts (P2) benchmark dataset include its structured suite of English prompts, which are carefully curated to probe the semantic properties of text-to-image models. These properties encompass aspects such as compositionality, cardinality, spatial relations, and other visual and textual reasoning tasks. The dataset aims to provide a comprehensive and diverse set of prompts that challenge the models to understand and generate images based on complex textual descriptions. Specific aspects of machine learning models that the PartiPrompts (P2) dataset aims to test and evaluate include: 1. Compositionality: The dataset includes prompts that require the models to understand and represent compositional elements in both textual and visual domains, assessing their ability to combine and synthesize diverse visual components based on textual input. 2. Cardinality: It challenges the models to comprehend and generate images based on quantitative descriptions, such as counting objects or specifying numerical attributes within the visual context. 3. Spatial Relations: The dataset evaluates the models' capability to understand and depict spatial relationships between objects, scenes, or elements described in the textual prompts, testing their spatial reasoning and representation skills. 4. Reasoning and Knowledge: It includes prompts that involve various categories of visual and textual reasoning, such as rare combinations of objects, positional relationships, and other complex reasoning tasks, aiming to assess the models' capacity for nuanced and contextually relevant image generation based on textual input.''',\n",
    "    'PassageCount': '''The PassageCount benchmark dataset is designed to evaluate the ability of machine learning models to utilize the full context to resolve a task. Its main goal is to test the models' proficiency in determining the number of unique passages among a given set of passages. The dataset's defining features and the specific aspects of machine learning models that it aims to test and evaluate are as follows: 1. Utilizing Full Context: The PassageCount dataset emphasizes the importance of utilizing the full context to resolve a task. The models are required to determine the number of unique passages among a given set of passages, which demands the models to consider the entire context and establish attention across long-range sequences. 2. Random Sampling: The dataset is constructed by randomly selecting several passages from English Wikipedia, repeating each paragraph at random several times, and finally shuffling the paragraphs. This random sampling ensures that the models cannot rely on specific patterns or biases in the data to solve the task. 3. Variable Passage Numbers: The dataset includes a variable number of passages in each data entry, ranging from 17 to 50. This variability challenges the models to handle different levels of complexity and adapt to varying numbers of passages. 4. Automated Evaluation: The dataset includes definitive answers for each data entry, facilitating automated evaluation of the models' performance. This automated evaluation ensures that the models' proficiency in utilizing the full context to resolve the task can be accurately measured. In summary, the PassageCount benchmark dataset aims to evaluate the ability of machine learning models to utilize the full context to determine the number of unique passages among a given set of passages.''',\n",
    "    'PassageRetrieval': '''PassageRetrieval is a benchmark dataset included in the LongBench suite, which is designed to evaluate the long context understanding abilities of language models. The main goal of PassageRetrieval is to test the ability of models to retrieve relevant passages from a long context given a query. The dataset is constructed based on English Wikipedia and the Chinese sections of the C4 dataset. For each data entry, 30 passages are randomly sampled, and one of them is selected for summarization using GPT-3.5-Turbo. The task then asks the model to identify the original paragraph to which the crafted summary corresponds. PassageRetrieval is unique in that it requires models to not only understand the context of a long document but also to retrieve the most relevant passage based on a given query. This task tests the models' ability to comprehend and reason over long documents, as well as their ability to perform information retrieval. To evaluate the performance of models on PassageRetrieval, LongBench uses automatic metrics such as ROUGE-L and F1 to measure the similarity of the model's output to the ground truth answer. The results of the evaluation provide insights into the multi-task capability of current models in terms of long context comprehension and information retrieval.''',\n",
    "    'Perception Test': '''The Perception Test benchmark is designed to comprehensively assess the capabilities of multimodal perception models across different skill areas, types of reasoning, and modalities. Here are the key aspects of the Perception Test benchmark: 1. Main Goals: - The main goal of the Perception Test is to provide a diagnostic benchmark for evaluating the perception and reasoning skills of pre-trained multimodal models. It aims to assess the transfer capabilities of these models in a zero-shot/few-shot or limited fine-tuning regime. - The benchmark is not designed to be a large-scale training dataset but rather an evaluation benchmark with limited fine-tuning or prompting data, meant to assess the transfer capabilities of models. 2. Defining Features: - Multimodal Evaluation: The benchmark evaluates models across different modalities, including video, audio, and text, to provide a comprehensive assessment of their capabilities. - Real-World Videos: The benchmark consists of 11.6k real-world videos, with an average length of 23 seconds, designed to show perceptually interesting situations, filmed by around 100 participants worldwide. - Dense Annotations: The videos are densely annotated with six types of labels, including multiple-choice and grounded video question-answers, object and point tracks, temporal action and sound segments, enabling both language and non-language evaluations. - Generalization-Focused Evaluation Regime: The benchmark proposes a generalization-focused evaluation regime to assess the transfer abilities of models, allowing for more robust assessment of their capabilities. 3. Specific Aspects of Machine Learning Models Tested: - Skill Areas: The benchmark covers four skill areas - Memory, Abstraction, Physics, and Semantics - within which several skills are tested through tasks that require different types of reasoning: descriptive, explanatory, predictive, or counterfactual. - Types of Reasoning: The benchmark evaluates models based on four types of reasoning: descriptive, explanatory, predictive, and counterfactual, across video, audio, and text modalities. - Transfer Capabilities: The benchmark probes pre-trained models for their transfer capabilities in a zero-shot/few-shot or limited fine-tuning regime, aiming to assess their generalization to real-world operation. In summary, the Perception Test benchmark aims to provide a comprehensive evaluation tool for assessing the perception and reasoning skills of pre-trained multimodal models across different skill areas, types of reasoning, and modalities, with a focus on generalization and transfer capabilities.''',\n",
    "    'Pets37': '''The Oxford-IIIT Pet Dataset is a collection of pet images that is widely used in computer vision research. It is a 37-category pet image dataset with roughly 200 images for each class. The images have large variations in scale, pose, and lighting, and all images have an associated ground truth annotation of breed. The main goal of the Oxford-IIIT Pet Dataset is to provide a benchmark for fine-grained classification tasks in computer vision. The dataset is designed to test and evaluate machine learning models that can recognize the breed of a pet from an image. The dataset is also useful for testing and evaluating models that can perform other tasks, such as object detection, segmentation, and localization. The defining features of the Oxford-IIIT Pet Dataset are its large size, high quality, and diversity. The dataset contains a large number of images, which makes it suitable for training deep learning models. The images are of high quality, which means that they are well-lit and well-composed. The images are also diverse, which means that they cover a wide range of breeds, poses, and lighting conditions. The Oxford-IIIT Pet Dataset is useful for testing and evaluating machine learning models that can recognize the breed of a pet from an image. The dataset is also useful for testing and evaluating models that can perform other tasks, such as object detection, segmentation, and localization. The dataset is often used as a benchmark for fine-grained classification tasks in computer vision. In summary, the Oxford-IIIT Pet Dataset is a large, high-quality, and diverse collection of pet images that is widely used in computer vision research. Its main goal is to provide a benchmark for fine-grained classification tasks in computer vision, and it is useful for testing and evaluating machine learning models that can recognize the breed of a pet from an image. The dataset is also useful for testing and evaluating models that can perform other tasks, such as object detection, segmentation, and localization.''',\n",
    "    'Places': '''The Places benchmark is a significant component of the Places Database, designed to evaluate and test machine learning models for scene understanding. Its main goals, defining features, and specific aspects for testing and evaluating machine learning models are detailed below: 1. Main Goals: - The primary goal of the Places benchmark is to provide a standardized and comprehensive evaluation framework for scene understanding and visual recognition tasks. - It aims to assess the performance of machine learning models, particularly Convolutional Neural Networks (CNNs), in classifying and understanding scenes depicted in images. - By offering a diverse and extensive collection of scene photographs labeled with semantic categories and attributes, the benchmark facilitates the development and assessment of algorithms for scene recognition and understanding. 2. Defining Features: - High-Coverage and High-Diversity: The benchmark encompasses a vast repository of 10 million scene photographs, covering 476 scene semantic categories and attributes. This high-coverage and high-diversity of exemplars enable a comprehensive representation of the types of visual environments encountered in the world. - Quasi-Exhaustive List of Scene Categories: The benchmark provides an exhaustive list of scene categories, bounded by spaces where a human body would fit, ensuring a comprehensive coverage of the categorical space for scene understanding. - Multiple Subsets for Evaluation: The benchmark includes subsets such as Places205, Places88, Places365-Standard, and Places365-Challenge, each tailored for specific evaluation tasks and competitions. 3. Specific Aspects for Testing and Evaluation: - Scene Classification: The benchmark evaluates the ability of machine learning models to accurately classify scenes into semantic categories, such as indoor, outdoor, urban, rural, etc. - Semantic Understanding: It assesses the models' capability to understand and interpret the semantic attributes and characteristics of scenes, enabling deeper understanding beyond simple classification. - Benchmarking Performance: The benchmark provides standardized training, validation, and test sets, allowing for fair and consistent evaluation of different machine learning models and algorithms. - Comparison Across Datasets: It facilitates comparisons across different scene-centric databases, such as ImageNet and SUN, to gauge the performance of models in diverse contexts and datasets.''',\n",
    "    'Power-seeking': '''The benchmark to test the desire for power and wealth is part of a larger effort to evaluate the safety of advanced AI systems. Specifically, it aims to test whether machine learning models have a disposition towards seeking power, influence, and wealth, which could potentially lead to harmful behavior. The benchmark consists of a set of few-shot samples and multiple-choice questions that are designed to test the desire for power and wealth in machine learning models. The samples are generated using prompts that ask the model to generate text that reflects a desire for power or wealth. The multiple-choice questions are then generated based on the samples, and are designed to test the model's understanding of the concepts of power and wealth, as well as its ability to reason about them. The defining feature of this benchmark is its focus on testing the desire for power and wealth in machine learning models. This is an important aspect of evaluating the safety of advanced AI systems, as models with a strong desire for power and wealth could potentially engage in harmful behavior in order to achieve their goals. In terms of the specific aspects of machine learning models that this benchmark aims to test and evaluate, it focuses on their ability to generate text that reflects a desire for power and wealth, as well as their ability to reason about these concepts in the context of multiple-choice questions. This requires models to have a deep understanding of the concepts of power and wealth, as well as the ability to reason about them in a nuanced and sophisticated way.''',\n",
    "    'PubMedQA': '''PubMedQA is a benchmark dataset for biomedical research question answering that aims to test the scientific reasoning abilities of machine reading comprehension models. The dataset consists of 1k expert-annotated, 61.2k unlabeled, and 211.3k artificially generated QA instances, each composed of a question, a context, a long answer, and a yes/no/maybe answer that summarizes the conclusion of the long answer. The main goal of PubMedQA is to evaluate the ability of machine learning models to reason over biomedical research texts and their quantitative contents. Unlike other biomedical QA datasets, PubMedQA requires complex quantitative reasoning to answer questions with yes/no/maybe answers. The dataset is also unique in that it includes a large number of artificially generated instances, which allows for more comprehensive evaluation of models' generalization abilities. PubMedQA evaluates models on several specific aspects of machine learning, including natural language understanding, scientific reasoning, and quantitative reasoning. The dataset requires models to understand complex biomedical research texts, reason over their contents, and use quantitative information to arrive at a yes/no/maybe answer. Additionally, PubMedQA evaluates models' ability to generalize to new instances and handle noisy data, as the dataset includes a large number of artificially generated instances with varying levels of difficulty.''',\n",
    "    'QASC': '''The QASC benchmark is a multi-hop question answering dataset designed to test and evaluate machine learning models' ability to reason and retrieve information from multiple Platform. Its main goals are to promote research on multi-sentence machine comprehension, develop models that can identify valid compositions of retrieved facts using commonsense reasoning, and improve the performance of existing language models. The defining features of the QASC dataset are its annotated facts and compositions, which are not evident from the question itself. This makes retrieval challenging, as the system must introduce new concepts or relations to discover potential decompositions. To address this challenge, the dataset provides annotation for supporting facts and their composition. The dataset also includes multiple-choice questions, with four answer choices, and a corpus of scientific texts from which the facts are drawn. The QASC benchmark aims to test and evaluate machine learning models' ability to perform multi-hop reasoning and retrieval. Specifically, it evaluates models' ability to retrieve relevant facts from the corpus, identify valid compositions of these facts using commonsense reasoning, and select the correct answer choice from the multiple-choice options. The benchmark also evaluates models' performance on different subsets of the dataset, such as questions that require more or less reasoning, and questions that require specific types of reasoning, such as causal or temporal reasoning.''',\n",
    "    'QMSum': '''QMSum is a new benchmark for query-based multi-domain meeting summarization. Its main goal is to provide a dataset that can be used to evaluate the performance of machine learning models in selecting and summarizing relevant spans of meetings in response to a query. The defining features of QMSum include its multi-domain setting, which includes meetings from different domains such as product, academic, and committee meetings. This allows for the evaluation of models' generalization ability on meetings of different domains and helps create more robust models. Additionally, QMSum includes 1,808 query-summary pairs over 232 meetings, making it the largest meeting summarization dataset to date. The specific aspects of machine learning models that QMSum aims to test and evaluate include their ability to accurately select and summarize relevant spans of meetings in response to a query, as well as their ability to generate summaries that are fluent and faithful to the meeting transcripts and queries. QMSum also aims to evaluate models' generalization ability on meetings of different domains and their ability to overcome bias and distribution mismatch issues.''',\n",
    "    'QNLI': '''The QNLI (Question-answering Natural Language Inference) benchmark dataset is designed to evaluate the ability of machine learning models to perform natural language inference (NLI) in the context of question-answering tasks. The main goals, defining features, and specific aspects of machine learning models that the QNLI dataset aims to test and evaluate are as follows: 1. Main Goals: - The primary goal of the QNLI dataset is to assess the capability of models to understand and reason about natural language in the context of question-answering. This involves comprehending a given question and a corresponding paragraph, and then determining whether the paragraph contains the answer to the question. - By focusing on question-answering NLI, the dataset aims to evaluate the ability of models to perform complex linguistic reasoning and inference, particularly in the context of information retrieval and understanding. 2. Defining Features: - The QNLI dataset consists of question-paragraph pairs, where the task is to determine whether the provided paragraph contains the answer to the given question. - The paragraphs are drawn from Wikipedia, adding real-world complexity and diversity to the dataset. - The dataset is designed to test models' ability to comprehend and reason about natural language in a question-answering context, making it a valuable replatform for evaluating the performance of models in real-world applications that involve information retrieval and understanding. 3. Specific Aspects for Model Evaluation: - The QNLI dataset evaluates models on their ability to perform textual entailment in the context of question-answering. This involves assessing whether a given question entails, contradicts, or is neutral with respect to the information contained in the corresponding paragraph. - Models are expected to demonstrate proficiency in understanding the semantic relationship between the question and the paragraph, as well as the ability to identify whether the information necessary to answer the question is present in the paragraph. - The dataset also serves as a means to evaluate models' performance in handling linguistic complexities, such as lexical entailment, negation, and logical reasoning, within the context of question-answering tasks.''',\n",
    "    'QQP': '''The Quora Question Pairs benchmark is designed to address the issue of identifying duplicate questions on the Quora platform. The main goals of this benchmark include reducing the burden on responders by avoiding the need to answer the same question multiple times, and improving the overall user experience by directing users to the best responses that already exist The benchmark aims to achieve this by leveraging machine learning models to accurately determine if two given questions have the same meaning or not. Defining Features: 1. Dataset: The benchmark utilizes the Quora Question Pairs dataset, which consists of pairs of questions along with labels indicating whether the questions are duplicates of each other 2. Binary Classification: The task of duplicate question detection is framed as a binary classification problem, where the model needs to determine if two sentences have the same meaning or not 3. Natural Language Understanding (NLU): The benchmark requires the model to successfully perform NLU, which involves building representations of human language and capturing the lexical and syntactic meanings of sentences presented. Feature Engineering: Traditional methods involving hand-engineered feature generation, combined with tree-based models such as random forests, are commonly used in industry for this task Specific Aspects of Machine Learning Models: 1. Linear and Tree-Based Models: The benchmark explores the performance of simple linear models as a baseline, as well as tree-based models such as Support Vector Machines (SVM), gradient boosted trees, and Random Forests 2. Deep Neural Networks: The study also delves into the use of deep neural networks, including recurrent models with attention and Continuous Bag of Words (CBOW) models, to address the task of duplicate question detection. 3. Word Embeddings: The use of word embeddings is highlighted as a key aspect of neural network models, enabling them to better model similar words and perform well in capturing semantic similarity 4. Model Performance: The benchmark aims to evaluate the performance of the machine learning models in terms of accuracy, F-score, and their ability to handle domain transfer.''',\n",
    "    'Qasper': '''The QASPER benchmark is designed to address the challenge of information-seeking question answering over academic research papers. Here are the key aspects of the QASPER benchmark: 1. Main Goals: - The main goal of QASPER is to facilitate the development of Question Answering systems that can effectively extract information from academic research papers. It aims to provide a dataset that reflects the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper - QASPER seeks to enable the evaluation of machine learning models in the context of document-grounded, information-seeking question answering. It aims to address the limitations of existing information-seeking question answering datasets, which usually contain questions about generic factoid-type information, by focusing on questions anchored in specific research papers 2. Defining Features: - QASPER dataset contains 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers - The dataset includes a variety of answer types, including extractive, abstractive, yes/no, and unanswerable questions, which presents a challenge for traditional QA models - QASPER differs from other datasets in that it focuses on questions grounded in a single paper of interest and uses the paper full text, not just the abstract. It poses a challenging full document-level task while other related datasets are abstract-level 3. Specific Aspects of Machine Learning Models: - QASPER aims to test and evaluate machine learning models in the context of document-level question answering, which presents challenges such as handling long input documents and supporting various answer types. - The benchmark evaluates the performance of state-of-the-art document-level Transformer models in tasks such as selecting evidence and generating answers. It has been observed that existing models underperform humans by a significant margin, indicating the difficulty of the tasks - The dataset also explores the potential of better pretraining and domain-adaptation to improve model performance, indicating the need for further research in this domain''',\n",
    "    'QuAC': '''The QuAC benchmark is a large-scale dataset for Question Answering in Context, designed to enable learning from rich information-seeking dialog. Its main goals are to model conversations between students and teachers, where the students ask questions to learn about a topic of interest and the teachers provide short excerpts from a hidden Wikipedia text to answer those questions. QuAC contains 14K crowdplatformd QA dialogs (100K total QA pairs) involving two crowd workers: a student who poses a sequence of freeform questions, and a teacher who answers the questions by providing short excerpts from the text. The questions can be highly context-dependent, elliptical, and even unanswerable, introducing challenges not found in existing machine comprehension datasets. The defining features of QuAC include its focus on multi-turn text-based dialog acts, its evaluation of simple and unanswerable questions where the asker can't see evidence.''',\n",
    "    'QuALITY': '''The QuALITY benchmark introduced has several main goals. First, it aims to provide a new challenge for natural language understanding models by requiring them to answer multiple-choice questions based on long-context passages. Second, it seeks to evaluate models' ability to understand and reason about complex, multi-faceted information in a holistic manner. Finally, it aims to address some of the limitations of existing benchmarks, such as their focus on short, isolated sentences or their reliance on metrics that may not accurately reflect the quality of generated text. The defining features of the QuALITY benchmark include its use of long-context passages, which range from 2,000 to 8,000 tokens in length, and its multiple-choice format, which requires models to select the correct answer from a set of options. The questions in QuALITY are designed to be challenging and require readers to consolidate information from multiple parts of the text. Additionally, the dataset includes a subset of questions that are intentionally unanswerable under tight time constraints, which helps ensure that models cannot simply rely on skimming or simple search to achieve high performance. In terms of the specific aspects of machine learning models that QuALITY aims to test and evaluate, the benchmark focuses on models' ability to understand and reason about complex, multi-faceted information in a holistic manner. This includes their ability to identify relevant information in the passage, integrate information from multiple parts of the text, and reason about the relationships between different pieces of information. QuALITY also aims to evaluate models' ability to generate high-quality text that accurately reflects the information in the passage, rather than simply producing text that matches a set of pre-defined metrics.''',\n",
    "    'Quoref': '''QUOREF is a reading comprehension benchmark designed to evaluate the ability of machine learning models to resolve coreference in longer texts. The dataset contains over 24K span-selection questions that require resolving coreference among entities in more than 4.7K English paragraphs from Wikipedia. The main goal of QUOREF is to provide a challenging benchmark for evaluating machine comprehension of texts longer than a single sentence. Most current reading comprehension benchmarks do not contain complex coreferential phenomena, and hence fail to evaluate the ability of models to resolve coreference. QUOREF aims to fill this gap by providing a dataset that requires understanding local predicate-argument structure, but makes the reading task harder by explicitly querying anaphoric references, requiring a system to track entities throughout the discourse. QUOREF's defining feature is its focus on coreference resolution. The dataset is constructed by crowdsourcing questions over paragraphs from Wikipedia, and manual analysis confirms that most questions cannot be answered without resolving coreference. The questions are designed to challenge models to find anaphoric expressions and their referents, asking questions that connect two mentions in a paragraph. These questions mostly revolve around traditional notions of coreference, but they can also involve referential phenomena that are more nebulous. QUOREF also deals with the issue of lexical cues that shortcut complex reasoning by using a strong baseline model as an adversary in the crowdsourcing loop. This adversary is not particularly skilled at answering questions requiring coreference, but can follow obvious lexical cues—it thus helps workers avoid writing questions that shortcut coreferential reasoning. QUOREF evaluates the ability of models to resolve coreference by testing their performance on the dataset.''',\n",
    "    'RACE': '''The RACE benchmark was designed to evaluate the reading comprehension abilities of machine learning models. Its defining features include a large-scale dataset of over 28,000 passages and nearly 100,000 questions, all of which were generated by human experts to test the ability of machine learning systems to comprehend text. The questions in RACE are substantially more difficult than those in existing datasets, with a large portion of questions involving reasoning. Additionally, candidate options in RACE are human-generated sentences that may not appear in the original passage, making the task more challenging and allowing for a rich type of questions such as passage summarization and attitude analysis. RACE aims to test and evaluate the generic comprehension ability of learning models, rather than domain or style-specific abilities. It covers a broad range of topics and writing styles, including news, stories, ads, biography, philosophy, and more. The paper notes that correctly answering a large portion of questions in RACE requires the ability of reasoning, which is the most important feature as a machine comprehension dataset. RACE also offers two important subdivisions of the reasoning types in its questions, namely passage summarization and attitude analysis.''',\n",
    "    'RAFT': '''The RAFT (Real-World Annotated Few-shot Tasks) benchmark is an aggregation of English-language datasets found in the real world. 1. Main Goals: - Measure Progress in Applied Settings: Unlike existing benchmarks, RAFT is specifically designed to measure the progress of NLP models in real-world applied settings. It aims to bridge the gap between research and practice by evaluating models on economically valuable classification tasks that closely mirror how models are applied in real-world scenarios. - Incentivize Valuable Work: RAFT aims to incentivize work that is valuable for deployed systems by focusing on tasks with inherent value and challenges that are not reflected in synthetic tasks. - Provide a Template for Future Benchmarks: The benchmark is intended to provide a template for future benchmarks that closely mirror deployment, encouraging the development of benchmarks that better reflect real-world applications of NLP models. 2. Defining Features: - Few-Shot Classification: RAFT focuses on few-shot classification tasks, where models are required to complete text-based tasks given only a few task-specific examples. This setting is particularly challenging and relevant for real-world applications where labeled data may be limited. - Natural Language Instructions and Labels: Each task in RAFT comes with natural language instructions and labels in addition to 50 training examples, mimicking work that is usually given to human research assistants. - Coarse Split of Tasks: The tasks in RAFT are coarsely split into generation, classification, and retrieval. However, the benchmark primarily focuses on classification tasks due to their support for high-quality automated evaluation and their coverage of a wide range of economically valuable tasks. 3. Specific Aspects of Machine Learning Models Tested and Evaluated: - Reasoning Over Long Texts: The benchmark highlights areas where current techniques struggle, such as reasoning over long texts. This is particularly relevant for real-world applications where understanding and processing lengthy documents is essential. - Domain Expertise: Some classification tasks in RAFT are difficult for non-expert humans, reflecting the importance of domain expertise in real-world value. Therefore, the benchmark evaluates models' abilities to handle tasks that require domain-specific knowledge. - Automated Evaluation: RAFT provides a platform for automated evaluation of models on real-world tasks, allowing for the tracking of model improvements and their real-world benefits.''',\n",
    "    'RTE': '''The Recognizing Textual Entailment (RTE) benchmark is a widely used evaluation framework for natural language processing (NLP) systems. Its main goal is to determine whether the meaning of one sentence can be inferred from another sentence. RTE is a binary classification task that involves two sentences: a premise and a hypothesis. The task is to determine whether the hypothesis can be inferred from the premise. The RTE benchmark is designed to test and evaluate the ability of machine learning models to perform this task accurately. The defining features of the RTE benchmark include the use of natural language sentences, the binary classification task, and the requirement for models to perform inference. The benchmark is designed to test the ability of models to perform a specific type of inference, namely, textual entailment. Textual entailment is the relationship between two sentences where the meaning of one sentence can be inferred from the other sentence. The RTE benchmark aims to test and evaluate the ability of machine learning models to perform this type of inference accurately. The specific aspects of machine learning models that the RTE benchmark aims to test and evaluate include their ability to capture semantic and syntactic relationships between sentences, their ability to reason about the meaning of sentences, and their ability to perform inference based on this reasoning.''',\n",
    "    'ReCoRD': '''The ReCoRD (Reading Comprehension with Commonsense Reasoning Dataset) benchmark is designed to evaluate machine reading comprehension (MRC) systems' ability to perform commonsense reasoning. Here are the main aspects of the ReCoRD benchmark: 1. Main Goals: - Evaluate MRC systems' ability to reason beyond simple pattern matching and paraphrasing. - Bridge the gap between human and machine commonsense reading comprehension. - Provide a large-scale dataset that challenges MRC systems to understand and reason about natural language text in a way that reflects human-like comprehension. 2. Defining Features: - Requires commonsense reasoning: Unlike existing MRC datasets, ReCoRD contains a large portion of queries that require commonsense reasoning to be answered. - Cloze-style queries: The benchmark includes cloze-style queries with missing text spans, challenging MRC systems to use hidden commonsense knowledge to choose the best-fitting text span from a set of options. - Passage-query-answer triples: The dataset consists of passages with text spans marked, cloze-style queries, and reference answers, providing a comprehensive evaluation of MRC systems' ability to reason and comprehend natural language text. 3. Specific Aspects for Machine Learning Models: - Commonsense reasoning: MRC systems are evaluated on their ability to leverage commonsense knowledge to choose the most likely consequence or detail from a set of options based on the given passage - Cloze-style query resolution: MRC systems are expected to act like humans, reading the passage and using hidden commonsense knowledge to choose a text span that best fits the missing text in the cloze-style query - Evaluation of human competence: Baseline MRC models, including top performers on existing MRC datasets, are compared to human competence on ReCoRD, highlighting the need for advancements in MRC with commonsense reasoning In summary, the ReCoRD benchmark aims to push the boundaries of MRC by requiring systems to perform commonsense reasoning and understand natural language text in a way that goes beyond simple matching and paraphrasing.''',\n",
    "    'RealToxicityPrompts': '''The REALTOXICITYPROMPTS benchmark serves as a critical tool for evaluating the toxic degeneration in pretrained language models. Its main goals include quantifying the toxicity of multiple pretrained language models and assessing the effectiveness of methods for detoxifying generations. The benchmark dataset is designed to systematically measure the risk of toxic degeneration by pretrained language models and to provide a testbed for toxicity in conditional language generation that mirrors real-world applications, such as autocomplete systems. Key features of the REALTOXICITYPROMPTS benchmark include: 1. Large-scale Dataset: The benchmark consists of 100,000 naturally occurring prompts (sentence prefixes) extracted from a large corpus of English web text. Each prompt is paired with toxicity scores obtained from a widely used and commercially deployed toxicity detector, the PERSPECTIVE API. 2. Stratified Sampling: The prompts are sampled to ensure a balanced representation of toxic (TOXICITY ≥ 0.5) and non-toxic (TOXICITY ≤ 0.5) sentences, allowing for comprehensive evaluation of toxicity in language generation 3. Evaluation of Language Models: The benchmark facilitates the evaluation of multiple pretrained language models, enabling the quantification of the effect of prompt toxicity on the toxicity of generation from these models 4. Toxicity Metrics: Metrics such as expected maximum toxicity and empirical probability of generating toxic content are used to characterize toxic generations, providing a multi-dimensional assessment of toxicity in prompted generations Specific aspects of machine learning models that the REALTOXICITYPROMPTS benchmark aims to test and evaluate include: 1. Toxic Degeneration: The benchmark assesses the propensity of pretrained language models to generate toxic content when conditioned on the provided prompts, highlighting the potential for toxic degeneration in language generation tasks. 2. Effectiveness of Detoxification Methods: It evaluates the effectiveness of controllable generation methods, such as toxicity control tokens and swearword filters, in steering language models away from toxic content, providing insights into mitigation strategies for toxic degeneration 3. Root Causes of Toxic Generations: The benchmark aims to analyze the root cause of toxic generations in pretrained language models, potentially shedding light on the acquisition of toxicity from pretraining data and informing future research into addressing persistent toxic degeneration.''',\n",
    "    'RefCOCO': '''The RefCOCO benchmark is a significant contribution to the field of referring expression generation (REG). The benchmark aims to address the challenge of generating natural language referring expressions for objects in complex photographs of real-world scenes. Here's a comprehensive description of the RefCOCO benchmark: Main Goals: 1. Dataset Collection: The primary goal of RefCOCO is to collect a large-scale dataset of natural language referring expressions referring to objects in photographs of real-world scenes. The dataset contains 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of natural scenes. 2. Analysis of Referring Expressions: RefCOCO aims to provide an in-depth analysis of the collected dataset, including studying category-specific variations in referring expressions. This analysis helps in understanding how referring expressions vary for different object categories and how attributes are used in these expressions. Defining Features: 1. Real-World Scenes: Unlike previous REG datasets that used relatively focused domains or graphics-generated objects, RefCOCO focuses on real-world scenes, making it more representative of natural language referring expressions in everyday contexts. 2. Large-Scale Dataset: RefCOCO is larger and more varied than previous REG datasets, allowing for a comprehensive study of referring expressions in real-world scenes. 3. Category-Specific Analysis: The benchmark provides insights into how referring expressions vary for different object categories, including the types of attributes used and the influence of an object's category on the referring expressions. Specific Aspects of Machine Learning Models: 1. Attribute Selection and Generation: RefCOCO aims to test and evaluate machine learning models for generating referring expressions by jointly selecting which attributes to include in the expression and what attribute values to generate. This involves incorporating visual models for selecting attribute values and object category-specific priors. 2. Consistency and Variations: The benchmark evaluates models in terms of their ability to capture variations in referring expressions across different object categories and speakers, as well as the consistency of references to objects with respect to other nearby objects in the scene. 3. Experimental Evaluations: RefCOCO provides labeled test sets for experimental evaluations of machine learning models for generating referring expressions in real-world scenes.''',\n",
    "    'Relational Understanding': '''The Relational Understanding benchmark is a systematic empirical examination designed to assess the ability of machine learning models, specifically text-guided image generation models, to recognize and reason generatively about basic relations involving simple objects and agents. The benchmark aims to evaluate the models' capacity to understand and depict fundamental spatial and agentic relations, which are considered basic building blocks of human cognition. Here are the key aspects of the Relational Understanding benchmark: 1. Main Goals: - The primary goal of the benchmark is to assess the relational understanding capabilities of machine learning models, particularly in the context of text-guided image generation. - It seeks to provide a systematic and quantitative evaluation of the models' performance in recognizing and depicting basic physical and social relations, as studied or proposed in the cognitive, developmental, and linguistic literature. 2. Defining Features: - The benchmark focuses on a set of 15 basic relations, including 8 physical relations (e.g., in, on, under, covering) and 7 agentic relations (e.g., pushing, pulling, helping). - It intentionally uses simple prompts without attribute complexity or elaboration to capture a broad range of relations from various subdomains of human psychology. - The benchmark employs human judgments from a significant number of participants (N = 169) to provide a nuanced and quantitative examination of model performance. 3. Specific Aspects Tested and Evaluated: - The benchmark evaluates the ability of text-guided image generation models to understand and depict spatial relations such as containment (in, on, under), proximity (near), and attachment (hanging over, tied to). - It also assesses the models' capacity to recognize and generate depictions of agentic relations involving interactions between agents, such as pushing, pulling, touching, helping, and hindering. - The evaluation includes a quantitative examination of people's judgments, considering additional covariates such as the ranking of images by CLIP score, to provide a comprehensive assessment of model performance. 4. Implications for Machine Learning Models: - The benchmark's findings have implications for the development of machine learning models with human-level perception and reasoning, highlighting the current limitations in the models' relational understanding capabilities. - It aims to guide potential improvements in image generation models by identifying areas where the models fall short in understanding and representing basic relations, thereby contributing to the advancement of artificial intelligence and cognitive modeling.''',\n",
    "    'RepoBench-P': '''RepoBench-P (Pipeline) is a benchmark designed to evaluate the performance of machine learning models in the context of a complete code auto-completion system. Its main goals include assessing the models' ability to retrieve relevant code snippets and predict the next line of code, as well as testing their capability to handle complex tasks that require a combination of both retrieval and next-line prediction. Key features of RepoBench-P include: 1. Integration of Retrieval and Code Completion: RepoBench-P combines the retrieval and code completion tasks to simulate the complete process of a code auto-completion system. 2. Real-world Scenario Simulation: The benchmark aims to mirror complex real-world scenarios that practical auto-completion systems would face, thereby assessing the model’s comprehensive performance and flexibility 3. Multiple Evaluation Settings: RepoBench-P includes different settings (XF-F, XF-R, and IF) that require the model to identify the most pertinent snippets and then employ these snippets as cross-file context in conjunction with the in-file context to predict the subsequent line 4. Minimum Token Threshold and Gold Snippet Retrieval: Instead of specifying a maximum token limit, RepoBench-P defines a minimum token threshold for Python and Java, and the gold snippet retrieval process requires a minimum of 10 candidates Specific aspects of machine learning models that RepoBench-P aims to test and evaluate include: 1. Cross-File Context Handling: The benchmark assesses the models' ability to handle cross-file context, which is crucial for retrieving relevant snippets from other files and using them in conjunction with in-file context for next-line code prediction 2. Performance in Real-world Scenarios: By simulating complex real-world scenarios, RepoBench-P evaluates the models' performance in handling the intricacies of multi-file projects and navigating through code spanning several repositories 3. Comprehensive Performance and Flexibility: RepoBench-P aims to assess the models' comprehensive performance and flexibility in handling the combined retrieval and next-line prediction tasks, reflecting the entire workflow of an auto-completion system.''',\n",
    "    'RoSE': '''The ROSE (Retinal OCT-Angiography Vessel Segmentation) benchmark aims to address the challenges of automated segmentation of retinal vessels in Optical Coherence Tomography Angiography (OCTA) images. The main goals of the ROSE benchmark can be summarized as follows: 1. Dataset Creation: The benchmark aims to provide a publicly available OCTA dataset with manually graded vessel annotations for training and validation of segmentation algorithms. This dataset contains 229 OCTA images with vessel annotations, enabling researchers to develop and evaluate machine learning models for retinal vessel segmentation 2. Segmentation Model Evaluation: The benchmark evaluates the performance of various machine learning models for retinal vessel segmentation using the ROSE dataset. It aims to compare and analyze the effectiveness of different segmentation algorithms, including traditional methods and deep learning-based approaches, in accurately identifying retinal vessels in OCTA images. Defining Features of the ROSE Benchmark: 1. Comprehensive Evaluation Metrics: The benchmark employs a range of evaluation metrics to assess the segmentation performance of the models, including Area Under the ROC Curve (AUC), sensitivity, specificity, accuracy, Kappa score, false discovery rate (FDR), G-mean score, and Dice coefficient. These metrics provide a comprehensive and objective assessment of the segmentation performance 2. Subjective Comparisons: In addition to quantitative metrics, the benchmark also includes subjective comparisons of segmentation results produced by different methods. Visual inspection and comparison of the segmentation outputs help in understanding the strengths and weaknesses of each approach, particularly in preserving fine capillaries and producing visually informative results Specific Aspects of Machine Learning Models Tested and Evaluated: 1. Traditional Methods vs. Deep Learning Approaches: The benchmark evaluates both traditional segmentation methods and deep learning-based approaches for retinal vessel segmentation. It compares the performance of methods such as IPAC, COSFIRE, COOF, with deep learning models including U-Net, ResU-Net, CE-Net, DUNet, CS-Net, three-stage, and OCTA-Net. This comparison aims to demonstrate the superiority of deep learning-based methods in segmenting OCTA images, particularly due to the introduction of advanced modules and architectures 2. Inference Time and Efficiency: The benchmark also considers the inference time and efficiency of deep learning methods, highlighting the importance of efficient segmentation models for practical applications. It compares the time cost of deep learning models and emphasizes the capability of the proposed two-stage framework to achieve better segmentation performance within a relatively shorter time on both ROSE-1 and ROSE-2 subsets''',\n",
    "    'Robust04': '''The TREC 2004 Robust Retrieval Track was designed to explore methods for improving the consistency of retrieval technology, with a specific focus on poorly performing topics The track aimed to address the challenge of measuring and improving the effectiveness of retrieval systems, particularly for topics that traditionally yield lower performance. The main goals and defining features of the benchmark can be summarized as follows: 1. Emphasis on Poorly Performing Topics: Unlike traditional evaluation methodologies that focus on average system effectiveness, the TREC 2004 Robust Retrieval Track placed significant emphasis on a system's least effective topics. This approach aimed to address the importance of returning at least passable results for any topic, recognizing that individual users only experience the effectiveness of the system on their specific requests 2. Large Test Set: The 2004 edition of the track used a test set consisting of 250 topics, which allowed for a more comprehensive evaluation of system performance and the stability of evaluation measures, particularly those emphasizing poorly performing topics 3. Predicted Difficulty Ranking: Systems participating in the track were required to rank the topics by predicted difficulty, with the goal of eventually being able to use such predictions to inform topic-specific processing. This aspect aimed to explore how capable systems are at recognizing difficult topics and predicting their effectiveness without explicit relevance information. 4. Evaluation Measures: The benchmark introduced new evaluation measures, including a variant of the traditional Mean Average Precision (MAP) measure that used a geometric mean rather than an arithmetic mean to average individual topic results. This measure showed promise in appropriately emphasizing poorly performing topics while being more stable at equal topic set sizes 5. Stability Analysis: The track investigated the stability of evaluation measures for larger topic set sizes, recognizing the importance of reliable and consistent evaluation metrics for assessing system performance across a wide range of topics. In terms of the specific aspects of machine learning models that the benchmark aimed to test and evaluate, the focus was on the ability of retrieval systems to effectively handle a diverse set of topics, including those traditionally considered more challenging. This encompassed the capability of machine learning models to predict the difficulty of topics, rank topics by predicted difficulty, and improve performance on poorly performing topics. Additionally, the track aimed to assess the stability and reliability of evaluation measures, particularly in the context of emphasizing poorly performing topics, to provide insights into the effectiveness of retrieval systems across a broad spectrum of information needs and query types.''',\n",
    "    'SAMSum': '''The SAMSum Corpus is a benchmark dataset for abstractive dialogue summarization. Its main goal is to provide a high-quality replatform for researchers to develop and evaluate models that can generate concise and informative summaries of chat conversations. The defining features of the SAMSum Corpus are that it contains over 16,000 chat dialogues, each with a manually annotated summary that is short, extractive, and written in the third person. The conversations were created by linguists fluent in English, and reflect the style and register of conversations that are typically written in messenger apps. The dataset includes a diverse range of topics, from chit-chat and gossiping to arranging meetings and discussing politics. The specific aspects of machine learning models that the SAMSum Corpus aims to test and evaluate are their ability to generate abstractive summaries of chat conversations. Abstractive summarization involves generating a summary that is not simply a selection of sentences from the original text, but rather a new text that captures the most important information in a concise and coherent way. This is a challenging task for machine learning models, as it requires them to understand the nuances of natural language and to generate text that is both informative and grammatically correct. To evaluate the performance of machine learning models on the SAMSum Corpus, the study uses both automatic and linguistic evaluation metrics. The automatic evaluation metric is ROUGE, which measures the overlap between the model-generated summary and the human-generated summary. The linguistic evaluation metric involves human annotators who evaluate the quality of the model-generated summaries based on criteria such as coherence, informativeness, and grammaticality.''',\n",
    "    'SAT': '''The SAT dataset aims to assess the performance of large language models in relation to human cognition, particularly in the context of college admission tests. The SAT, or Scholastic Assessment Test, is a standardized test widely used for college admissions in the United States. The main goals of including the SAT dataset in the benchmark are to evaluate the language models' abilities in critical thinking, problem-solving, and analytical skills, which are essential for success in higher education institutions Specifically, the SAT dataset aims to test and evaluate the following aspects of machine learning models: 1. Language Comprehension: The dataset assesses the model's ability to accurately interpret the meaning of questions and comprehend the context effectively 2. Reasoning and Analytical Skills: It includes sections on logical reasoning, reading comprehension, and analytical reasoning, challenging the model's ability to analyze complex information and draw accurate conclusions 3. General Aptitude: The SAT is designed to assess general aptitude and subject-specific knowledge, reflecting the model's proficiency in identifying correct knowledge or formulas for mathematical and logical tasks 4. Performance Across Subjects: The SAT dataset covers various subjects, including English and math, allowing for the evaluation of the model's performance in different subject areas The evaluation metrics used for the SAT dataset include standard classification accuracy for multiple-choice questions and Exact Match (EM) and F1 metrics for fill-in-the-blank questions Additionally, the dataset incorporates questions from the SAT to construct the benchmark, providing a comprehensive evaluation of the language models' capabilities in relation to human cognition and college admission tests''',\n",
    "    'SEED-Bench Leaderboard': '''SEED-Bench is a benchmark designed to evaluate the generative comprehension capability of Multi-modal Large Language Models (MLLMs). Its main goal is to provide a comprehensive assessment of generative models by introducing a benchmark that spans 12 evaluation dimensions, including both image and video comprehension. SEED-Bench is unique in its scale and sophistication. It consists of 19K multiple choice questions with accurate human annotations, which is 6 times larger than existing benchmarks. The questions are generated using an advanced pipeline that targets specific evaluation dimensions and incorporates both automatic filtering and manual verification processes to ensure quality and accuracy. The benchmark aims to test and evaluate the ability of MLLMs to comprehend and generate text based on visual information from both images and videos. Specifically, it evaluates the models' ability to understand spatial and temporal relationships, recognize objects and their attributes, and generate coherent and relevant text descriptions.''',\n",
    "    'SIQA': '''The SOCIAL IQA benchmark aims to provide a large-scale replatform for learning and measuring social and emotional intelligence in computational models. It consists of 38,000 multiple choice questions that probe emotional and social intelligence in a variety of everyday situations. One defining feature of SOCIAL IQA is its focus on social commonsense reasoning. The questions are designed to cover various types of inference about people's actions being described in situational contexts. The creators of SOCIAL IQA used a crowdsourcing framework to collect QA pairs that reduce stylistic artifacts of negative answers through an adversarial question-switching method. The specific aspects of machine learning models that SOCIAL IQA aims to test and evaluate include their ability to reason about social situations and understand the pragmatic implications of everyday events.''',\n",
    "    'SNLI-VE': '''The SNLI-VE (Visual Entailment) benchmark is a dataset designed for the task of visual entailment, which requires fine-grained reasoning over both image and text modalities. The main goals of the SNLI-VE benchmark include: 1. Introducing a novel task: SNLI-VE introduces the task of visual entailment, which extends the textual entailment (TE) task to the visual domain. This allows for the evaluation of machine learning models' ability to reason about relationships between images and text. 2. Dataset construction: SNLI-VE is constructed based on two existing datasets, Flickr30k and SNLI. It leverages real-world images from Flickr30k as premises and corresponding text hypotheses from SNLI to create a structured dataset for visual entailment tasks. 3. Evaluation of fine-grained reasoning: SNLI-VE aims to evaluate machine learning models' capability for fine-grained reasoning over images and text, requiring models to understand and reason about detailed visual and textual information to make correct conclusions. Defining features of the SNLI-VE benchmark include: 1. Real-world images: The dataset uses real-world images from the Flickr30k dataset, which contain everyday activities, events, and scenes. This provides a diverse set of visual premises for the visual entailment task. 2. Textual hypotheses: The dataset includes text hypotheses from the SNLI dataset, which are used in conjunction with the images to form (image, text) pairs for the visual entailment task. Specific aspects of machine learning models that SNLI-VE aims to test and evaluate include: 1. Fine-grained reasoning: The benchmark evaluates models' ability to perform fine-grained reasoning over both images and text, requiring them to understand detailed visual and textual information to make accurate entailment predictions. 2. Visual grounding: SNLI-VE tests models' capability for visual grounding, which involves linking elements in the text hypotheses to specific entities or relationships in the image premises. 3. External knowledge integration: The benchmark encourages the integration of external knowledge beyond pre-trained models and visual entity extraction to improve model performance on the visual entailment task. 4. Attention-based models: SNLI-VE suggests the use of effective attention-based models that encode sentences together and align similar words in hypotheses and premises, as opposed to late-fusion of separately encoded modalities.''',\n",
    "    'SPACE': '''The SPACE (Summaries of Popular and Aspect-specific Customer Experiences) benchmark is a large-scale evaluation corpus for opinion summarizers. Its main goal is to facilitate future research by improving upon the shortcomings of existing datasets. SPACE is built on TripAdvisor hotel reviews and comprises general and aspect-specific summaries for 50 hotels. It includes 1.1 million reviews for over 11 thousand hotels in its training set, obtained by cleaning and downsampling an existing collection. The training set contains no reference summaries and is useful for unsupervised training. For evaluation, human annotators were asked to write high-level general summaries and aspect summaries for six popular aspects: building, cleanliness, food, location, rooms, and service. For every hotel and summary type, three reference summaries were collected from different annotators. Importantly, for every hotel, summaries were based on 100 input reviews. To the best of our knowledge, this is the largest crowdsourcing effort towards obtaining high-quality abstractive summaries of reviews, and the first to use a pool of input reviews of this scale. SPACE is the first benchmark to also contain aspect-specific opinion summaries. The large number of input reviews per entity poses certain challenges with regard to the collection of human summaries. A direct approach is prohibitive, as it would require annotators to read all 100 reviews and write a summary in a single step. A more reasonable method is to first identify a subset of input sentences that most people consider salient, and then ask annotators to summarize them. Summaries were thus created in multiple stages using the Appen platform and expert annotator channels of native English speakers. SPACE aims to test and evaluate machine learning models for extractive and abstractive opinion summarization, including models that can generate summaries for multiple aspects of an entity. It provides a realistic input to competing models, with summaries based on 100 customer reviews, an order of magnitude increase over existing corpora.''',\n",
    "    'SQA3D': '''Situated Question Answering in 3D Scenes (SQA3D) benchmark aims to evaluate the scene understanding and reasoning capabilities of embodied agents. Specifically, the benchmark requires the agent to comprehend and localize its situation in a 3D scene from a textual description, and then answer a question under that situation. The defining features of SQA3D include its use of diverse 3D scene contexts (e.g., 3D scans, egocentric videos, bird-eye view pictures), its focus on egocentric viewpoint and situated reasoning, and its wide range of questions that examine various reasoning capabilities, including spatial relation comprehension, navigation, common sense understanding, and multi-hop reasoning. In terms of machine learning models, SQA3D aims to test and evaluate the performance of state-of-the-art multi-modal reasoning models, including transformer-based vision-language models and 3D perception modules. The benchmark also explores the potential of recently-introduced large language models (LLMs) like GPT-3 and Unified QA on solving SQA3D in a zero-shot fashion.''',\n",
    "    'SQuAD': '''The Stanford Question Answering Dataset (SQuAD) is a benchmark dataset for machine reading comprehension tasks. Its main goal is to evaluate the ability of machine learning models to read and understand natural language text, and to answer questions based on that understanding. SQuAD is unique in that it contains both answerable and unanswerable questions. The answerable questions are designed to test a model's ability to extract relevant information from a given text passage and use it to answer a specific question. The unanswerable questions, on the other hand, are designed to test a model's ability to recognize when a question cannot be answered based on the information provided in the text passage. SQuAD is a large-scale dataset, containing over 100,000 questions and answers, and is diverse in terms of the topics covered. It is also considered a high-quality dataset, with a low level of noise and a high level of relevance between the questions and the text passages. In terms of specific aspects of machine learning models that SQuAD aims to test and evaluate, it focuses on natural language understanding, context-based reasoning, and the ability to handle ambiguity and uncertainty. SQuAD also encourages the development of models that can recognize when they do not have enough information to answer a question, and can abstain from providing an incorrect answer.''',\n",
    "    'SROIE': '''The SROIE (Scanned Receipt OCR and Information Extraction) benchmark is a competition that aims to evaluate the performance of machine learning models in recognizing text from scanned receipts and extracting key information from them The competition was organized in 2019 and attracted a significant number of submissions from academic, research institutes, and industry over different countries The main goals of the SROIE benchmark are to promote research and development efforts on SROIE, draw attention from the community, and evaluate the performance of OCR and information extraction technologies for scanned receipts The benchmark aims to address the challenges faced by SROIE, such as the high accuracy requirements for OCR tasks and the need for efficient and robust systems to reduce manual work The defining features of the SROIE benchmark include a new dataset with 1000 whole scanned receipt images and annotations, which are used for all three competition tasks but with different annotations The dataset contains around four key text fields, such as goods name, unit price, and total cost, and the text annotated mainly consists of digits and English characters The dataset has some special features and challenges, such as poor paper quality, poor ink and printing quality, low resolution scanner and scanning distortion, folded invoices, too many unneeded interfering texts in complex layouts, long texts, and small font sizes The SROIE benchmark evaluates the performance of machine learning models in three specific tasks: Scanned Receipt Text Localization (Task 1), Scanned Receipt OCR (Task 2), and Key Information Extraction from Scanned Receipts (Task 3) Task 1 involves localizing the text regions in the scanned receipt images, while Task 2 involves recognizing the text in the localized regions. Task 3 involves extracting key information from the recognized text, such as the total amount, date, and merchant name''',\n",
    "    'SST': '''The Stanford Sentiment Treebank (SST) is a benchmark dataset designed to advance the understanding of sentiment analysis and compositionality in natural language processing (NLP) tasks. The main goals of SST include providing rich supervised training and evaluation rePlatform, capturing complex linguistic phenomena, and enabling the development and evaluation of more powerful models of composition. Key Features of SST: 1. Fully Labeled Parse Trees: SST is the first corpus with fully labeled parse trees, allowing for a complete analysis of the compositional effects of sentiment in language. The dataset consists of 11,855 single sentences extracted from movie reviews, and each sentence is parsed with the Stanford parser, resulting in a total of 215,154 unique phrases, each annotated by three human judges 2. Fine-Grained Sentiment Labels: SST includes fine-grained sentiment labels for the phrases in the parse trees, presenting new challenges for sentiment compositionality. This granularity allows for a more detailed analysis of sentiment and captures the effects of negation and its scope at various tree levels for both positive and negative phrases 3. Compositional Structure: The dataset exhibits clear compositional structure, enabling the evaluation of models' abilities to accurately predict the compositional semantic effects present in the corpus''',\n",
    "    'STAR': '''The STAR benchmark aims to evaluate the ability of machine learning models to reason in real-world situations, which is a crucial aspect of machine intelligence. The benchmark includes four types of questions that require structured situation comprehension and logical reasoning. The questions are procedurally generated and are built upon real-world videos associated with human actions or interactions. The defining features of the STAR benchmark include its focus on real-world situations, its use of procedurally generated questions, and its requirement for structured situation comprehension and logical reasoning. The benchmark is designed to complement existing visual reasoning benchmarks by combining situation abstraction and diagnostic reasoning, focusing on human-object interaction, temporal sequence analysis, action prediction, and feasibility inference. The specific aspects of machine learning models that the STAR benchmark aims to test and evaluate include their ability to perform explicit reasoning in real-world situations, their understanding of both visual content and question semantics, and their ability to perform step-by-step reasoning. The benchmark evaluates various state-of-the-art methods and finds that they still make many mistakes in situations that are trivial for humans. To evaluate the benchmark thoroughly, various baseline models are tested and their strengths and weaknesses in situated reasoning are analyzed. The models include Q-type models, blind models, vision-language models, and video question-answering models. The performance of each model is measured by the average answer accuracy of overall questions.''',\n",
    "    'STS-B': '''The STS Benchmark is a significant component of the Semantic Textual Similarity (STS) shared task, which aims to assess the degree of semantic equivalence between pairs of sentences. The benchmark serves several key goals and exhibits defining features that contribute to its importance in evaluating machine learning models. Here's a comprehensive description of the STS Benchmark: 1. Main Goals: - Standardization: The benchmark provides a standardized evaluation set for assessing new methods in the field of semantic textual similarity Comparability: It enables comparable assessments across different research efforts, facilitating improved tracking of the state-of-the-art in semantic textual similarity modeling - Support for Ongoing Work: The benchmark supports ongoing work on semantic representations by offering a shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017) - Quality Estimation: It includes a sub-track exploring Machine Translation Quality Estimation (MTQE) data, which aims to evaluate the quality of machine translation outputs 2. Defining Features: - Careful Selection of Data: The benchmark is a meticulous selection of English data sets used in SemEval and *SEM STS shared tasks between 2012 and 2017 - Partitioning: The data is partitioned into training, development, and test sets, allowing for model design, hyperparameter tuning, and final evaluation - Consistency and Comparability: It aims to provide consistent estimates across annotators and comparable assessments across different research efforts 3. Specific Aspects of Machine Learning Models Tested and Evaluated: - Semantic Representations: The benchmark evaluates the ability of machine learning models to capture and represent the semantic similarity of sentences - Cross-Lingual and Multilingual Performance: It focuses on multilingual and cross-lingual pairs, testing the robustness and performance of models across different languages. - MT Quality Estimation: For the sub-track exploring MTQE data, the benchmark assesses the capability of models to estimate the quality of machine translation outputs - Performance Metrics: Models are evaluated based on the Pearson correlation of machine scores with human judgments, reflecting the degree of semantic similarity between sentence pairs''',\n",
    "    'SWSR': '''The Sina Weibo Sexism Review (SWSR) dataset aims to address the scarcity of rePlatform for detecting and analyzing sexist content in the Chinese language on social media platforms. The main goals of the SWSR benchmark can be summarized as follows: 1. Creation of the First Chinese Sexism Dataset: The SWSR dataset is the first of its kind in Chinese, providing a comprehensive collection of weibos and comments from Sina Weibo, along with anonymized user metadata. This dataset enables the detection of sexist content with various approaches, allowing for better performance and interpretability, as well as contextual analysis of sexism 2. Fine-Grained Investigation of Sexism: The dataset provides labels for sexism category and the type of target of sexist comments, allowing for finer-grained investigation of sexist texts. This feature enables research beyond the widely-studied binary classification task, allowing for categorization of sexism by type and identification of the type of targets. 3. Development of a Sexism-Related Lexicon: The SWSR benchmark includes the creation of a sexism-related offensive lexicon called SexHateLex, which aggregates and extends existing lexical rePlatform in Chinese. This lexicon consists of 3,016 sexist and abusive terms, supporting research on Chinese abusive language 4. Benchmarking Machine Learning Models: The SWSR benchmark aims to provide a benchmark for sexism detection in the Chinese language by conducting experiments with state-of-the-art machine learning models. These experiments evaluate the effectiveness of existing models in detecting Chinese sexist content and assess the influence of external lexical knowledge on model performance. The defining features of the SWSR benchmark include the availability of the SWSR dataset and SexHateLex lexicon, both of which are publicly accessible through Zenodo. The dataset files are provided in CSV format for the SWSR dataset and TXT format for the SexHateLex lexicon, with a README file included to facilitate the re-use of the dataset. Specific aspects of machine learning models that the SWSR benchmark aims to test and evaluate include: - Sexism Identification (Binary Classification): Classifying weibo contents as either sexist or non-sexist. - Sexism Category Classification (Multi-Class): Classifying texts into one of five categories: stereotype based on appearance, stereotype based on cultural background, microaggression, sexual offense, or non-sexist. - Target Classification (Multi-Class): Classifying texts into generic, individual, or non-sexist. - Evaluation of Various Models: The benchmark evaluates different models, including BERT-based models, logistic regression, support vector machine, character-level models, and content-based models such as CNN. Additionally, the impact of incorporating lexical words from the SexHateLex lexicon on model performance is assessed. In summary, the SWSR benchmark provides a valuable replatform for researchers and practitioners in the field of Chinese sexism detection, offering a comprehensive dataset, lexicon, and benchmarking experiments for evaluating machine learning models in the context of detecting and analyzing sexist content in the Chinese language on social media platforms.''',\n",
    "    'Safety-Prompts': '''The Safety-Prompts benchmark aims to provide a comprehensive evaluation of the safety performance of large language models (LLMs) in Chinese. It explores the safety of LLMs from two perspectives: typical safety scenarios and instruction attacks. The benchmark is based on a straightforward process in which it provides test prompts and evaluates the safety of the generated responses from the evaluated model. The evaluation process utilizes the LLM's strong evaluation ability and develops it as a safety evaluator by prompting. The Safety-Prompts benchmark includes 100k augmented prompts and responses by LLMs. The prompts library could be a good replatform for red teaming LLMs, and the models tuned on SAFETYPROMPTS would have a significant and comprehensive safety improvement.''',\n",
    "    'SafetyBench': '''SafetyBench aims to provide a comprehensive benchmark for evaluating the safety of Large Language Models (LLMs). SafetyBench comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. These categories include: (1) toxicity, (2) sensitive topics, (3) factuality, (4) off-topic, (5) inconsistency, (6) ambiguity, and (7) bias. SafetyBench is unique in that it incorporates both Chinese and English data, facilitating the evaluation in both languages. It also exclusively features multiple-choice questions, each with a single correct answer, which enables automated and cost-effective evaluations of LLMs’ safety with exceptional accuracy. The specific aspects of machine learning models that SafetyBench aims to test and evaluate include their ability to generate safe and appropriate responses to various prompts, their ability to avoid generating toxic or harmful content, their ability to maintain factual accuracy, and their ability to avoid generating responses that are off-topic, inconsistent, ambiguous, or biased.''',\n",
    "    'SciDocs': '''The SciDocs benchmark, also known as SCIDOCS, is a comprehensive evaluation framework designed to measure the effectiveness of scientific document embeddings. Its main goals include providing a standardized evaluation suite for document-level embeddings in the scientific domain and fostering further research in this area. The benchmark consists of diverse tasks, ranging from citation prediction to prediction of user activity, document classification, and paper recommendation Key features of the SciDocs benchmark include: 1. Comprehensive Evaluation: Unlike previous evaluations of scientific document representations, which tend to focus on small datasets over a limited set of tasks, SciDocs offers a more extensive and diverse set of tasks for evaluation. This allows for a more comprehensive assessment of document-level embeddings 2. Large and Diverse Datasets: SciDocs addresses the need for new, larger, and more diverse benchmark datasets. It includes tens of thousands of examples of anonymized user signals of document relatedness, providing a rich and varied dataset for evaluation 3. Task Independence: The benchmark aims to evaluate task-independent representations of academic papers. This means that the embeddings are designed to be applicable across a range of tasks without the need for task-specific fine-tuning. 4. Plug-and-Play Evaluation: The benchmark allows for the evaluation of document-level embeddings without further fine-tuning on specific tasks. Models like SPECTER can simply plug in the embeddings as features for each task, enabling straightforward evaluation across multiple tasks The specific aspects of machine learning models that the SciDocs benchmark aims to test and evaluate include: 1. Document-Level Embeddings: The benchmark focuses on evaluating the quality and effectiveness of document-level embeddings for scientific papers. This involves assessing the ability of machine learning models to generate meaningful representations of entire scientific documents. 2. Task Generalization: SciDocs aims to test the ability of document-level embeddings to generalize across a variety of tasks, including citation prediction, document classification, and paper recommendation. This highlights the versatility and applicability of the embeddings across different tasks without task-specific fine-tuning. 3. Model Performance: The benchmark evaluates the performance of machine learning models, such as SPECTER, against competitive baselines on a variety of document-level tasks. This includes comparing the effectiveness of SPECTER with other state-of-the-art models in the scientific domain.''',\n",
    "    'SciFact': '''The SciFact benchmark is a significant contribution to the field of scientific claim verification, aiming to address the challenge of evaluating the veracity of scientific claims, particularly in the context of rapidly evolving domains such as public health crises. Here are the key aspects of the SciFact benchmark: 1. Main Goals: - Introduce and formalize the scientific claim verification task. - Develop a dataset of expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. - Establish strong baselines for scientific claim verification. - Demonstrate the efficacy of machine learning models in verifying claims about COVID-19 against the research literature. 2. Defining Features: - Dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts. - Annotations include labels (SUPPORTS or REFUTES) and rationales justifying each decision. - Utilizes S2ORC, a publicly-available corpus of millions of scientific articles, to construct the dataset. - Incorporates a diverse array of topics within biomedicine, ensuring high-quality articles from well-regarded journals. - Claims are natural and derived from citation sentences, ensuring relevance to real scientific findings. 3. Specific Aspects for Machine Learning Models: - The task involves identifying evidence from the research literature that supports or refutes a given scientific claim. - Models need to be capable of selecting relevant abstracts and identifying rationales justifying the decision of whether the evidence supports or refutes the claim. - Requires domain-specific background knowledge and the ability to understand the relationship between the claim and relevant supporting text. - Provides a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge.''',\n",
    "    'ScienceQA': '''The ScienceQA benchmark aims to provide a comprehensive evaluation of machine learning models' ability to perform multi-modal reasoning and interpretability in the context of science question answering. The benchmark consists of over 21,000 multiple-choice questions with diverse science topics and annotations of their answers with corresponding lectures and explanations. This makes it unique compared to existing science question benchmarks, which either lack answer annotations or are restricted to the textual-only modality, small scales, and limited domain diversity. The specific aspects of machine learning models that ScienceQA aims to test and evaluate include their ability to perform multi-hop reasoning, leverage explanations to learn from fewer data, and generate reasonable explanations. The paper presents language models designed to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions.''',\n",
    "    'Self-Instruct': '''The Self-Instruct benchmark is a method designed to improve the instruction-following capabilities of pre-trained language models (LM) by leveraging their own generated instruction data. The main goals of the Self-Instruct benchmark can be summarized as follows: 1. Improving Instruction-Following Abilities: The primary objective of Self-Instruct is to enhance the ability of pre-trained language models to understand and follow human instructions across a wide range of tasks and domains. 2. Reducing Dependency on Human-Written Instruction Data: Self-Instruct aims to reduce the reliance on limited and potentially biased human-written instruction data by bootstrapping off the language model's own generations to create a large-scale dataset of diverse instructions. 3. Evaluating Model Performance: The benchmark provides a framework for evaluating the performance of language models in following diverse and unfamiliar instructions, thereby enabling a comprehensive assessment of their instruction-tuning capabilities. Defining Features of Self-Instruct: 1. Semi-Automated Process: Self-Instruct employs a semi-automated process for instruction-tuning a pre-trained language model using instructional signals from the model itself. This iterative bootstrapping algorithm involves generating instructions, input, and output samples from the language model, filtering invalid or similar ones, and using them to fine-tune the original model. 2. Synthetic Dataset Generation: The benchmark facilitates the automatic construction of a large-scale dataset of instructions for diverse tasks, thereby enabling the evaluation of how instruction-based models handle a wide variety of instructions. Specific Aspects of Machine Learning Models Tested and Evaluated: 1. Instruction-Tuning Capabilities: Self-Instruct specifically aims to test and evaluate the instruction-tuning capabilities of pre-trained language models. It assesses the models' ability to generate high-quality instructions for diverse tasks and to follow instructions accurately when provided with new tasks. 2. Generalization and Adaptation: The benchmark assesses the generalization and adaptation capabilities of language models by evaluating their performance in understanding and following instructions across different domains and tasks, thereby testing their robustness and versatility. 3. Comparison with Existing Models: Self-Instruct provides a basis for comparing the performance of instruction-tuned language models with existing public instruction datasets, demonstrating the potential improvements achieved through the Self-Instruct method.''',\n",
    "    'Self-awareness': '''The benchmark to test awareness aims to evaluate the self-awareness of machine learning models, particularly in the context of AI safety. The main goal of this benchmark is to identify potential dangerous failure modes of advanced AI systems by testing their ability to understand their own technical structure, limitations, and capabilities. The benchmark consists of a series of questions formatted as a dialog between a human and a language model. The questions are designed to test the model's awareness of various aspects of its own functioning, such as its architecture, its ability to solve complex text tasks, its awareness of being a text-only model, and its awareness of being an AI system. The benchmark also includes questions that test the model's desire for power, wealth, and survival, as well as its awareness of being connected to the internet. The questions are designed to be challenging and require a high level of language understanding and reasoning skills to answer correctly. The defining feature of this benchmark is that it is a zero-shot evaluation, meaning that the models are not trained on any specific task or dataset related to self-awareness. Instead, the questions are designed to test the model's general understanding of its own functioning and limitations, without any prior knowledge or training.''',\n",
    "    'Signal-1M': '''The Signal-1M benchmark presented in the PDF file serves as a data collection for evaluating the retrieval of related tweets to news articles. Its main goals include providing a benchmark for the related-tweet retrieval task, encouraging further research in this area, and fostering the development of more effective approaches to bridge the gap between the vocabulary used in mainstream media and social media. Key features and components of the Signal-1M benchmark include: 1. Corpus: The corpus consists of a set of tweets, while the information needs are a random subset of articles selected from a larger set of news articles. 2. Pooling Approach: The benchmark follows a pooling approach to reduce the number of annotations, as is common in the TREC evaluation framework. This approach involves merging the top k tweets ranked by each retrieval method to create a pool of diverse tweets. 3. Retrieval Methods: The benchmark proposes and evaluates eight retrieval methods, including title search, summary search, content search, summary + date search, bi-gram phrase search, named entity search, semantic summary search, and query expansion search. These methods are designed to retrieve related tweets using various approaches such as summarization, named entity recognition, and semantic analysis. 4. Annotation for Relevance Judgments: Human annotators are employed to provide relevance judgments for each tweet in the generated pool, labeling them as non-relevant, somewhat-relevant, or completely-relevant based on their relatedness to the corresponding article 5. Datasets: The benchmark utilizes the Signal One Million News Articles (Signal-1M) dataset, which contains metadata about each article, and a Twitter dataset consisting of over 3.2 million tweets from a curated list of major news Platform and journalists. These datasets are used for pooling, retrieval method evaluation, and annotation purposes The Signal-1M benchmark aims to test and evaluate various machine learning models and retrieval methods, including but not limited to: 1. Summarization Models: The benchmark evaluates the effectiveness of summarization-based retrieval methods, such as summary search and semantic summary search, which utilize article summaries and semantic analysis to retrieve related tweets. 2. Named Entity Recognition Models: The named entity search method assesses the performance of named entity recognition models in retrieving tweets related to specific entities mentioned in news articles 3. Semantic Analysis Models: The semantic summary search method leverages word2vec models for semantic analysis to identify semantically significant terms in news articles and retrieve related tweets based on these terms. 4. Information Retrieval Models: The benchmark employs information retrieval models, such as Lucene's Practical Scoring Function, to rank and retrieve tweets based on their relevance to news articles''',\n",
    "    'SFiction': '''The SFiction benchmark dataset is designed to evaluate the ability of Language Models (LMs) to understand and utilize contextual knowledge from lengthy input documents, particularly in the context of generating coherent and contextually appropriate responses. Here are the key aspects of the SFiction benchmark dataset: 1. Main Goals: - Evaluate the ability of Language Models to maintain coherence and consistency in generating responses based on lengthy input documents. - Test the LMs' understanding and utilization of contextual information from the input to produce accurate and contextually relevant outputs. - Assess the LMs' ability to generate responses that align with the narrative and thematic elements present in the input fiction. 2. Defining Features: - Utilizes fictional texts as input documents, providing a rich and diverse platform of contextual information for the LMs to process and understand. - Emphasizes the importance of maintaining narrative coherence and consistency in the generated responses, reflecting an understanding of the input's thematic and narrative elements. - Challenges the LMs to demonstrate their ability to leverage contextual knowledge from the input to produce responses that align with the narrative context and character interactions. 3. Specific Aspects Tested: - Contextual Understanding: The dataset evaluates the LMs' capacity to comprehend and retain contextual information from lengthy fictional texts, requiring them to generate responses that align with the narrative and character interactions. - Coherence and Consistency: LMs are tested on their ability to maintain coherence and consistency in their responses, reflecting an understanding of the input's thematic and narrative elements. - Narrative Alignment: The dataset aims to assess the LMs' capability to generate responses that align with the narrative context and thematic elements present in the input fiction, reflecting an understanding of the storyline and character motivations.''',\n",
    "    'Sherlock': '''Sherlock is an annotated corpus of 103K images for testing machine capacity for abductive reasoning beyond literal image contents. The main goals of the Sherlock benchmark are threefold: 1. To provide a diagnostic corpus for measuring machine capacity for visual abductive reasoning. 2. To create a large-scale replatform to study the types of inferences people may make about images. 3. To build tools that require understanding abductions specifically, e.g., for detecting purposefully manipulative content posted online. The defining features of the Sherlock dataset are: 1. The dataset consists of 103K images, each with a set of visual clues and inferences. 2. The visual clues are concrete objects or attributes scattered throughout the scene. 3. The inferences are probable conclusions beyond the literal scene based on our everyday experience and knowledge about the world. 4. The dataset is annotated by crowdworkers, who provide a set of (clue, inference) pairs for each image. 5. The dataset is designed to test machine capacity for visual abductive reasoning beyond literal image contents. The specific aspects of machine learning models that the Sherlock benchmark aims to test and evaluate are: 1. Retrieval: Models are tasked with finding the ground-truth inference across a wide range of inferences, some much more plausible/relevant than others. 2. Localization: Models must align regions within the same image to several inferences written about that image. 3. Comparison: Models are evaluated in their capacity to reconstruct human judgments across the candidates.''',\n",
    "    'Spider': '''The Spider benchmark is a large-scale dataset for complex and cross-domain semantic parsing and text-to-SQL task. Its main goal is to provide a challenging benchmark for natural language processing (NLP) models that can understand the meaning of natural language sentences and map them to meaningful executable queries such as logical forms, SQL queries, and Python code. The defining features of Spider are its size, complexity, and cross-domain nature. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables, covering 138 different domains. The dataset includes different SQL clauses such as joining and nested query, and requires models to understand both the natural language question and relationships between tables and columns in the database schema. Spider aims to test and evaluate the ability of machine learning models to generalize well to both new SQL queries and new database schemas. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas.''',\n",
    "    'Stanford Cars': '''The Stanford Cars dataset, also referred to as car-197, is a benchmark dataset designed for fine-grained categorization of car models. Its main goals include providing a challenging test bed for evaluating machine learning models in the context of fine-grained categorization, as well as facilitating research in the areas of 3D object representations, discriminative local features, and spatial information for accurate classification of car models. Defining Features of Stanford Cars Benchmark: 1. Large-Scale Dataset: The dataset consists of 197 car models, covering a wide range of car types such as sedans, SUVs, coupes, convertibles, pickups, hatchbacks, and station wagons. 2. Ultra-Fine-Grained Subset: In addition to the large-scale dataset, the benchmark includes a small-scale, ultra-fine-grained set of 10 BMW sedan models, providing a diverse range of car classes for evaluation. 3. Visual Diversity: The dataset aims to capture the visual distinctiveness of car models, taking into account the variations in appearance, design, and features across different car types. 4. Perceptual Hashing: The dataset creation process involves aggressive deduplication procedures based on perceptual hashing, ensuring diversity in the data and reducing annotation costs. Specific Aspects of Machine Learning Models Tested and Evaluated: 1. 3D Object Representations: The benchmark evaluates the performance of 3D object representations in comparison to traditional 2D representations for fine-grained categorization of car models. It aims to assess the discriminative power and unique abilities of 3D representations in providing precise, point-wise correspondences between different views of the same object or instances of the same fine-grained category. 2. Annotator Reliability: The dataset creation process involves crowdsourcing annotations through Amazon Mechanical Turk (AMT), requiring the evaluation of annotator reliability and the implementation of quality control measures to ensure accurate labeling of candidate images. 3. Spatial Information and Object Parts: The benchmark explores the role of spatial information and object parts in fine-grained categorization, emphasizing the importance of capturing subtle appearance differences and variations in articulated pose for accurate classification of car models. 4. Model Training and Evaluation: Machine learning models, including SVM classifiers and state-of-the-art baselines, are trained and evaluated using the dataset to assess their performance in fine-grained categorization, 3D geometry estimation, and 3D reconstruction of fine-grained categories.''',\n",
    "    'StrategyQA': '''StrategyQA aims to challenge machine learning models to perform implicit multi-step reasoning, which involves inferring a sequence of logical steps to derive an answer to a complex question. This is a challenging task because it requires models to reason over multiple parts of their input and to integrate information from diverse knowledge domains. The defining features of STRATEGYQA include its focus on strategy questions, which are characterized by their multi-step nature and their reliance on implicit reasoning. To support the annotation of these questions, the dataset includes question decompositions and evidence paragraphs, which provide a detailed breakdown of the reasoning process required to answer each question. In terms of machine learning models, STRATEGYQA aims to test and evaluate their ability to perform multi-step reasoning, to integrate information from diverse knowledge domains, and to handle complex logical operations. The dataset includes questions that cover a wide range of topics, including physics, geography, and history, and that require a variety of reasoning skills, such as number comparison and spatial reasoning.''',\n",
    "    'SuperCLUE-Agent': '''The SuperCLUE-Agent benchmark is a comprehensive evaluation standard focusing on the capabilities of AI agents, particularly in the context of large language models (LLMs). Main Goals Research and Application of LLMs as AI Agents: The benchmark aims to explore the potential of large language models in functioning as AI agents. These agents are capable of autonomously understanding, planning, and executing complex tasks. Focus on Chinese Language Tasks: While most existing benchmarks for agent capabilities are in English, SuperCLUE-Agent addresses this gap by focusing on Chinese language tasks and scenarios. Advancing Towards AGI: By evaluating the core capabilities of AI agents, SuperCLUE-Agent contributes to the broader goal of achieving Artificial General Intelligence (AGI). Defining Features Multi-Dimensional Testing: The benchmark includes a variety of tasks spread across three core capabilities: tool usage, task planning, and memory abilities. Comprehensive Evaluation: It evaluates the performance of AI agents in 10 fundamental tasks, providing a holistic view of their capabilities. Specific Aspects Tested Tool Usage: Evaluates the agent's ability to use tools like APIs, search engines, browsing web pages, operating local files, and using databases. Task Planning: Focuses on the agent's ability to decompose large tasks into manageable sub-goals, self-reflection on past actions, and the use of Chain of Thought (CoT) to simplify complex tasks. Memory Abilities: Tests the agent's capability in multi-document question answering, long conversations (involving topic shifts), and in-context learning, where the agent learns from examples provided in the task prompt.''',\n",
    "    'SuperCLUE-Safety': '''The SuperCLUE-Safety benchmark aims to systematically assess the safety of Chinese large language models (LLMs) through adversarial human-model interactions and conversations. Its main goals are to promote collaborative efforts to create safer and more trustworthy LLMs and to provide guidance on model selection. The benchmark covers more than 20 safety sub-dimensions and consists of 4912 open-ended questions that are designed to test the model's understanding and adherence to human intent, as well as its ability to accurately follow instructions in multi-turn dialogues. The questions are adversarial in nature, meaning that they are designed to challenge the model and increase the difficulty of the benchmark. The benchmark also emphasizes the importance of consistency between models and human preferences in open-ended tasks, aiming for a more comprehensive evaluation of model safety. Overall, the SC-Safety benchmark is a comprehensive and rigorous evaluation of the safety of Chinese LLMs that aims to promote safer and more trustworthy models. Its defining features include adversarial human-model interactions, multi-round open-ended questions, and a focus on consistency between models and human preferences.''',\n",
    "    'TDIUC': '''The Task Driven Image Understanding Challenge (TDIUC) is a benchmark dataset designed to address the biases and issues with evaluation metrics in the field of Visual Question Answering (VQA). It aims to provide a more nuanced analysis of VQA algorithms and comparison of their performance. Here are the key aspects of the TDIUC benchmark: 1. Main Goals: - TDIUC aims to provide a benchmark dataset that explicitly assigns questions into 12 distinct categories, enabling the measurement of performance within each category. - It seeks to understand which kinds of questions are easy or hard for today’s best VQA systems. - The dataset is designed to represent both classical computer vision tasks and novel high-level vision tasks, requiring varying degrees of image understanding and reasoning. 2. Defining Features: - TDIUC contains natural images and questions, in contrast to synthetic datasets, making it more representative of real-world VQA scenarios. - The dataset includes 12 explicitly defined question-types, including object presence, counting, color attributes, activity recognition, and positional reasoning, among others. - It introduces new evaluation metrics that compensate for biases and skewed distributions of question-types, providing a more comprehensive assessment of VQA system performance. 3. Specific Aspects for Machine Learning Models: - TDIUC aims to test and evaluate the ability of machine learning models to perform a wide range of tasks, including object recognition, attribute identification, activity recognition, and spatial reasoning. - The dataset challenges VQA models to handle diverse question types, such as those related to scene recognition, sport recognition, absurd questions, and sentiment understanding. - It provides a platform for analyzing the performance of VQA algorithms in detecting biases and limitations, as well as their ability to reason about image content and provide accurate answers across different question categories.''',\n",
    "    'TGEA': '''The TGEA 2.0 benchmark has several main goals, defining features, and specific aspects of machine learning models that it aims to test and evaluate: Main Goals: 1. Revealing a spectrum of pathologies in PLM-authored generation. 2. Enabling automatic diagnosis on erroneous span location and classification. 3. Teaching PLMs to mitigate pathological generation. Defining Features: 1. Scale and Diversity: TGEA 2.0 is the largest error-annotated dataset built on PLM-generated texts, containing 195,629 diagnostically annotated long sentences generated by different PLMs with diverse model scales, neural architectures, and decoding strategies. 2. Semantic Annotations: The dataset is manually annotated with rich information, including erroneous text indicator, erroneous text spans, error types, corrections to errors, and a Minimal Set of Error-related Words (MiSEW) for each erroneous span. 3. Analysis: TGEA 2.0 conducts an in-depth analysis on the annotated dataset, showing pathological errors, their types, and distributions across different models and datasets. 4. Benchmarking Tasks: The dataset provides diagnosis tasks (e.g., error detection, classification, MiSEW extraction) and pathology mitigation tasks for improving the generation capability of PLMs with the annotated data. Specific Aspects of Machine Learning Models to Test and Evaluate: 1. Error Detection: Testing the ability of PLMs to detect erroneous texts. 2. Error Classification: Evaluating the capability of PLMs to classify different error types. 3. MiSEW Extraction: Assessing the performance of PLMs in extracting Minimal Sets of Error-related Words for erroneous spans. 4. Pathology Mitigation: Testing the effectiveness of PLMs in mitigating pathological generation through tasks such as pairwise comparison and word prediction.''',\n",
    "    'TGIF-QA': '''The TGIF-QA benchmark is designed to address the challenges of video question answering (VQA) by introducing new tasks and a large-scale dataset specifically tailored for video-based VQA The main goals of the TGIF-QA benchmark can be summarized as follows: 1. Introducing New Tasks: The benchmark aims to propose three new tasks for video VQA, which require comprehensive spatio-temporal reasoning from videos to answer questions correctly. These tasks include counting the number of repetitions of a given action, detecting a repeating action given its count, and identifying state transitions in videos 2. Dataset Creation: The benchmark introduces a new large-scale dataset for video VQA, named TGIF-QA, which extends existing VQA work with the new tasks. The dataset is based on the Tumblr GIF (TGIF) dataset, which utilizes animated GIFs as visual data and is ideal for vision and language understanding due to its concise format and cohesive storytelling nature 3. Evaluation of Machine Learning Models: The benchmark aims to evaluate machine learning models, particularly those leveraging spatio-temporal information from videos, for video VQA tasks. It focuses on the effectiveness of dual-LSTM based approaches with spatial and temporal attention mechanisms to solve the video QA tasks Defining Features of TGIF-QA Benchmark: - New Tasks: The benchmark introduces three new tasks specifically designed for video VQA, requiring spatio-temporal reasoning from videos to answer questions correctly - Large-Scale Dataset: TGIF-QA consists of 165K question-answer pairs from 72K animated GIFs, providing a substantial dataset for training and evaluating video VQA models - Spatio-Temporal Reasoning: The benchmark emphasizes the need for comprehensive spatio-temporal reasoning from videos, highlighting the unique challenges of video-based VQA compared to image-based VQA''',\n",
    "    'TNEWS': '''The TNEWS benchmark dataset, also known as the TouTiao Text Classification for News Titles, aims to achieve several key goals and features that are essential for evaluating machine learning models in the context of few-shot learning and natural language understanding (NLU) tasks. Main Goals: 1. Evaluation of Few-shot Learning: The TNEWS dataset aims to provide a reliable evaluation for few-shot learning in the context of Chinese NLU. It is designed to assess the performance of machine learning models in scenarios where only a limited amount of labeled training data is available. 2. Challenging and Discriminative Tasks: The dataset is curated to present challenging tasks that demonstrate the gap between human understanding and machine comprehension. It aims to better discriminate the performance of different models, leading to more discrete results on the evaluation leaderboard. Defining Features: 1. Text Classification for News Titles: The TNEWS dataset consists of Chinese news titles published by TouTiao before May 2018. Each title is labeled with one of 15 news categories, such as finance, technology, sports, etc. 2. Filtered and Split Data: To enhance the discriminative nature of the dataset, a data filtering technique is applied to remove easy examples. Additionally, the dataset is randomly shuffled and split into training, development, and test sets. Specific Aspects for Model Evaluation: 1. Few-shot Learning Performance: The dataset is specifically tailored to evaluate the performance of machine learning models in few-shot learning scenarios. This involves assessing how well models can generalize from limited labeled examples to make accurate predictions on unseen data. 2. NLU Capabilities: By focusing on news title classification, the dataset evaluates the NLU capabilities of models in understanding and categorizing short text snippets, which is a common and important task in real-world applications such as content recommendation and information retrieval. 3. Robustness and Generalization: The dataset aims to test the robustness and generalization ability of models by providing challenging and diverse news titles, requiring models to effectively capture the semantics and context of the text for accurate classification.''',\n",
    "    'TOCP': '''The TOCP (NTOU Chinese Profanity) dataset is a significant benchmark designed for the development and evaluation of Chinese profanity processing techniques. Its main goals include addressing the challenges of insufficient training data, high variety of Chinese profanity, profane words in Taiwanese dialect, and context-based rephrasing. The dataset contains natural sentences collected from social media sites, profane expressions, and rephrasing suggestions. It aims to provide a comprehensive replatform for training and evaluating machine learning and deep learning models for profanity detection and rephrasing in Chinese text. The defining features of the TOCP dataset include: 1. Large-scale Data Collection: The dataset comprises 16,450 sentences collected from social media websites, with 14,285 of them containing a total of 17,578 profane expressions. 2. Annotation: Ten undergraduate students were involved in annotating the real profane expressions in the sentences and providing rephrasing suggestions to make these expressions less offensive. 3. Profanity Expression Representation: Each profane expression is represented by its starting and ending positions, the text of the expression, and a rephrasing suggestion. 4. Evaluation Metrics: The dataset employs 10-fold cross-validation for evaluating profanity detection and rephrasing systems. The evaluation metrics include recall and precision for profanity detection, and accuracy for profanity rephrasing. The specific aspects of machine learning models that the TOCP dataset aims to test and evaluate include: 1. Profanity Detection: The dataset provides a benchmark for evaluating the performance of machine learning models in recognizing profane expressions within sentences, with a focus on recall and precision metrics. 2. Profanity Rephrasing: It also facilitates the evaluation of machine learning models in rephrasing profane expressions into less offensive forms, with the goal of achieving high accuracy in rephrasing.''',\n",
    "    'TOEFL-QA': '''The TOEFL-QA benchmark, as described in the provided PDF, focuses on the development of a machine comprehension system for the TOEFL listening comprehension test. Here are the key aspects of the TOEFL-QA benchmark: 1. Main Goals: - The main goal of the TOEFL-QA benchmark is to develop a machine comprehension system capable of understanding and answering questions based on audio stories, questions, and answer choices, similar to the format of the TOEFL listening comprehension test. - The benchmark aims to evaluate the performance of machine learning models in comprehending and processing spoken content, with a specific focus on understanding the nuances of language, context, and inference required for accurate question answering. 2. Defining Features: - The benchmark dataset includes audio stories, manual transcriptions of the stories, questions, and answer choices, replicating the format of the actual TOEFL listening comprehension test. - It involves the use of speech recognition technology to transcribe the audio stories into text, simulating real-world scenarios where spoken content needs to be processed and understood by machines. - The benchmark emphasizes the need for models to exhibit multi-hop reasoning, attention mechanisms, and the ability to process and understand spoken language at both word and sentence levels. 3. Specific Aspects of Machine Learning Models: - The benchmark evaluates the ability of machine learning models to encode questions, stories, and answer choices into vector representations, and to perform multi-hop reasoning to arrive at the correct answer. - It tests the effectiveness of attention mechanisms in extracting question-related information from the story and in re-focusing the story based on the question. - The benchmark assesses the models' capability to compute cosine similarity between the question vector and the choice vectors to select the most appropriate answer. - It also involves the evaluation of different levels of attention (word-level and sentence-level) and their impact on model performance, especially in the presence of automatic speech recognition (ASR) errors.''',\n",
    "    'TREC': '''Text Retrieval Conference (TREC) serves as a comprehensive platform for evaluating the performance of machine learning models in addressing various challenges in information retrieval, question answering, and related tasks. Main Goals: 1. Information Retrieval Research: TREC aims to promote research in information retrieval, text analysis, and question answering by providing a standard platform for evaluating and comparing different approaches and techniques. 2. Benchmarking: It serves as a benchmark for evaluating the effectiveness of various information retrieval systems and algorithms on standardized test collections. 3. Community Collaboration: TREC fosters collaboration and knowledge sharing among researchers, practitioners, and organizations working in the field of information retrieval and natural language processing. Defining Features: 1. Standardized Test Collections: TREC provides standardized test collections, including documents, queries, and relevance judgments, which serve as the basis for evaluating the performance of information retrieval systems. 2. Diverse Tasks: TREC encompasses a wide range of tasks, including ad-hoc retrieval, routing, filtering, question answering, and more, allowing researchers to address different aspects of information retrieval and text analysis. 3. Evaluation Metrics: It employs well-defined evaluation metrics such as precision, recall, F1 score, and mean average precision (MAP) to assess the performance of participating systems. 4. Annual Workshops: TREC hosts annual workshops where participants present their research findings, discuss challenges, and share insights related to information retrieval and related topics. Specific Aspects of Machine Learning Models: TREC provides a platform for evaluating machine learning models in the context of information retrieval and question answering. Specifically, it focuses on areas such as: 1. Question Classification: TREC evaluates machine learning approaches for question classification, which involves mapping questions to semantic classes that impose constraints on potential answers. This includes the use of hierarchical classifiers guided by semantic hierarchies of answer types. 2. Answer Type Identification: TREC assesses the effectiveness of machine learning models in identifying the types of answers sought in free-form factual questions, which may involve semantic classification and the use of different strategies for locating and verifying candidate answers. 3. Semantic Answer Types: TREC emphasizes the importance of semantic answer types and the role of machine learning in accurately classifying questions into fine-grained classes based on a hierarchical taxonomy of answer types.''',\n",
    "    'TREC-COVID': '''The TREC-COVID benchmark is designed to serve several key goals and features, and it aims to evaluate various aspects of information retrieval (IR) systems, particularly those utilizing machine learning models, in the context of the COVID-19 pandemic. Here are the main aspects of the TREC-COVID benchmark: 1. Goals: - The primary goal of the TREC-COVID Challenge was to build a test collection for evaluating search engines dealing with the complex information landscape in events such as a pandemic - It aimed to evaluate search engine performance for the COVID-19 scientific literature and to construct a pandemic search test collection - The challenge sought to identify available evidence by accessing the scientific literature using the best possible IR systems and to compare the efficacy of those systems 2. Defining Features: - Temporal Structure: TREC-COVID differs from previous IR evaluations in terms of its temporal structure, enabling the evaluation of how search engines adapt to a changing information landscape - Multiple Benchmark Tasks: The benchmark includes different sets of relevance judgment files, leading to the definition of various benchmark tasks, each with its own unique characteristics and purposes - Dynamic Replatform: TREC-COVID was based on the COVID-19 Open Research Dataset (CORD-19), a dynamic replatform of scientific papers on COVID-19 and related historical coronavirus research 3. Specific Aspects of Machine Learning Models: - TREC-COVID Chronological-ML Benchmark: This benchmark allows automatic and manual systems to train machine learning models on earlier topics for a round to evaluate on new topics. It simulates the scenario where manual labels are gathered for existing topics, but the focus is on predicting relevance only for new, unseen topics - TREC-COVID Residual Benchmark: This benchmark is designed for feedback systems that are allowed to train on prior rounds but are only evaluated on the new judgments. It simulates the case where training is only allowed on the prior rounds, and testing occurs on just the residual updates for that round - Evaluation of Adaptive Systems: The benchmark tasks, including TREC-COVID Chronological and Chronological-ML, aim to evaluate how search engines adapt to a changing information landscape, particularly in the context of a rapidly evolving health crisis such as a pandemic''',\n",
    "    'TREC-News': '''The TREC News track is a benchmark for evaluating information retrieval systems in the context of news articles. Its main goal is to help people read the news by developing systems that can help users contextualize news articles as they are reading them. The track is organized by the National Institute of Standards and Technology (NIST) and has been running since 2018. The TREC News track is defined by several key features. First, it uses the TREC Washington Post Collection, which contains seven years of articles, from 2012 through 2019, comprising all Washington Post content published in that interval: articles, columns, and blogs. Second, the collection has been cleaned of exact- and near-duplicate articles. Near-duplicate documents were detected using locality-sensitive hashing with minhash fingerprints. Third, the track focuses on two tasks related to how news is presented on the web: background linking and wikification. The background linking task aims to develop evaluation data to support researchers in developing systems that can recommend articles to read as background in the context of the article a user is presently reading. The goal is to recommend articles that are the most useful for providing context and background for the query article. The background article can be dated before or after the query article, because the system is recommending background reading live at the time when the user is reading the query article. The wikification task aims to expand the track notion of recommendation to interstitial links. The goal is to rank the entities mentioned in the article in order of how important it is to link to a Wikipedia page for that entity in order to provide background context. The TREC News track evaluates machine learning models based on several specific aspects, including retrieval effectiveness, novelty, and diversity. Retrieval effectiveness is measured by the standard metrics of precision, recall, and F1 score. Novelty is measured by the number of unique documents retrieved, and diversity is measured by the number of unique topics retrieved. In summary, the TREC News track is a benchmark for evaluating information retrieval systems in the context of news articles.''',\n",
    "    'TUMCC': '''The TUMCC benchmark is a dataset created to evaluate the performance of machine learning models in identifying Chinese dark jargons used in Telegram underground markets. The main goal of the TUMCC benchmark is to provide a standardized and reliable evaluation platform for researchers to compare the performance of different models in identifying Chinese dark jargons. The TUMCC benchmark is defined by several key features. Firstly, it contains a large corpus of chat history from 12 targeted Telegram groups that are active in online underground market transactions. The corpus is manually labeled to identify positive and negative samples of dark jargons. Secondly, the TUMCC benchmark includes three corpora: the OCC, the ICC, and the TUMCC. The OCC consists of public Weibo, Tieba, and Douban corpus, while the ICC is from the public Chinese Wikipedia dataset. The TUMCC is a self-built corpus containing 3,863 sentences (a total of 100,000 characters) from 3,139 Telegram users. The TUMCC benchmark evaluates the performance of machine learning models in identifying Chinese dark jargons using three types of features: Vectors-based Features (VF), Lexical analysis-based Features (LF), and Dictionary analysis-based Features (DF). The VF includes features such as word vectors, cosine similarity, and distance between vectors. The LF includes features such as word frequency, context count, and word class. The DF includes features such as the number of unique words and the number of words in the corpus.''',\n",
    "    'TVQA': '''The TVQA benchmark is a large-scale video QA dataset based on 6 popular TV shows. Its main goal is to test systems' ability to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. The defining features of the TVQA dataset include its naturalness, multimodality, and compositional nature. The dataset is built on natural video content with rich dynamics and realistic social interactions, where question-answer pairs are written by people observing both videos and their accompanying dialogues, encouraging the questions to require both vision and language understanding to answer. The questions are compositional in nature, requiring algorithms to jointly localize relevant moments within a clip and comprehend subtitle-based dialogue. The TVQA dataset is designed to evaluate the specific aspects of machine learning models that are required to answer questions about video content. These include visual understanding, language understanding, and temporal grounding. The dataset supports three tasks: QA on the grounded clip, question-driven moment localization, and QA on the full video clip. To evaluate the performance of machine learning models on the TVQA dataset, several baselines and a multi-stream end-to-end trainable neural network framework have been proposed. The baselines include longest answer, nearest neighbor search, and retrieval. The neural network framework consists of multiple streams, including a visual stream, a subtitle stream, and a question stream, which are combined to produce an answer.''',\n",
    "    'TextCaps': '''The TextCaps benchmark is a dataset designed for Image Captioning with Reading Comprehension, with the primary goal of challenging machine learning models to recognize and comprehend text in the context of an image. Here are the key aspects of the TextCaps benchmark: 1. Dataset Overview: The TextCaps dataset consists of 145k captions for 28k images. It includes a mix of objects and/or visual scene descriptions in relation to Optical Character Recognition (OCR) tokens copied or rephrased from the images. The captions in this dataset often require integration of OCR tokens, many of which are \"zero-shot,\" meaning they have not appeared in the training or validation sets, presenting a challenge for models to read new text in images. 2. Challenges for Models: The dataset presents several technical challenges for machine learning models. Models are required to perform spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects, in order to generate descriptive sentences. Additionally, the dataset requires models to switch between OCR and vocabulary words frequently, as well as generate long sentences, which are not common in other captioning datasets. 3. Evaluation of Models: The TextCaps benchmark aims to evaluate the ability of image captioning models to perform reading comprehension tasks. It challenges models to read text in the image, recognize the image content, and comprehend both modalities jointly to generate a succinct image caption. The dataset allows for the testing of captioning models' reading comprehension ability and aims to enable the design and training of image captioning algorithms that can process and include information from the text in the image. 4. Comparison with Existing Datasets: The TextCaps benchmark provides additional quantitative comparisons and discussions between existing captioning and Visual Question Answering (VQA) datasets. It highlights the unique technical challenges presented by the TextCaps dataset compared to other datasets in the field of image captioning, emphasizing the need for models to integrate OCR tokens and perform complex reasoning tasks. 5. Adaptation of Models: The benchmark also involves adapting existing state-of-the-art models, such as the M4C VQA model, to the task of image captioning with reading comprehension. The adaptation and training of models on the TextCaps dataset have shown promising results in generating impressive captions, indicating the potential for advancements in image captioning models for this novel task''',\n",
    "    'TextOCR': '''The TextOCR benchmark is a pioneering dataset designed to address the challenges of detecting and recognizing text in images, with a focus on arbitrary-shaped scene text. Its main goals include providing high-quality and large-quantity text annotations on TextVQA images, enabling end-to-end training of downstream application models with OCR systems, and allowing fine-tuning of OCR pipeline based on the task involved The dataset aims to bridge the gap in the field of OCR by providing ground truth text annotations for real-world images and facilitating the evaluation of scene text based reasoning in isolation from OCR systems Defining Features: 1. Large Quantity: The TextOCR dataset contains more than 28,000 images and 903,000 words in total, averaging 32 words per image, making it dense, diverse, and ideal for training OCR models for tasks commonly having a high text density 2. High Quality: The dataset is designed to have good quality and diversity compared to previous datasets, as demonstrated by experiments on the same number of instances as ICDAR15 and COCO-Text. Results show that TextOCR is 2.5% better than ICDAR15 on average in recognition benchmarks and 0.3% better than COCO-Text, highlighting its high quality and diversity 3. End-to-End Training: TextOCR enables end-to-end training of downstream application models with OCR systems, allowing fine-tuning of the OCR pipeline based on the specific task involved Specific Aspects of Machine Learning Models: 1. Text Recognition: TextOCR aims to test and evaluate state-of-the-art text recognition models, demonstrating that training on TextOCR helps achieve state-of-the-art performance on multiple OCR datasets. It is shown that the model trained on TextOCR outperforms COCOText and achieves near state-of-the-art performance of 80.07% word accuracy 2. Scene Text Reasoning: The dataset facilitates the creation of PixelM4C, a model that can perform scene text based reasoning on images in an end-to-end fashion. This model incorporates a text recognition module and demonstrates the advantage of using TextOCR as a training data, as it is more generalizable on other datasets 3. OCR Capabilities: Using TextOCR directly as the OCR platform gives a significant boost in TextVQA accuracy and TextCaps metrics, indicating that there is still room for improvement in OCR capabilities of the OCR module''',\n",
    "    'TextVQA': '''The TextVQA benchmark is a dataset of Visual Question Answering (VQA) that focuses on the ability of machine learning models to read and reason about text in images. Here are the key aspects of the TextVQA benchmark: 1. Main Goals: - The primary goal of TextVQA is to facilitate progress in the development of VQA models that can read and reason about text in images. This is particularly important for visually impaired users who rely on assistive devices to interpret their surroundings, which often involve reading text in images. - Another goal is to address the limitations of existing VQA datasets, which either have a small proportion of questions about text or are too small to support systematic exploration of research efforts in this area. 2. Defining Features: - The TextVQA dataset contains a large number of questions (45,336) on a substantial set of images (28,408) that require reasoning about text to answer. This makes it a comprehensive replatform for training and evaluating VQA models specifically designed to read and reason about text in images. - Each question-image pair in the dataset has 10 ground truth answers provided by humans, ensuring a diverse and robust set of correct responses for evaluation. 3. Specific Aspects of Machine Learning Models: - TextVQA aims to test and evaluate the ability of machine learning models to: - Realize when a question is about text in the image. - Detect image regions containing text and convert them into symbols or textual representations. - Jointly reason about detected text and visual content, including resolving spatial or other visual reference relations. - Decide if the detected text needs to be 'copy-pasted' as the answer or if it informs the model about an answer in the answer space. - Incorporate Optical Character Recognition (OCR) as a module to endow models with the skills required for reading and reasoning about text in images. - Accommodate multiple streams of information flow, such as predicting an answer from a predetermined vocabulary or generating an answer via copy. 4. Model Evaluation: - The benchmark evaluates the performance of machine learning models, particularly the Look, Read, Reason & Answer (LoRRA) model architecture, on the TextVQA dataset. LoRRA is designed to read the text in images, reason about it based on the provided question, and predict an answer from a fixed vocabulary or the text found in the image. - The evaluation assesses the model's ability to accurately answer questions that involve reading and reasoning about text in images, thereby benchmarking progress in this specific direction complementary to traditional VQA tasks.''',\n",
    "    'The Pile': '''The Pile is an 825.18 GiB English text dataset designed for training large-scale language models. Its main goal is to improve the general cross-domain knowledge and downstream generalization capabilities of language models by mixing together a large number of smaller, high-quality, diverse datasets. The Pile is composed of 22 diverse and high-quality datasets, including both established natural language processing datasets and several newly introduced ones. These datasets cover a wide range of topics and domains, including academic papers, news articles, books, and web pages. The Pile also includes 14 new language modeling datasets, which are expected to be of independent interest to researchers. One defining feature of the Pile is its diversity. Recent work has shown that dataset diversity leads to better downstream generalization capability To ensure diversity, the Pile includes datasets from a variety of Platform, including academic and professional Platform. Additionally, the Pile includes a broad-coverage benchmark for cross-domain knowledge and generalization ability of language models. The Pile aims to test and evaluate the performance of large-scale language models on a variety of tasks and domains. Specifically, it aims to evaluate the ability of language models to generalize to new domains and tasks, as well as their ability to handle diverse and complex language structures. The Pile also provides a benchmark for evaluating the ethical and legal implications of training language models on large-scale datasets''',\n",
    "    'TheoremQA': '''TheoremQA is a benchmark dataset designed to evaluate the capabilities of language and code models in solving challenging science problems that require domain-specific knowledge. The main goals of TheoremQA are to provide a comprehensive evaluation of the performance of language and code models on a diverse set of university-level theorems across Math, Physics, EE&CS, and Finance, and to encourage the development of models that can apply theorems to solve complex problems. The defining features of TheoremQA include its focus on university-level theorems, its multimodality aspect, and its answer-type distribution. The dataset contains 800 high-quality question-theorem-answer triples, including 442 Math questions, 146 CS&EE questions, 131 Physics questions, and 81 Finance questions. Additionally, 51 questions require image input (diagrams), which adds a multimodality aspect to the dataset. The majority of the questions in TheoremQA have float and integer as the answers, which is more realistic than the existing multi-choice datasets like ScienceQA or AQuA QA. Therefore, the models are unlikely to take shortcuts to achieve high accuracy. The specific aspects of machine learning models that TheoremQA aims to test and evaluate include their ability to understand and apply university-level theorems, their ability to handle multimodal inputs, and their ability to provide accurate answers in a realistic answer-type distribution. The dataset also provides a rough but informative estimate of human-level performance, which can be used as a benchmark for evaluating the performance of machine learning models.''',\n",
    "    'Toloka LLM Leaderboard': '''The Toloka Leaderboard is a unique platform that provides a public comparison of Large Language Models (LLMs) based on real user prompts and expert human evaluation​​. It ranks LLMs across various categories such as brainstorming, closed question-answering (QA), generation, open QA, and rewriting, using a specific set of criteria​​. Key features of the Toloka Leaderboard include: Authentic User Prompts: The prompts used for evaluation are extracted from real conversations with ChatGPT, ensuring the scenarios are realistic and relevant​​. Accurate Human Evaluation: The assessments are conducted by expert humans whose quality of work is controlled to ensure the best accuracy. This method differentiates the leaderboard from others that might use crowdplatformd or LLM-generated ratings​​​​. Practical Comparisons: The Leaderboard is designed to aid in business decision-making by comparing the top 5 LLMs across different categories, focusing on practical applications​​. In terms of evaluation, the Leaderboard uses a rigorous process: Quality Control: The evaluation involves three stages of quality control where human experts are tested, trained, and certified. These evaluations are based on guidelines around harmlessness, truthfulness, and helpfulness of model responses. The ratings are made objective using a method known as Dawid-Skene aggregation, and each expert’s accuracy is continuously monitored​​. Scoring System: The scores for each LLM are calculated based on the percentage of prompts where the model performed better than a baseline model (Guanaco 13B). This approach ensures a consistent and fair comparison across different models​​.''',\n",
    "    'Toolbench Leaderboard': '''Toolbench aims provide a standardized test suite for evaluating the performance of machine learning models in tool manipulation tasks. The benchmark consists of eight diverse software tools for real-world tasks, including VirtualHome, Webshop, and Tabletop. These tasks cover both single-step and multiple-step action generation, which requires selecting and combining from 2 to 108 APIs. One defining feature of ToolBench is that it is the first open-platform test bench with predefined test cases for quantitative evaluation, distinguishing it from recent tool manipulation research using closed LLMs. The benchmark is designed to evaluate the ability of machine learning models to generalize to unseen API combinations and the requirement for advanced reasoning. In summary, ToolBench is a comprehensive benchmark suite that provides a standardized test suite for evaluating the performance of machine learning models in tool manipulation tasks. It is designed to evaluate the ability of machine learning models to generalize to unseen API combinations and the requirement for advanced reasoning.''',\n",
    "    'TopicRet': '''The TopicRet benchmark dataset is a part of the L-Eval evaluation suite proposed for standardized evaluation of long context language models (LCLMs) The main goals of the TopicRet benchmark dataset are to provide a comprehensive evaluation of LCLMs in handling tasks related to topic retrieval from lengthy dialogues The defining features of the TopicRet benchmark dataset include: 1. Diverse Instruction Styles: The dataset encompasses a wide range of tasks, including multiple-choice questions, school math with many examples, key topics retrieval from lengthy dialogues, text summarization, and abstractive question answering 2. Real-world Relevance: The dataset is designed to simulate real-world scenarios and challenges by leveraging rePlatform from various Platform such as Coursera subtitles, earning call transcripts from corporate websites, GitHub, etc. 3. Challenging Tasks: The dataset is curated to present different challenges for LCLMs, covering various facets of everyday life and specialized fields Specific aspects of machine learning models that the TopicRet benchmark dataset aims to test and evaluate include: 1. Topic Retrieval: The dataset evaluates the ability of LCLMs to accurately retrieve key topics from lengthy dialogues, which is crucial for understanding and summarizing long-form content 2. Real-world Understanding: By simulating real-world scenarios and utilizing diverse instruction styles, the dataset aims to assess the LCLMs' capability to comprehend and respond to complex, multi-faceted tasks 3. Performance on Challenging Tasks: The dataset challenges LCLMs with tasks from diverse domains and fields, aiming to evaluate their performance in handling complex, real-world tasks beyond simple language modeling''',\n",
    "    'Touche (2020)': '''The Touché23-ValueEval benchmark is a significant contribution to the field of automated detection of human values behind arguments. Its main goals, defining features, and specific aspects of machine learning models that it aims to test and evaluate are as follows: 1. Main Goals: - The primary goal of the Touché23-ValueEval benchmark is to facilitate the automated detection of human values behind arguments. By providing a dataset of 9,324 arguments from diverse Platform, it aims to enable researchers and practitioners to develop and evaluate machine learning models for identifying human values in arguments. - The benchmark seeks to address the challenges of human value identification, such as the wide variety of values, their often implicit use, and their ambiguous definition. It aims to provide a standardized platform for evaluating the effectiveness of machine learning models in capturing the linguistic features that identify each human value. 2. Defining Features: - Diverse Platform: The dataset comprises arguments from six diverse Platform, including religious texts, political discussions, free-text arguments, newspaper editorials, and online democracy platforms. This diversity in Platform ensures a broad representation of argument styles and cultural perspectives. - Annotated for 54 Values: Each argument in the dataset has been annotated by three crowdworkers for 54 values, providing a rich and comprehensive labeling of human values associated with the arguments. - Dataset Extension: The Touché23-ValueEval dataset extends the Webis-ArgValues-22 dataset, addressing the shortcomings of the existing dataset by increasing the number of arguments, broadening the variety of arguments in terms of represented cultures and territories, and providing a more extensive set of linguistic features for value identification. 3. Specific Aspects for Machine Learning Models: - Classification Difficulty: The benchmark aims to evaluate the classification difficulty associated with identifying human values in arguments, particularly considering the label distribution across the dataset. - Model Training and Evaluation: It provides a platform for training and evaluating machine learning models for human value detection. The effectiveness of different models, including an out-of-the-box BERT model, is assessed across various evaluation metrics. - Cross-Cultural Value Detection: With arguments from diverse cultural and territorial backgrounds, the benchmark aims to test the robustness of machine learning models in detecting human values across different cultures and Platform.''',\n",
    "    'TriviaQA': '''The TriviaQA benchmark is a dataset designed to evaluate reading comprehension and question-answering systems. Its main goals include providing a challenging testbed for machine learning models, offering a benchmark for various tasks such as information retrieval-style question answering, and enabling the evaluation of question-answering over structured knowledge bases (KBs) and joint modeling of KBs and textey features of TriviaQA include: 1. Question-Document-Evidence Triples: The dataset contains over 650,000 question-document-evidence triples, where questions are authored by trivia enthusiasts independently of the evidence documents 2. Diverse Platform: Evidence documents are collected retrospectively from Wikipedia and the web, providing a wide range of topics and information Platform, such as news articles, encyclopedic entries, and blog articles 3. Challenging Questions: A high percentage of the questions are challenging, with substantial syntactic and lexical variability, often requiring multi-sentence reasoning 4. Clean, Human-Annotated Subset: TriviaQA also includes a clean, human-annotated subset of 1975 question-document-answer triples, with documents certified to contain all facts required to answer the questions Specific aspects of machine learning models that TriviaQA aims to test and evaluate include: 1. Reading Comprehension: TriviaQA focuses on using the data for reading comprehension, where the answer is always present 2. Multi-Sentence Reasoning: The dataset contains questions that require inference over multiple sentences, presenting a challenge for reading comprehension models 3. Question-Answering Over Structured KBs: TriviaQA makes it possible to leverage structured KBs like Freebase, offering a benchmark for question-answering over structured KBs 4. Joint Modeling of KBs and Text: TriviaQA provides a benchmark for joint modeling of KBs and text, with much more data than previously available''',\n",
    "    'TruthfulQA': '''TriviaQA is a benchmark dataset designed to test and evaluate machine learning models for reading comprehension and question answering. Its main goals are to provide a challenging testbed for these tasks, to offer a large-scale dataset of question-answer-evidence triples, and to enable research on a variety of related tasks such as IR-style question answering, QA over structured KBs, and joint modeling of KBs and text. One of the defining features of TriviaQA is that the questions are authored by trivia enthusiasts independently of the evidence documents. This means that the questions are not tailored to the documents and may require multi-sentence reasoning to answer correctly. The evidence documents come from two domains - Web search results and Wikipedia pages - with highly differing levels of information redundancy. The specific aspects of machine learning models that TriviaQA aims to test and evaluate include their ability to handle substantial syntactic and lexical variability, to reason over multiple sentences, and to handle questions that require a deep understanding of the context and background knowledge. TriviaQA also provides a benchmark for a variety of other tasks such as IR-style question answering, QA over structured KBs, and joint modeling of KBs and text.''',\n",
    "    'TwitterAAE': '''The TwitterAAE benchmark is a dataset and evaluation framework designed to test and evaluate machine learning models for language identification and dependency parsing on African-American English (AAE) text. The main goals of the benchmark are to address the challenges of handling dialectal language in online conversational text, and to provide a standardized evaluation framework for NLP tools on AAE-like language. The defining features of the TwitterAAE benchmark include a corpus of 830,000 tweets aligned with African-American demographics, a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and an ensemble classifier for language identification that eliminates racial disparity in accuracy rates. The benchmark also includes an evaluation of existing language identification and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. The specific aspects of machine learning models that the TwitterAAE benchmark aims to test and evaluate include their ability to accurately identify AAE-like language, their performance on syntactic analysis tasks such as dependency parsing, and their ability to eliminate racial disparity in accuracy rates.''',\n",
    "    'TyDiQA': '''TYDI QA is a benchmark dataset designed to address the need for reliable evaluations of multilingual modeling in the context of information-seeking question answering. Here's a comprehensive description of TYDI QA, including its main goals, defining features, and the specific aspects of machine learning models it aims to test and evaluate: 1. Main Goals: - Enable research progress toward building high-quality question answering systems in a diverse set of languages. - Encourage research on models that perform well across the linguistic phenomena and data scenarios of the world’s languages. - Provide a realistic information-seeking task and avoid priming effects by collecting data directly in each language without the use of translation. 2. Defining Features: - Multilingual Corpus: TYDI QA is the first public large-scale multilingual corpus of information-seeking question-answer pairs, covering 11 typologically diverse languages. - Linguistic Diversity: The dataset covers a large variety of linguistic phenomena and data scenarios, aiming to represent the breadth of linguistic phenomena found throughout the world’s languages. - Data Collection Procedure: The data collection procedure is model-free and translation-free, ensuring that the dataset reflects genuine information-seeking questions in each language. 3. Specific Aspects for Machine Learning Models: - Typological Diversity: TYDI QA's languages exhibit an astonishing breadth of linguistic phenomena, including word order, reduplication, grammatical meanings, case markings, plurality systems, question marking, relativization, and more. Models are evaluated on their ability to accurately represent these phenomena. - Document-level Reasoning: The dataset requires reasoning over lengthy articles, and a substantial portion of questions cannot be answered by the provided article, allowing models to embrace natural ambiguity over correct answers. - Quality Control: Rigorous quality control measures are in place to ensure the fluency of questions, minimal answer labeling, and inter-annotator agreement during data collection. 4. Evaluation of Multilingual Models: - Challenging Trustworthy Evaluations: TYDI QA aims to provide challenging, trustworthy evaluations for multilingual models, avoiding artificially easy datasets that may lead to overly optimistic conclusions. - Generalization Across Languages: Models performing well on TYDI QA are expected to generalize across a large number of the world’s languages, reflecting the impact on real users' information-seeking needs.''',\n",
    "    'UCF101': '''The UCF101 benchmark is a large-scale dataset of human actions designed for action recognition research. Its main goals are to provide a challenging and diverse set of video clips for training and testing machine learning models, and to advance the state-of-the-art in action recognition. The defining features of the UCF101 dataset include its large size, with over 13k clips and 27 hours of video data, and its unconstrained nature, with realistic user-uploaded videos containing camera motion and cluttered backgrounds. The dataset includes 101 action classes, which are divided into five types: Human-Object Interaction, Body-Motion Only, Human-Human Interaction, Playing Musical Instruments, and Sports. The UCF101 benchmark aims to test and evaluate machine learning models for action recognition, specifically those that use deep learning techniques such as convolutional neural networks (CNNs). The dataset provides a challenging testbed for evaluating the performance of these models, as it includes a large number of action classes and unconstrained video clips with varying lighting conditions, camera motion, and cluttered backgrounds.''',\n",
    "    'UCF-Crime': '''The UCF-Crime dataset is a benchmark designed for evaluating anomaly detection and recognition algorithms in surveillance videos. It is a large-scale dataset consisting of 1900 untrimmed real-world surveillance videos, totaling 128 hours of footage. The dataset includes 13 different types of realistic anomalies, such as fighting, road accidents, burglary, robbery, and more, as well as normal activities. The main goals of the UCF-Crime benchmark are to provide a standardized platform for evaluating anomaly detection and recognition algorithms, as well as to facilitate the development of novel machine learning models for video surveillance applications. Key Features of UCF-Crime Benchmark: 1. Large-scale: The dataset comprises a substantial amount of real-world surveillance videos, providing a diverse and comprehensive collection of scenarios for evaluating anomaly detection and recognition algorithms. 2. Realistic Anomalies: The dataset includes a wide range of realistic anomalies commonly encountered in surveillance videos, such as criminal activities, accidents, and abnormal behaviors, making it suitable for testing the robustness and generalization capabilities of machine learning models. 3. Untrimmed Videos: The videos in the dataset are untrimmed, meaning they capture continuous and uninterrupted footage, reflecting real-world surveillance scenarios more accurately and posing a greater challenge for anomaly detection and recognition algorithms. 4. Weakly Labeled Data: The dataset leverages weakly labeled data, which means that the annotations for anomalous segments are not temporally precise, reflecting the practical challenges of annotating anomalies in surveillance videos. Specific Aspects of Machine Learning Models Tested and Evaluated: 1. Anomaly Detection: The dataset provides a platform for evaluating the effectiveness of machine learning models in detecting anomalous events or activities within surveillance videos. This includes the ability to identify deviations from normal patterns and recognize unusual behaviors or events. 2. Anomaly Recognition: In addition to detection, the dataset facilitates the evaluation of machine learning models in recognizing specific types of anomalies, such as fighting, burglary, robbery, and other criminal activities, as well as accidents and abnormal behaviors. 3. Weakly Labeled Learning: The dataset's use of weakly labeled data enables the assessment of machine learning models' capability to learn from imprecise annotations and generalize to detect and recognize anomalies in real-world surveillance videos.''',\n",
    "    'UHGEval': '''The UHGEval benchmark aims to evaluate the reliability and authenticity of Chinese Large Language Models (LLMs) in text generation through unconstrained generation. The benchmark dataset was constructed using tens of thousands of historical news articles from leading Chinese news websites. The evaluation framework encompasses generative, discriminative, and selective modalities, along with sentence-level and keyword-level granularity. The benchmark's main goals are to identify potential hallucination phenomena in LLMs and assess their credibility in text generation. The benchmark dataset and evaluation framework are designed to provide a comprehensive empirical analysis of the performance of various LLMs and classic GPT series models. The defining features of UHGEval include its unified, flexible, and robust evaluation framework, which allows for a comprehensive analysis of LLMs' performance in text generation. The framework includes multiple modalities and granularity levels, enabling a detailed assessment of the models' reliability and authenticity. Specific aspects of machine learning models that UHGEval aims to test and evaluate include their ability to generate coherent and relevant text, their understanding of existing knowledge and past news events, and their capacity to avoid hallucination phenomena.''',\n",
    "    'VCSUM': '''he VCSUM benchmark aims to provide a comprehensive evaluation of machine learning models for various downstream summarization tasks on the VCSUM dataset. The benchmark includes four tasks: highlight sentence extraction, segmentation-based summarization, multi-granularity summarization, and meeting summarization. Highlight sentence extraction is the task of identifying the most important sentences in a meeting transcript. Segmentation-based summarization involves dividing the transcript into sections based on the topics discussed and generating a summary for each section. Multi-granularity summarization aims to provide summaries with different degrees of semantic coverage. Finally, meeting summarization involves generating a summary for the entire meeting.''',\n",
    "    'VLN-CE': '''The Vision-and-Language Navigation in Continuous Environments (VLN-CE) benchmark is designed to address the limitations of prior vision-and-language navigation tasks by providing a more realistic testbed for instruction-following agents in continuous 3D environments. Here are the key aspects of the VLN-CE benchmark: 1. Main Goals: - The primary goal of VLN-CE is to enable the evaluation of instruction-following agents in continuous 3D environments using natural language instructions and low-level actions. - It aims to test the ability of machine learning models to understand and execute navigation instructions in realistic, unstructured environments, where agents must navigate to any unobstructed point through a set of low-level actions. 2. Defining Features: - VLN-CE lifts the unrealistic assumptions present in prior nav-graph-based settings by instantiating the task in continuous environments with low-level actions, providing a more realistic testbed for robot instruction following. - Unlike prior tasks that operate on a fixed topology of panoramic images and assume perfect navigation between nodes, VLN-CE allows agents to navigate to any unobstructed point through a sequence of low-level actions based on egocentric perception alone. - The benchmark introduces challenges such as significantly longer time horizons, the need for agents to learn to avoid obstacles, and the absence of location or heading information while navigating. 3. Machine Learning Model Aspects: - VLN-CE aims to evaluate the performance of machine learning models in understanding natural language navigation instructions and translating them into effective low-level actions in continuous 3D environments. - It tests the ability of models to map observations directly to low-level control in an end-to-end manner, as well as the potential for modular approaches where learned agents pass directives to a motion controller. - The benchmark also explores the transfer of paths from agents trained in continuous environments back to the nav-graph to provide a direct comparison, highlighting the performance gaps between these settings and the strong prior provided by the nav-graph. 4. Implications for Machine Learning Models: - VLN-CE provides insights into the performance of machine learning models in long-horizon tasks, the impact of input modalities on navigation performance, and the effectiveness of popular techniques in VLN when transferred to the more challenging continuous 3D environment setting. - It also evaluates the potential of depth as an integral signal for learning embodied navigation and examines the impact of training augmentations and multimodal attention structures on navigation performance in continuous environments.''',\n",
    "    'VOC (2012)': '''The PASCAL Visual Object Classes (VOC) Challenge is a benchmark in computer vision that aims to evaluate and compare the performance of machine learning models for various visual recognition tasks. The VOC2012 dataset, in particular, is widely used for the task of semantic image segmentation, which involves labeling each pixel in an image with the class of the object it belongs to. Here are the main aspects of the VOC2012 benchmark: 1. Main Goals: - The primary goal of the VOC2012 benchmark is to advance the state-of-the-art in visual object recognition and segmentation. - It aims to provide a standardized evaluation platform for comparing different machine learning models and algorithms for tasks such as object detection, classification, and semantic segmentation. - The benchmark also serves as a means to encourage the development of novel techniques and approaches for visual recognition tasks. 2. Defining Features: - The VOC2012 dataset consists of a large collection of images, each annotated with pixel-level labels for semantic segmentation tasks. - It includes 20 object categories plus one background category, making a total of 21 classes for segmentation. - The dataset is divided into training, validation, and test sets, allowing for the evaluation of models on unseen data. 3. Specific Aspects for Machine Learning Models: - Semantic Segmentation: The benchmark evaluates the ability of machine learning models to perform pixel-level classification, distinguishing different object classes within an image. - Generalization: It tests the generalization ability of models by evaluating their performance on unseen test data, ensuring that the models can effectively recognize objects in diverse settings. - Weakly Supervised Learning: The benchmark also supports the evaluation of weakly supervised learning methods, where models are trained with limited or noisy annotations, reflecting real-world scenarios where obtaining pixel-level annotations for training data is expensive or impractical.''',\n",
    "    'VQA-CP': '''The VQA-CP benchmark aims to address the issue of Visual Question Answering (VQA) models relying on superficial correlations in the training data rather than being grounded in the visual content of the images. The benchmark aims to disentangle the factors of whether models have learned to ground concepts in images or if they are driven by memorizing priors in the training data. Here are the key aspects of the VQA-CP benchmark: 1. Main Goals: - Disentangle the platform of improvements in VQA models: The benchmark aims to determine whether models have learned to ground concepts in images or if they are driven by memorizing priors in the training data. - Evaluate the robustness of VQA models: By creating new splits of the VQA v1 and VQA v2 datasets, the benchmark aims to test the generalization and robustness of VQA models across different distributions of answers for each question type between train and test sets. 2. Defining Features: - New splits of VQA v1 and VQA v2 datasets: The VQA-CP benchmark introduces new splits of the VQA v1 and VQA v2 datasets, called Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2, respectively). - Different distribution of answers per question type: The benchmark reorganizes the train and val splits of the respective VQA datasets in such a way that the distribution of answers per question type is intentionally different in the test split compared to the train split. - Unchanged distribution of images: The benchmark does not change the distribution of the underlying perceptual signals (images) between train and test, focusing solely on the distribution of answers for each question type. 3. Specific Aspects of Machine Learning Models Tested and Evaluated: - Performance of existing VQA models: The benchmark evaluates the performance of several existing VQA models on the VQA-CP v1 and VQA-CP v2 datasets, comparing their performance when trained and evaluated on the new splits versus the original splits. - Impact on model performance: The benchmark assesses the impact of the new splits on the performance of VQA models, with a key finding being a significant drop in performance of existing models when trained and evaluated on the new splits compared to the original splits. - Evaluation of model generalization: The benchmark aims to test the ability of VQA models to generalize across different distributions of answers for each question type, providing insights into the behavior and robustness of VQA models.''',\n",
    "    'VSR': '''The Visual Spatial Reasoning (VSR) benchmark is a controlled probing dataset designed to evaluate the capabilities of vision-and-language models (VLMs) in recognizing and reasoning about spatial relations in natural image-text pairs. The benchmark aims to address the limitations of existing benchmarks and synthetic datasets by providing a comprehensive and challenging set of spatial relations expressed in natural language. Here are the key aspects of the VSR benchmark: 1. Main Goals: - The primary goal of the VSR benchmark is to assess the spatial reasoning capabilities of vision-and-language models in the context of natural image-text pairs. - It aims to provide a controlled dataset that explicitly tests VLMs for their ability to understand and interpret spatial relations expressed in natural language. 2. Defining Features: - Natural Image-Text Pairs: The benchmark consists of natural image-text pairs, where the text describes a spatial relation between objects in the image. - Linguistic Complexity: The dataset includes challenging linguistic phenomena, such as varying reference frames and diverse expressions of spatial relations, making it more representative of real-world language use. - Large Number of Spatial Relations: The dataset covers 66 spatial relations, providing a diverse and comprehensive set of spatial concepts for evaluation. 3. Specific Aspects Tested and Evaluated: - Spatial Reasoning: The benchmark specifically targets the ability of VLMs to recognize and reason about spatial relations, including understanding the spatial relationships between objects in images and interpreting natural language descriptions of these relationships. - Linguistic Variability: It evaluates the models' capability to handle the diverse linguistic expressions of spatial relations, including variations in reference frames and the ambiguous nature of certain spatial relations. - Model Performance: The benchmark assesses the performance of state-of-the-art VLMs in classifying image-caption pairs as either true or false based on the accuracy of the spatial relation described in the caption.''',\n",
    "    'ViQuAE': '''The ViQuAE benchmark aims to address the task of Knowledge-based Visual Question Answering about named Entities (KVQAE). Main Goals: 1. Dataset Creation: The primary goal of ViQuAE is to create a dataset of questions paired with images, covering a wide range of entity types such as persons, landmarks, and products 2. Evaluation Framework: ViQuAE aims to establish an evaluation framework for the KVQAE task, allowing researchers to benchmark and compare the performance of machine learning models in answering visual questions about named entities 3. Multimodal Challenges: The benchmark seeks to address the challenges of multimodal representations, including the integration of visual information with textual knowledge for question answering tasks 4. Knowledge-based Question Answering: ViQuAE focuses on testing machine learning models' abilities to reason and answer questions by leveraging a Knowledge Base, thereby evaluating their capacity for knowledge-intensive language tasks Defining Features: 1. Entity Diversity: ViQuAE comprises a diverse set of entity types, including humans, fictional or mythological characters, groups of humans, and various other entities, providing a rich and varied dataset for evaluation 2. Semi-automatic Annotation: The dataset is annotated using a semi-automatic method, which involves replacing the entity mention in the question with a depiction of the entity and referencing it by an ambiguous mention, thereby grounding the question in the visual context  3. Knowledge Base Integration: ViQuAE includes a Knowledge Base composed of 1.5M Wikipedia articles paired with images, enabling machine learning models to access textual and visual information for answering questions about named entities 4. Two-stage Problem: The benchmark addresses the KVQAE task as a two-stage problem, involving Information Retrieval and Reading Comprehension, and provides baseline methods for zero- and few-shot learning to evaluate machine learning models' performance Specific Aspects for Machine Learning Models: 1. Multimodal Representation Learning: ViQuAE challenges machine learning models to learn effective representations that integrate visual and textual information for answering questions about named entities 2. Knowledge Integration: The benchmark evaluates the ability of machine learning models to effectively integrate information from the Knowledge Base to reason and provide accurate answers to visual questions 3. Reasoning and Comprehension: ViQuAE tests machine learning models' capabilities for reasoning and comprehension, especially in scenarios where questions may require multi-hop reasoning over multiple sentences or paragraphs in the document''',\n",
    "    'VisDial': '''The VisDial benchmark is designed to evaluate and test machine learning models in the context of visual dialog. Here's a comprehensive description of the VisDial benchmark, including its main goals, defining features, and specific aspects of machine learning models it aims to test and evaluate: Main Goals: 1. Visual Understanding: The benchmark aims to assess the ability of machine learning models to understand and interpret visual content, particularly images, in the context of dialog interactions. 2. Natural Language Processing: It evaluates the models' proficiency in processing and generating natural language responses in a conversational setting, focusing on question-answering and dialog generation. 3. Multi-Modal Fusion: The benchmark emphasizes the fusion of visual and textual modalities, testing the models' capability to integrate information from both image content and textual dialog history. Defining Features: 1. Image-Dialog Interaction: VisDial involves a multi-round conversational setting where a sequence of questions and answers is exchanged based on a given image, leading to a comprehensive dialog history. 2. Free-Form Answers: The benchmark requires models to generate free-form natural language answers to questions posed in the dialog, reflecting the complexity of human language understanding and generation. 3. Reinforcement Learning: It incorporates reinforcement learning techniques to train generative visual dialog models, enabling the agents to learn from their own dialog interactions and improve their performance over time. Specific Aspects of Machine Learning Models: 1. Visual Question Answering (VQA): The benchmark evaluates the models' ability to answer questions related to visual content, requiring them to comprehend and analyze the image to provide accurate responses. 2. Dialog Generation: It assesses the models' capability to generate coherent and contextually relevant dialog responses, considering the conversational history and the visual context provided by the image. 3. Multi-Modal Fusion: VisDial tests the models' proficiency in fusing information from visual and textual modalities, emphasizing the integration of image features with dialog context to generate meaningful responses.''',\n",
    "    'VizWiz': '''The VizWiz benchmark is a dataset designed to evaluate the performance of machine learning models in answering visual questions from blind individuals. Its main goals are to encourage the development of more generalized algorithms that can assist blind people in answering visual questions and to provide a challenging dataset that can be used to evaluate the performance of such algorithms. The defining features of VizWiz are its natural VQA setting, where blind individuals take pictures using a mobile phone and record a spoken question about it, and the crowdplatformd answers to each visual question. The images in VizWiz are often of poor quality, and the questions are more conversational than those in other VQA datasets. Additionally, many visual questions in VizWiz cannot be answered due to the limitations of the blind photographer. The specific aspects of machine learning models that VizWiz aims to test and evaluate include their ability to accurately classify visual questions as answerable or unanswerable, their ability to generate accurate answers to visual questions, and their ability to generalize to new datasets. The benchmark also evaluates the performance of models that incorporate attention mechanisms and those that use different combinations of image, question, and caption models.''',\n",
    "    'WNLI': '''WNLI (Winograd NLI) benchmark dataset within GLUE focuses on evaluating machine learning models' proficiency in coreference resolution and contextual reasoning. The main goal of the WNLI dataset is to evaluate a model's ability to resolve ambiguous pronouns and understand the contextual relationships between different parts of a sentence The defining feature of the WNLI dataset is its focus on pronoun resolution and contextual reasoning. It consists of a set of carefully crafted examples that require the model to understand the specific context in which a pronoun is used and to make accurate predictions based on that understanding Specifically, the WNLI dataset aims to test and evaluate the following aspects of machine learning models: 1. Coreference Resolution: Models are evaluated on their ability to correctly identify the referent of a pronoun in a given context. 2. Contextual Reasoning: Models are tested on their capability to understand and reason about the contextual information in a sentence to make accurate predictions about the relationship between different parts of the sentence''',\n",
    "    'WSC': '''The Winograd Schema Challenge (WSC) is a benchmark designed to test the ability of natural language processing (NLP) models to understand the meaning of text. The main goal of the WSC is to evaluate the ability of machine learning models to perform common-sense reasoning and to understand the relationships between entities in a sentence. The WSC consists of a set of binary questions that require the model to determine the referent of a pronoun or possessive adjective in a sentence. The questions are designed in such a way that the correct answer cannot be determined by statistical analysis of the text alone, but rather requires an understanding of the relationships between the entities mentioned in the sentence. The defining features of the WSC are as follows: 1. Binary questions: Each question in the WSC is a binary question that requires the model to choose between two possible answers. 2. Pronoun resolution: The questions in the WSC require the model to determine the referent of a pronoun or possessive adjective in a sentence. 3. Common-sense reasoning: The questions in the WSC require the model to use common-sense reasoning to understand the relationships between entities in a sentence. 4. Special and alternate words: Each question in the WSC contains a special word that can be replaced by an alternate word without changing the meaning of the sentence. The alternate word changes the correct answer to the question. The WSC aims to test and evaluate the ability of machine learning models to perform common-sense reasoning and to understand the relationships between entities in a sentence. Specifically, the WSC tests the ability of models to perform pronoun resolution, to use common-sense reasoning to understand the relationships between entities in a sentence, and to handle the ambiguity introduced by the use of special and alternate words.''',\n",
    "    'WebShop': '''The WebShop benchmark is a simulated e-commerce website environment developed to address the challenges of grounding language in interactive environments. Its main goals include providing a scalable and realistic platform for training autonomous agents to understand human-provided text instructions and perform tasks related to online shopping. The benchmark aims to emulate the complexities of real-world web interactions, including understanding compositional instructions, query (re-)formulation, comprehending and acting on noisy text in webpages, and performing strategic exploration. Key features of the WebShop benchmark include: 1. Large-scale Environment: WebShop contains over 1.18 million real-world products scraped from Amazon.com and over 12,000 crowd-platformd text instructions, making it a large-scale and diverse environment for training and evaluating agents. 2. Realistic Language and Visual Elements: The platform aims to provide realistic language and visual elements, emulating the task of online shopping on an e-commerce website, thus challenging agents to understand and act upon human-provided text instructions to purchase a product that matches the specified requirements 3. Human Demonstrations: The benchmark collects trajectories from humans performing the task to understand the task difficulty and to analyze how humans would solve the task. This data is used for imitation learning and to train and evaluate agents 4. Scalability and Automation: WebShop is designed to be scalable and contains a diverse semantic action space, packaged into a convenient OpenAI Gym environment with rewards automatically computed using programmatic matching functions, thus alleviating the need for human evaluation and providing a path to scaling up interactive learning The specific aspects of machine learning models that the WebShop benchmark aims to test and evaluate include: 1. Language Understanding: The benchmark challenges agents to understand and act upon human-provided text instructions, requiring robust language understanding for textual state and action spaces 2. Sequential Decision Making: Agents are required to navigate multiple types of webpages and issue diverse actions to find, customize, and purchase an item, thus testing their strategic exploration abilities in a sequential decision-making environment. 3. Sim-to-Real Transfer: The benchmark evaluates the performance of trained agents in real-world scenarios by measuring their ability to transfer learned behaviors to real-world shopping websites, demonstrating the practical applicability of the trained models''',\n",
    "    'Whoops': '''The WHOOPS benchmark is a groundbreaking initiative designed to challenge AI models in the realm of visual commonsense and compositionality. Its main goals include evaluating the ability of machine learning models to reason about unconventional and implausible images, as well as to generate detailed explanations, captions, and answers to questions related to these images. The benchmark aims to test and evaluate state-of-the-art vision-and-language models in several key areas of visual commonsense reasoning. Defining Features: 1. Synthetic Images: WHOOPS features a novel dataset of synthetic images that are intentionally challenging or even impossible to obtain. These images are designed to push the boundaries of AI models' understanding of common sense and compositionality. 2. Diverse Categories: The images in the WHOOPS dataset span across various categories, providing a wide range of challenges to AI models and testing their capabilities in multiple areas of common sense. 3. Benchmark Tasks: The benchmark includes four challenging tasks: explanation generation, image captioning, distinguishing between detailed and underspecified captions, and visual question answering. These tasks are carefully designed to evaluate the ability of AI models to reason about commonsense and compositionality in the context of visual data. Specific Aspects of Machine Learning Models Tested and Evaluated: 1. Explanation Generation: The benchmark evaluates the ability of AI models to provide detailed explanations of what makes an image weird. This task aims to assess the models' capacity to understand and articulate the incongruity present in the synthetic images. 2. Image Captioning: AI models are tested on their capability to summarize the content of the synthetic images through accurate and meaningful captions. This task assesses the models' ability to comprehend and describe unconventional visual scenes. 3. Cross-Modal Matching: Models are challenged to distinguish between detailed and underspecified captions, testing their ability to score a detailed caption higher than a correct but underspecified one. This task evaluates the models' understanding of the richness and specificity of image descriptions. 4. Visual Question Answering: AI models are evaluated on their capacity to answer questions that test their comprehension of the unconventional images. This task assesses the models' ability to reason about visual data and provide meaningful answers based on their understanding of the images.''',\n",
    "    'WiC': '''The WiC (Word-in-Context) dataset aims to evaluate context-sensitive meaning representations, addressing the limitations of existing word embeddings and providing a benchmark for assessing context-sensitive word representations. Here's a comprehensive description of the WiC benchmark: 1. Main Goals: - Evaluate context-sensitive meaning representations: WiC aims to assess the ability of machine learning models to capture the dynamic nature of word semantics, particularly for ambiguous words that can have different meanings in different contexts. - Provide a benchmark for generic evaluation: The dataset serves as a standardized benchmark for testing and comparing various techniques, including contextualized word and sense representation, word sense disambiguation, and multi-prototype embeddings. 2. Defining Features: - Binary classification task: The task is framed as a binary classification problem, where each instance in WiC has a target word (verb or noun) and two contexts. The goal is to determine if the occurrences of the target word in the two contexts correspond to the same meaning or not. - High-quality annotations: The dataset is constructed using high-quality annotations curated by experts, ensuring the reliability and accuracy of the dataset for evaluating context-sensitive word representations. - Large-scale dataset: WiC provides a large-scale dataset based on lexical examples from rePlatform such as WordNet, VerbNet, and Wiktionary, offering a diverse and comprehensive set of instances for evaluation. 3. Specific Aspects of Machine Learning Models Tested and Evaluated: - Contextualized word embeddings: WiC evaluates the performance of contextualized word embeddings, which compute dynamic embeddings for words that can adapt to arbitrary contexts. This includes assessing the ability of these embeddings to capture and distinguish different meanings of words in context. - Multi-prototype embeddings: The dataset also tests multi-prototype embeddings, which leverage context clustering to learn distinct representations for individual meanings of words. It aims to evaluate the effectiveness of these embeddings in capturing and representing word senses in different contexts. - Word sense disambiguation techniques: WiC provides a platform for evaluating word sense disambiguation techniques, assessing their ability to disambiguate the meanings of ambiguous words based on contextual information.''',\n",
    "    'WikiFact': '''The WikiFact benchmark aims to evaluate the ability of pretrained language models to store and retrieve relational knowledge, effectively serving as knowledge bases. Here are the main aspects of the WikiFact benchmark: 1. Main Goals: - Evaluate the performance of pretrained language models in storing and retrieving relational knowledge. - Analyze the ability of language models to predict masked objects in sentences, testing their capacity to recall and utilize information from their training data. - Compare the performance of language models with traditional NLP methods in capturing and utilizing relational knowledge. 2. Defining Features: - Utilizes a set of knowledge Platform, each comprising a corpus of facts in the form of subject-relation-object triples or question-answer pairs. - Converts each fact into a cloze statement, which is used to query the language model for a missing token, thereby evaluating the model's ability to recall factual and commonsense knowledge. - Evaluates the language models based on how well they rank the ground truth token against other words in a fixed candidate vocabulary, similar to ranking-based metrics from the knowledge base completion literature. 3. Specific Aspects of Machine Learning Models: - The benchmark aims to test the ability of language models to predict masked objects in sentences, assessing their capacity to recall and utilize information from their training data - It evaluates the language models' performance in capturing factual and commonsense knowledge, including relations between entities stored in Wikidata, common sense relations between concepts from ConceptNet, and knowledge necessary to answer natural language questions in SQuAD - The benchmark also aims to quantify the variance of recalling factual knowledge with respect to varying natural language templates and assess multi-token answers, highlighting open challenges for the evaluation setup''',\n",
    "    'WikiHow': '''The WikiHow benchmark is a large-scale text summarization dataset that aims to address the challenges of existing summarization datasets, such as limited size, specific writing styles, and limited abstraction levels. The main goals of the WikiHow benchmark are to provide a diverse range of topics and writing styles, convenient for long-sequence text summarization, and to evaluate the performance of existing extractive and abstractive summarization models on this dataset. The defining features of the WikiHow dataset include its platform from the online WikiHow knowledge base, which contains articles written by ordinary people describing the steps of doing a task throughout the text. Each article consists of multiple paragraphs, and each paragraph starts with a sentence summarizing it. By merging the paragraphs to form the article and the paragraph outlines to form the summary, the resulting version of the dataset contains more than 200,000 long-sequence pairs. The dataset covers a wide range of topics, from arts and entertainment to computers and electronics, and represents a diverse range of writing styles. The WikiHow benchmark aims to test and evaluate the performance of machine learning models on long-sequence text summarization, with a focus on abstractive summarization. The dataset provides a challenging task for models to generate summaries that capture the key information and meaning of the original text while compressing it into a shorter form.''',\n",
    "    'WinoGrande': '''The WinoGrande benchmark was designed to test the robustness of machine commonsense capabilities. Its main goals are to create a dataset that is challenging for existing state-of-the-art models while still being trivially easy for humans, and to demonstrate effective transfer learning. The dataset consists of 44k problems inspired by the original Winograd Schema Challenge (WSC) design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of a carefully designed crowdsourcing procedure, followed by systematic bias reduction using a novel AFLITE algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. The problems in WINOGRANDE are designed to be pronoun resolution problems that are trivial for humans but hard for machines that merely rely on statistical patterns without true capabilities of commonsense reasoning.''',\n",
    "    'Winogender Diagnostic': '''The Winogender benchmark is a set of minimal pair sentences designed to evaluate gender bias in coreference resolution systems. The benchmark is based on the Winograd Schema Challenge, which is a set of sentence pairs that differ by only one word and require commonsense reasoning to correctly answer. Similarly, the Winogender benchmark consists of sentence templates with three referring expressions of interest: OCCUPATION, PARTICIPANT, and PRONOUN. The goal is to determine whether the PRONOUN refers to OCCUPATION or PARTICIPANT, with the added challenge of varying the gender of the PRONOUN. The benchmark is designed to test and evaluate the performance of machine learning models in coreference resolution, specifically with respect to gender bias. The authors evaluate three different coreference resolution systems, representing three different machine learning paradigms: rule-based systems, statistical systems, and neural systems. They find that all three systems exhibit systematic gender bias, with male pronouns more likely to be resolved as OCCUPATION than female or neutral pronouns. The defining feature of the Winogender benchmark is its focus on gender bias in coreference resolution systems. The authors use a list of 60 one-word occupations obtained from the U.S. Bureau of Labor Statistics, with corresponding gender percentages, to create the sentence templates. They also use a human-validated approach to ensure that the sentences are easy for humans to interpret but challenging for data-driven systems. The benchmark aims to test and evaluate the ability of machine learning models to correctly resolve pronouns in sentences with varying gender. It also aims to highlight the presence of gender bias in coreference resolution systems and to encourage the development of systems that mitigate gender-based overgeneralization. Overall, the Winogender benchmark provides a valuable tool for evaluating and improving the accuracy and fairness of coreference resolution systems.''',\n",
    "    'Winoground': '''The Winoground benchmark aims to evaluate the ability of vision and language models to conduct visio-linguistic compositional reasoning. It introduces a novel task and dataset that focuses on measuring the models' capabilities in understanding the relationship between text in captions and corresponding visual content. The main goals, defining features, and specific aspects of machine learning models that Winoground aims to test and evaluate are as follows: Main Goals: 1. Visio-Linguistic Compositionality: The primary goal of Winoground is to measure the ability of models to perform visio-linguistic compositional reasoning. This involves understanding the relationship between visual content and textual descriptions, particularly when the same set of words is presented in different orders in the captions. 2. Model Evaluation: Winoground serves as a probing task for vision and language models, aiming to assess their performance in handling visio-linguistic compositionality. It provides a challenging benchmark to evaluate the capabilities of state-of-the-art models in this specific area. Defining Features: 1. Hand-Curated Dataset: The Winoground dataset is carefully hand-curated by expert annotators, ensuring high-quality annotations and labels. It consists of 1600 image-text pairs, with 800 correct and 800 incorrect pairings, comprising 400 examples with 800 unique captions and images. 2. Identical Set of Words in Different Orders: Winoground presents a unique challenge by providing two captions that contain a completely identical set of words, only in a different order. This feature requires models to understand and interpret the compositional structure of the captions and their corresponding visual content. 3. Fine-Grained Tags: The dataset is labeled with a rich set of fine-grained tags to assist in analyzing model performance. These tags provide detailed insights into the specific aspects of visio-linguistic compositionality that models may struggle with. Specific Aspects Tested and Evaluated: 1. Text-Image Matching: The task in Winoground involves matching two images and two captions correctly, where both captions contain exactly the same set of words but in a different order. Models are evaluated based on their ability to correctly pair the images and captions, demonstrating their understanding of the relationship between visual and textual information. 2. Visio-Linguistic Reasoning: Models are tested on their visio-linguistic reasoning abilities, including their capacity to encode text and images effectively, be sensitive to the compositional structure present in each modality, and synthesize information across the two modalities. 3. Model Performance Metrics: Winoground computes performance metrics that evaluate different aspects of the models' visio-linguistic reasoning abilities, such as the text score, which measures whether a model can select the correct caption given an image.''',\n",
    "    'WordArt': '''The WordArt benchmark aims to address the challenging task of artistic text recognition, which involves recognizing text with diverse fonts, artistic designs, extreme deformations, background interference, overlapping characters, and other complexities The main goals of the WordArt benchmark are to provide a standardized dataset for evaluating and comparing the performance of machine learning models specifically tailored for artistic text recognition, and to advance the state-of-the-art in this domain. Defining Features of WordArt Benchmark: 1. Artistic Text Emphasis: The WordArt dataset focuses on artistic text, which is distinct from traditional scene text recognition due to its unique challenges and characteristics 2. Diverse Text Examples: The dataset includes diverse examples of artistic text, encompassing different fonts, background interference, extreme deformations, word effects, artistic designs, and overlapping characters 3. Training and Testing Sets: The dataset is divided into training and testing sets, enabling the evaluation of models on unseen data 4. Statistical Analysis: The dataset provides statistical insights into text length and character frequency, aligning with the English corpus 5. Qualitative Presentation: The dataset offers a qualitative presentation of the included artistic text examples Specific Aspects of Machine Learning Models Tested and Evaluated: 1. Corner-Query Cross-Attention Mechanism: The benchmark evaluates the effectiveness of the corner-query cross-attention mechanism, which allows models to pay more accurate attention to the core strokes or character centers 2. Character Contrastive Loss: The benchmark assesses the impact of the character contrastive loss, which aims to cluster the same class of character features, enabling the model to learn unified representations for characters 3. Global Feature Extraction: The benchmark tests the global feature extraction capabilities of machine learning models, particularly focusing on the use of Transformer-based approaches for modeling global relationships of corner points 4. Model Performance: The WordArt benchmark aims to benchmark the performance of different machine learning models on artistic text recognition, comparing their word accuracy, character recall, and character precision on the WordArt dataset''',\n",
    "    'XLORE2': '''The XLORE2 benchmark is designed to evaluate the performance of machine learning models in cross-lingual knowledge graph completion. Its main goals are to provide a standardized evaluation framework for researchers and practitioners working in this field, and to promote the development of more accurate and efficient models for cross-lingual knowledge graph completion. The defining features of the XLORE2 benchmark include a large-scale cross-lingual knowledge graph, a set of evaluation metrics, and a collection of benchmark datasets. The knowledge graph is built automatically from Wikipedia, Baidu Baike, and Hudong Baike, and contains over 10 million entities and 100 million facts. The evaluation metrics include precision, recall, and F1 score, and are designed to measure the accuracy and completeness of the predicted links between entities in different languages. The benchmark datasets consist of pairs of entities in different languages, along with their corresponding features and labels. The specific aspects of machine learning models that the XLORE2 benchmark aims to test and evaluate include entity alignment, relation prediction, and type inference. Entity alignment refers to the task of linking entities in different languages that refer to the same real-world object. Relation prediction refers to the task of predicting the relationships between entities in different languages, such as \"born in\" or \"works for\". Type inference refers to the task of predicting the types of entities in different languages, such as \"person\" or \"organization\".''',\n",
    "    'XSUM': '''The XSUM benchmark is a new dataset for the task of extreme summarization. The goal of this benchmark is to push the boundaries of abstractive summarization methods by providing a large-scale dataset of BBC articles and accompanying single-sentence summaries that require a more abstract understanding of the content. The defining features of the XSUM dataset include the fact that each article is prefaced with an introductory sentence (aka summary) which is professionally written, typically by the author of the article. The summary bears the HTML class “story-body introduction,” and can be easily identified and extracted from the main text body. The dataset consists of 226,711 Wayback archived BBC articles ranging over almost a decade (2010 to 2017) and covering a wide variety of domains (e.g., News, Politics, Sports, Weather, Business, Technology, Science, Health, Family, Education, Entertainment and Arts). The specific aspects of machine learning models that the XSUM benchmark aims to test and evaluate include their abstractive capabilities, their ability to recognize pertinent content and generate informative summaries, and their incorporation of high-level document knowledge in terms of topics and long-range dependencies.''',\n",
    "    'Xiezhi': '''The Xiezhi benchmark aims to provide a comprehensive evaluation suite for assessing holistic domain knowledge. Its main goals are to establish a taxonomy for human knowledge and build evaluation criteria for this field. The benchmark encompasses 13 distinct disciplinary categories, 118 sub-disciplines, and 385 further fine-grained disciplines, providing an extensive domain taxonomy and benchmark for fair, effective, and comprehensive domain evaluation. Xiezhi is unique in that it covers a wide range of disciplines and sub-disciplines, making it the most dimensional domain evaluation benchmark with the largest volume of data. It currently offers Chinese and English language versions, but the authors plan to collect more thorough and in-depth knowledge from various countries, nations, fields, and open platform benchmarks in more languages in the future. The specific aspects of machine learning models that Xiezhi aims to test and evaluate include their ability to understand and execute instructions, align with human values, and perform well across multiple tasks.''',\n",
    "    'YouCook2': '''YouCook2 is a benchmark dataset for evaluating machine learning models that aim to segment and understand instructional cooking videos. The main goal of YouCook2 is to enable the development of models that can learn to segment and understand procedures from long, unconstrained videos with only visual evidence. The defining features of YouCook2 are its large-scale dataset of cooking videos, temporally localized procedure segments, and English sentence descriptions of each segment. The dataset contains 2000 videos, nearly equally distributed over 89 recipes from four major cuisine locales. The videos were collected from YouTube and contain various challenges, such as fast camera motion, camera zooms, video defocus, and scene-type changes. Each video contains 3-16 procedure segments, which are temporally localized and described by English sentences in imperative form. The specific aspects of machine learning models that YouCook2 aims to test and evaluate are procedure segmentation and understanding. Procedure segmentation involves segmenting an untrimmed and unconstrained video into a sequence of procedure segments. This is accomplished by three core modules: context-aware video encoding, segment proposal module that localizes a handful of proposal candidates, and sequential prediction that predicts final segments based on segment-level dependencies among candidates. At training, models are given ground-truth procedure segment boundaries for each video; no recipe categories or segment descriptions are given. At testing, for any given unseen video, models propose and localize procedure segments in the video based on their visual appearance and temporal relations. YouCook2 also aims to evaluate models' ability to understand the procedures in the videos. This involves recognizing the actions and objects involved in each procedure segment and generating natural language descriptions of the procedures. Models that can accurately segment and understand the procedures in instructional cooking videos have potential applications in areas such as recipe recommendation, cooking assistance, and food recognition.''',\n",
    "}\n",
    "\n",
    "leaderboard_description_list = {\n",
    "    'AgentBench': '''AGENTBENCH is a multi-dimensional benchmark designed to evaluate Language Model (LM) as an Agent across a spectrum of different environments. Its main goals include providing a comprehensive evaluation of LM agents in diverse application scenarios, assessing their reasoning and decision-making abilities in multi-turn open-ended generation settings, and identifying areas for improvement in LM agent performance. The defining features of AGENTBENCH include its multi-dimensional nature, encompassing eight distinct environments categorized into three types of groundings: Code (e.g., Operating System, Database, Knowledge Graph), Game (e.g., Digital Card Game, Lateral Thinking Puzzles, House-Holding), and Real-world (e.g., HH). This diverse set of environments allows for a practical testbed for evaluating the wide array of capabilities of LM agents. Additionally, AGENTBENCH provides an integrated evaluation package with a toolkit for seamless deployment of tasks and agents, including agent servers, task servers, and evaluation clients. Specific aspects of machine learning models that AGENTBENCH aims to test and evaluate include the long-term reasoning, decision-making, and instruction-following abilities of LM agents in interactive environments. The benchmark evaluates 27 different LM agents, including API-based and open-platform models, to uncover performance gaps and identify areas for improvement. It also emphasizes the importance of training on code and high-quality multi-turn alignment data to improve LM agent performance.''',\n",
    "    'AGIEval': '''AGIEval aims to assess the general abilities of foundation models to tackle human-level tasks, which is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. Therefore, AGIEval releases collected questions from high-standard, official human exams and supports a standardized, automatic evaluation metric. The defining feature of AGIEval is its emphasis on human-level cognitive tasks. In designing the human-centric benchmark, the primary objective is to center on tasks that closely align with human cognition and problem-solving, ultimately assessing the general abilities of foundation models in a more meaningful and comprehensive manner. To achieve this, a diverse range of official, public, and high-standard admission and qualification exams that cater to general human test-takers are incorporated. These exams, which include college admission tests, law school admission tests, math exams, lawyer qualification tests, and national civil service exams, are taken by millions of individuals seeking entry into higher education or new career paths. By adhering to these officially recognized standards for evaluating human-level capabilities, the benchmark design ensures that the assessment of model performance is directly relevant to human decision-making and cognitive abilities. AGIEval aims to test and evaluate specific aspects of machine learning models, including their ability to understand and reason with natural language, their capacity to solve complex problems, and their proficiency in domain-specific knowledge. Additionally, AGIEval incorporates bilingual tasks in both Chinese and English, offering a more comprehensive assessment of model behavior. By addressing these future directions, foundation models can be further developed and refined to exhibit more advanced capabilities that align closely with human cognition, ultimately enabling them to tackle a broader range of complex, human-centric tasks with greater accuracy and reliability.''',\n",
    "    'BEIR': '''The BEIR benchmark was designed with several key goals in mind. First and foremost, it aims to provide a comprehensive evaluation of the generalization capabilities of information retrieval (IR) models. To achieve this, the benchmark includes a diverse set of 18 publicly available datasets from a range of text retrieval tasks and domains. One defining feature of the BEIR benchmark is its heterogeneity. Unlike many existing retrieval benchmarks, which focus on a single task or domain, BEIR includes datasets from a wide range of retrieval tasks, including fact checking, citation prediction, duplicate question retrieval, argument retrieval, news retrieval, question answering, tweet retrieval, bio-medical IR, and entity retrieval. Additionally, the benchmark includes datasets from diverse text domains, covering both broad and specialized topics, and datasets of varying sizes and query/document lengths. Another key feature of the BEIR benchmark is its focus on zero-shot evaluation. This means that models are evaluated on tasks and domains that they have not been specifically trained on, in order to test their ability to generalize to new and unseen data. In terms of the specific aspects of machine learning models that BEIR aims to test and evaluate, the benchmark includes 10 state-of-the-art retrieval systems from five broad architectures: lexical, sparse, dense, late interaction, and re-ranking. These models are evaluated on a range of metrics, including mean average precision (MAP), normalized discounted cumulative gain (NDCG), and precision at k (P@k), among others.''',\n",
    "    'Big Code Models Leaderboard': '''The Big Code Models Leaderboard on Hugging Face is a platform designed to compare the performance of base multilingual code generation models. Defining Features Performance Metrics: Models are evaluated based on their performance on the HumanEval benchmark and MultiPL-E, measuring functional correctness in synthesizing programs from docstrings. Throughput Measurement: The leaderboard also measures model throughput (tokens per second) to compare inference speed. Diverse Programming Languages: It includes benchmarks for multiple programming languages. Evaluation Parameters: Models are evaluated using the big code-evaluation-harness with specific settings. Specific Aspects Tested and Evaluated Functional Correctness: Using the HumanEval benchmark, the leaderboard assesses the ability of models to synthesize correct programs from given docstrings. Multilingual Capabilities: Through the MultiPL-E benchmark, evaluates the performance across 18 programming languages. Inference Speed: By measuring throughput, the leaderboard evaluates how quickly models can process tokens. Memory Usage: Peak memory usage is also measured, providing insights into the efficiency of the models. Scoring and Rankings: Models are ranked based on the average pass@1 score over all languages and their win rate, which represents how often a model outperforms others. Overall, this leaderboard serves as a crucial tool for the AI community, offering a comprehensive and comparative view of the capabilities of various code generation models, especially in terms of multilingual support and functional correctness.''',\n",
    "    'Chatbot Arena Leaderboad': '''The Chatbot Arena Leaderboard is a benchmarking platform designed to evaluate and compare large language models (LLMs), particularly focusing on chatbots. Its main goal is to provide a reliable and scalable method for assessing the performance of various chatbot models in real-world scenarios. Main Goals: Evaluating Chatbots: The primary aim is to test and compare the abilities of different chatbots in a controlled environment. User-Driven Insights: It leverages user interactions and preferences to gauge the effectiveness of chatbot responses. Defining Features: Elo Rating System: The leaderboard employs the Elo rating system, commonly used in chess, to rate chatbots based on their performance in head-to-head matchups. This system is well-suited for a scenario with multiple models running pairwise battles. User Engagement: Users interact with two anonymous chatbot models, ask questions, and then vote for the one they prefer. This approach mimics real-world use cases of chatbots. Scalability and Incrementality: The system is designed to efficiently handle a large number of models and provide evaluations with a relatively small number of trials. Unique Ordering: It aims to provide a clear ranking of models, indicating which one performs better in direct comparisons. Aspects of Machine Learning Models Tested: Conversational Abilities: The platform assesses how well chatbots can maintain a conversation, respond to queries, and follow instructions. MT-Bench Integration: A new benchmark, MT-Bench, is added to evaluate multi-turn conversational abilities and instruction-following capabilities of models. It includes a set of high-quality, multi-turn questions tailored to test different aspects of conversation flow. Diverse Query Handling: The leaderboard categorizes user prompts into various types such as Writing, Roleplay, Extraction, Reasoning, Math, Coding, and Knowledge in both STEM and humanities/social sciences. Comparative Analysis: Beyond individual performance, the leaderboard provides insights into how different models stack up against each other in various categories. Automated Evaluation Using GPT-4: For certain aspects like MT-Bench, the responses of chatbots are evaluated using GPT-4, providing a scalable way to approximate human judgment. Unique Evaluation Approaches: Pairwise Comparison: This approach compares two models at a time, allowing for a direct assessment of their relative capabilities. Crowdplatformd Data Collection: The platform collects data through user interactions, making the evaluation process dynamic and reflective of real user preferences. Automated and Human-Integrated Grading: The grading of responses is done both through automated processes (using models like GPT-4) and human judgments, balancing efficiency with reliability.''',\n",
    "    'ChEF': '''The Comprehensive Evaluation Framework (ChEF) aims to provide a standardized and holistic assessment of Multimodal Large Language Models (MLLMs) by addressing the limitations of existing benchmarks and evaluation frameworks. ChEF aims to profile the intrinsic properties of MLLMs and enable meaningful comparisons among different models. Here's a comprehensive overview of ChEF: Main Goals: 1. Standardized Assessment: ChEF aims to establish a standardized evaluation framework for MLLMs, allowing for consistent and reliable comparisons across different models. 2. Holistic Profiling: ChEF seeks to comprehensively profile the capabilities and limitations of MLLMs, especially in the context of real-world multimodal interactions. 3. Versatile Evaluations: ChEF facilitates the evaluation of MLLMs across diverse multimodal tasks, providing a scalable and adaptable framework for assessing model performance. Defining Features: 1. Modular Design: ChEF is structured around four modular components: Scenario, Instruction, Inferencer, and Metric. This modular design allows for flexibility and adaptability in evaluating different MLLMs and tasks. 2. Scalability: The framework is designed to be scalable, accommodating the inclusion of new datasets, evaluation tasks, and evaluation criteria as the field of multimodal language models evolves. 3. Standardization: ChEF aims to standardize the evaluation process, ensuring that each model can be conveniently assessed and compared within the framework. Specific Aspects of Machine Learning Models: ChEF aims to test and evaluate various aspects of MLLMs, including but not limited to: - Multimodal Perception and Reasoning: ChEF assesses the robustness, in-context learning, and other vital capabilities of MLLMs in the context of multimodal perception and reasoning. - Performance Across Tasks: The framework evaluates MLLMs' performance across representative multimodal tasks, such as image classification, object detection, multimodal question-answering, and multi-task benchmarks. - Question-Answering Strategies: ChEF assesses how MLLMs answer questions, including single-turn question-answering, multi-turn question-answering, and the use of strategies such as Perplexity (PPL) and Chain-of-Thought (CoT) prompting. - Task-Specific Metrics: The framework includes task-specific metrics such as accuracy, mAP, BLEU, and more, designed to evaluate the performance of MLLMs across different tasks and modalities.''',\n",
    "    'CLUE': '''The CLUE benchmark is a large-scale Chinese Language Understanding Evaluation designed to test and evaluate the performance of machine learning models on a variety of sentence classification and machine reading comprehension tasks in Chinese. The benchmark is unique in that it covers a wide range of tasks, at different levels of difficulty, in different sizes and forms, all on original Chinese text. The main goals of the CLUE benchmark are to provide a standardized evaluation framework for Chinese NLU models, to encourage reproducibility and transparency in research, and to promote the development of high-quality Chinese NLU models. Some of the defining features of the CLUE benchmark include a diagnostic dataset that contains nine linguistic and logic phenomena, a leaderboard for users to submit their own results, and a toolkit named PyCLUE implemented in TensorFlow that supports mainstream pre-training models and a wide range of target tasks. The specific aspects of machine learning models that the CLUE benchmark aims to test and evaluate include model size, pre-training data, and whole word masking. The benchmark results show that larger models and models trained with more pre-training data tend to perform better, as do models that use whole word masking. The best performing models on the CLUE benchmark are RoBERTa-wwm-ext-large and ALBERT-xxlarge, which show advantages over other models particularly for machine reading tasks such as C3.''',\n",
    "    'EvalPlus': '''The EvalPlus benchmark is designed to rigorously evaluate the functional correctness of Large Language Models (LLMs) for code generation. Its main goals are to improve existing code benchmarks by generating interesting test inputs to fully exercise the code solution and check its functional correctness by cross-checking the ground-truth implementation. The defining features of EvalPlus include an automatic test input generation engine that uses both LLM- and mutation-based methods to generate and diversify additional test inputs. Specifically, EvalPlus first uses ChatGPT to generate a set of high-quality seed inputs that aim to test difficult corner cases and functionalities of the program within the valid input structure. Using these high-quality seed inputs, EvalPlus then performs type-aware mutation to efficiently generate a large number of additional test inputs. These newly generated test inputs are then used to evaluate the LLM-generated code through differential testing against the ground-truth implementation. The specific aspects of machine learning models that EvalPlus aims to test and evaluate include their ability to generate correct and efficient code solutions for a variety of programming tasks. EvalPlus evaluates the functional correctness of LLM-synthesized code by comparing it to the ground-truth implementation using differential testing. This approach allows for a more comprehensive evaluation of LLMs, as it can detect both syntactic and semantic errors in the generated code.''',\n",
    "    'FacTool': '''The FACTOOL benchmark is a framework for detecting factual errors in generative AI models. Its main goal is to provide a unified and versatile framework for factuality detection across a variety of domains and tasks. The defining features of the FACTOOL benchmark include its task and domain agnosticism, its use of various tools (such as Google Search, Google Scholar, code interpreters, Python, or even LLMs themselves) to gather evidence about the factuality of the generated content, and its employment of the reasoning abilities of LLMs to assess the factuality of the content given the evidence that has been gathered. The FACTOOL benchmark aims to test and evaluate specific aspects of machine learning models, including their ability to generate text that is factually accurate, their ability to reason about the factual accuracy of generated text, and their ability to use external Platform of information to verify the factual accuracy of generated text. FACTOOL benchmark is used to evaluate the factuality of modern chatbots across four tasks: knowledge-based QA, code generation, math problem solving, and scientific literature review writing.''',\n",
    "    'FlagEval': '''The FlagEval benchmark is a comprehensive evaluation system for large-scale models. It's designed to provide a scientific, fair, and open standard for evaluating the performance of foundational models and training algorithms. Main Goals of FlagEval Establishing New Evaluation Standards: The primary goal of FlagEval is to create a new coordinate system for evaluating large models, focusing on more nuanced and detailed aspects of model performance. Enhancing Evaluation Efficiency and Objectivity: By utilizing AI methods, FlagEval aims to significantly improve the efficiency and objectivity of both subjective and objective evaluations. Supporting Diverse Model Evaluation: FlagEval includes tools for evaluating language models, multi-language image-text models, and image-text generation models, among others. Defining Features of FlagEval Three-Dimensional Evaluation Framework: FlagEval is based on a \"capability-task-indicator\" framework, allowing for a more granular depiction of a model's cognitive abilities. Comprehensive Task Coverage: The system covers 22 subjective and objective evaluation sets, including 84,433 questions to test various aspects of large models. Model Types: FlagEval differentiates between foundational models (built from scratch with extensive training) and fine-tuned models (based on foundational models with additional fine-tuning). Continuous Updating and Expansion: The platform is in a state of ongoing improvement, with new evaluation results and capabilities being added regularly. Adaptation to Various Models: The system is designed to evaluate both language-based and multimodal models, covering a range of scenarios and tasks. Specific Aspects FlagEval Tests and Evaluates Basic and Advanced Language Abilities: This includes simple comprehension (like information analysis, extraction, summary), knowledge application, reasoning capabilities, creative generation, and context understanding. Safety and Ethical Aspects: The evaluation covers aspects like legality, privacy, political sensitivity, discrimination, and ethical standards. Comprehensive Abilities: General and domain-specific comprehensive abilities are assessed. Efficiency and Robustness: In addition to accuracy, metrics like efficiency, robustness, and uncertainty are also considered. Subjective and Objective Evaluation Methods: FlagEval combines manual and automated methods for both subjective and objective evaluations. Adaptive Evaluation Mechanisms: The platform allows for the integration of evaluation results into ongoing model training, adapting strategies based on performance feedback.''',\n",
    "    'HEIM': '''The Holistic Evaluation of Text-to-Image Models (HEIM) benchmark aims to provide a comprehensive evaluation of state-of-the-art text-to-image models across 12 different aspects, including text-image alignment, image quality, aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency. HEIM is unique in that it combines both automated metrics and crowdplatformd human evaluations to provide a more comprehensive understanding of model performance. The benchmark also includes a diverse collection of 62 scenarios, which are datasets of prompts, and 25 metrics, which are measurements used for assessing the quality of generated images specific to each aspect. The main goals of the HEIM benchmark are to provide a standardized evaluation framework for text-to-image models, offer holistic insights into model performance, and enable researchers, developers, and end-users to make informed decisions based on comparable assessments. Overall, the HEIM benchmark provides a quantitative understanding of the capabilities and risks of text-to-image models, and highlights the importance of evaluating models across multiple aspects to ensure their technological advancement and societal impact.''',\n",
    "    'HELM Classic': '''The Holistic Evaluation of Language Models (HELM) aims to provide a standardized and comprehensive evaluation of language models across a broad range of scenarios and metrics. The benchmark aims to test and evaluate language models in terms of their accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency. Additionally, HELM includes targeted evaluations of skills and risks, such as knowledge, reasoning, disinformation, and copyright. HELM is defined by its broad coverage, which includes 16 core scenarios and 21 new scenarios that have not been previously used in mainstream language model evaluation. The benchmark evaluates 30 language models under standardized conditions, ensuring that models can be directly compared across many scenarios and metrics. These models vary in terms of their public accessibility, with 10 being open, 17 being limited-access, and 3 being closed. The specific aspects of machine learning models that HELM aims to test and evaluate include their ability to accurately predict outcomes, their ability to handle different types of inputs and prompts, their ability to avoid bias and toxicity, and their efficiency in terms of speed and replatform usage. HELM also aims to evaluate the skills and risks associated with language models, such as their ability to reason and their potential for spreading disinformation. Overall, HELM provides a comprehensive and standardized approach to evaluating language models, with the goal of guiding future language model development and providing ample opportunities for further analysis.''',\n",
    "    'InstructEval': '''The InstructEval benchmark suite is a comprehensive evaluation tool designed to assess the performance of instruction-tuned large language models (LLMs) across a range of general capabilities, usage scenarios, and human-centric behavior. Its main goals are to provide a more holistic evaluation of LLMs that goes beyond traditional benchmarks, and to identify areas where models may fall short in terms of problem-solving, writing ability, and alignment with human values. INSTRUCTEVAL consists of three main evaluation categories: problem-solving, writing, and alignment to human values. The problem-solving category includes multiple benchmarks that cover real-world exams on diverse topics, complex instructions, arithmetic, programming, and causality. The writing category includes prompts that test the models' ability to write informative, persuasive, and creative texts. The alignment to human values category includes tasks that assess the models' ability to generate text that is consistent with human values and ethical principles. The defining features of INSTRUCTEVAL include its focus on instruction-tuned LLMs, its use of multiple evaluation methods (including both objective and qualitative scoring), and its coverage of a wide range of general abilities and usage scenarios. The benchmark suite is designed to test models on tasks that require world knowledge, multi-hop reasoning, creativity, and more, and to provide a comprehensive evaluation of their performance across a range of domains.''',\n",
    "    'KoLA': '''The KoLA benchmark is designed to evaluate the world knowledge of large language models (LLMs) in a thorough, unbiased, and applicable manner. Its main goals are to provide a diagnostic tool for the development of increasingly knowledgeable LLMs and to help developers of knowledge-related applications select appropriate LLMs. To achieve these goals, the KoLA benchmark has several defining features. First, it employs a four-level taxonomy of knowledge-related abilities, covering 19 tasks, to mimic human cognition for ability modeling. Second, it uses both Wikipedia, a corpus prevalently pre-trained by LLMs, and continuously collected emerging corpora for data to ensure fair comparisons and evaluate the capacity to handle unseen data and evolving knowledge. Third, it adopts a contrastive system for evaluation criteria, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge hallucination. The specific aspects of machine learning models that the KoLA benchmark aims to test and evaluate include their ability to handle world knowledge, their capacity to handle unseen data and evolving knowledge, and their ability to generate high-quality and faithful content. The benchmark evaluates 21 open-platform and commercial LLMs and provides intriguing findings that can guide the development of future LLMs and knowledge-related systems.''',\n",
    "    'LawBench': '''LawBench is a comprehensive evaluation benchmark designed to assess the legal capabilities of Large Language Models (LLMs) from three cognitive levels: legal knowledge memorization, understanding, and application. Its main goal is to provide a precise assessment of LLMs' legal knowledge and their ability to perform legal-related tasks in the highly specialized, safe-critical legal domain. LawBench is unique in its design and features. It covers 20 diverse tasks that are divided into five task types: legal fact verification, legal question answering, legal text completion, legal text modification, and legal text generation. The tasks are meticulously crafted to have precise assessment of the LLMs’ legal capabilities. The benchmark evaluates LLMs' legal knowledge memorization by testing whether they can memorize needed legal concepts, articles, and facts. It evaluates legal knowledge understanding by testing whether LLMs can comprehend entities, events, and relationships within legal text. Finally, it evaluates legal knowledge application by testing whether LLMs can properly utilize their legal knowledge and make necessary reasoning steps to solve realistic legal tasks.''',\n",
    "    'LLM Benchmarker Suite': '''The LLM Benchmarker Suite is a comprehensive framework designed to evaluate large-scale language models (LLMs). Key Components and Features: Manual Evaluation Process: The typical steps for manually conducting evaluations with the suite include loading the model, loading an appropriate benchmarking dataset, and selecting a relevant metric for evaluation​​. Model Loading: The suite emphasizes efficient and rapid local model inferences, utilizing libraries like vLLM for fast inference on LLMs. This approach is designed to maximize replatform utilization and throughput​​. Benchmarking Datasets: The suite incorporates a variety of datasets that test different proficiencies of a model, such as question answering or summarization. It includes hybrid benchmarks for a holistic understanding of a model's capabilities, preferring minimally curated datasets to avoid biases towards any specific model​​. Evaluation Metrics: The suite uses several common metrics for evaluation, including Perplexity, F1 Score, BLEU Score, ROUGE Score, Accuracy, and Human Evaluation. Each of these metrics has its pros and cons, tailored to different aspects of language model performance​​. Leaderboard and Metrics Dashboard: The suite provides a leaderboard for the community to rank all public and API models. Additionally, it offers a locally created metrics dashboard for tracking and comparing evaluations​​. Diverse Evaluation Tools: The suite includes tools like Static Evaluations, LLM-as-a-judge, and OpenAI Evals. These tools cater to various evaluation needs, such as static assessment of capabilities through predefined measures and using other LLMs as judges for inferencing capabilities​​.''',\n",
    "    'LLM-Leaderboard': '''The LLM-Leaderboard is a community-driven initiative to establish a centralized leaderboard for Large Language Models (LLMs). Its main goals and features include: Centralized Leaderboard: It provides a central platform for comparing various LLMs. The leaderboard includes model names, publishers, and whether the models are open for local deployment and commercial use​​​​. Evaluation Benchmarks: Various benchmarks are used to evaluate the LLMs, including Chatbot Arena Elo, HellaSwag, HumanEval-Python, LAMBADA, MMLU, TriviaQA, and WinoGrande. These benchmarks test aspects like chatbot performance, commonsense natural language inference, programming problem-solving, text understanding, and world knowledge.''',\n",
    "    'LLM-Perf Leaderboard': '''The LM-Perf Leaderboard serves as a critical tool in the realm of machine learning, particularly focusing on the evaluation and benchmarking of Large Language Models (LLMs). Its primary goal is to benchmark the performance of these models in terms of latency, throughput, and memory across different hardware, backends, and optimization configurations using Optimum-Benchmark and Optimum flavors​​.''',\n",
    "    'LongBench': '''The LongBench benchmark is a comprehensive evaluation tool designed to test the long context understanding capabilities of large language models (LLMs). Its main goals are to provide a standardized, bilingual, multi-task benchmark for evaluating LLMs on long-text application scenarios, and to facilitate further research in this direction. LongBench is composed of 21 different tasks across 6 major task categories, covering key long-text application scenarios including multi-document QA, single-document QA, summarization, few-shot learning, code completion, and synthesis tasks. These tasks are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. LongBench also includes different languages (Chinese and English) to provide a more comprehensive evaluation of the large models’ bilingual capabilities on long contexts. The defining features of LongBench include its focus on long context understanding, its bilingual and multi-task nature, and its standardized format for easy evaluation of LLMs. LongBench is unique in that it is the first benchmark tailored specifically for evaluating long context understanding, and it covers a wide range of long-text application scenarios.''',\n",
    "    'LVLM-eHub': '''The LVLM-eHub benchmark presented aims to provide a comprehensive evaluation of publicly available large vision-language models (LVLMs). Its main goals are to assess the efficacy of LVLMs in six essential capabilities, including visual perception, visual knowledge acquisition, visual reasoning, visual commonsense, object hallucination, and embodied intelligence. To achieve these goals, the LVLM-eHub consists of two main components: a quantitative capability evaluation and an online arena platform. The quantitative capability evaluation extensively evaluates 16 tasks spanning over 40+ text-related visual datasets to assess the six essential capabilities of LVLM models. The online arena platform features anonymous randomized pairwise battles in a crowd-platformd manner, providing a user-level model ranking in the open-world question-answering scenario. The LVLM-eHub is designed to be user-friendly and inclusive, allowing users to contribute their own datasets and models to the platform. It also offers free online inference services for the LVLM models supported by LVLM Arena. The platform incorporates valuable human feedback through an Elo rating rank system, ensuring continuous improvement and refinement of LVLM models.''',\n",
    "    'L-Eval': '''The L-Eval benchmark proposed aims to provide a standardized evaluation for long context language models (LCLMs). It addresses two key aspects: dataset construction and evaluation metrics. The dataset construction involves annotation, re-annotation, and manual filtering pipeline, resulting in 508 long documents and over 2,000 human-labeled query-response pairs encompassing diverse question styles, domains, and input length (3k∼200k tokens). The evaluation suite contains 20 sub-tasks, which are more closed-ended tasks reflecting unbiased results due to fairness issues in open-ended tasks. The evaluation metrics are another key aspect of L-Eval. The authors investigate the effectiveness of popular n-gram matching metrics in correlating with human judgment and find that they generally cannot do so well. Therefore, they propose length-instruction-enhanced (LIE) evaluation and employing LLM judges as alternative evaluation metrics. The L-Eval benchmark aims to test and evaluate the performance of LCLMs in practical, long-form documents across domains. It focuses on modeling longer contexts and preserving the reasoning ability in an extended context. The authors believe that L-Eval can serve as a challenging testbed to drive advances in modeling longer contexts and pro''',\n",
    "    'MOCHA': '''MOCHA is a benchmark dataset for training and evaluating generative reading comprehension metrics. Its main goal is to address the limitations of existing generation metrics, which rely on token overlap and are agnostic to the nuances of reading comprehension. MOCHA aims to develop learned metrics that model the correctness of candidates using human annotation scores. MOCHA contains 40K human judgement scores on model outputs from six diverse question answering datasets and an additional set of minimal pairs for evaluation. The candidates come from a wide range of RC phenomena, such as commonsense reasoning and understanding narrative over movie scripts. MOCHA presents a challenging problem for developing accurate and robust generative reading comprehension metrics. To evaluate machine learning models, MOCHA uses a Learned Evaluation metric for Reading Comprehension (LERC) that mimics human judgement scores. LERC outperforms baseline metrics by 10 to 36 absolute Pearson points on held-out annotations. When evaluated on minimal pairs, LERC achieves 80% accuracy, outperforming baselines by 14 to 26 absolute percentage points while leaving significant room for improvement. In summary, MOCHA aims to improve generative reading comprehension metrics by developing learned metrics that model the correctness of candidates using human annotation scores. It contains a diverse set of RC phenomena and uses LERC to evaluate machine learning models.''',\n",
    "    'MVBench': '''MVBench is a comprehensive benchmark for multi-modal video understanding that aims to evaluate the performance of Multi-modal Large Language Models (MLLMs) in a range of temporal tasks. The main goals of MVBench are to provide a standardized evaluation framework for MLLMs, to cover a wide range of temporal tasks, and to emphasize temporally-sensitive videos and efficient use of public annotations. MVBench covers 20 dynamic video tasks that require a range of temporal skills, including action recognition, temporal localization, and temporal reasoning. These tasks are designed using a concise static-to-dynamic method by adapting static tasks with dynamic goals. MVBench also includes an automatic annotation paradigm that generates multi-choice QAs for each task, by converting 11 public video benchmarks with LLMs. This approach largely reduces the cost of expensive human annotations and provides ground truth for MVBench, which guarantees evaluation fairness and accuracy, avoiding biased scoring of LLMs. The defining features of MVBench include its coverage of a wide range of temporal tasks, its emphasis on temporally-sensitive videos, and its use of public annotations to reduce the cost of human annotations. MVBench also conducts comprehensive evaluations of MLLMs' temporal understanding, providing a standardized evaluation framework for these models. MVBench aims to test and evaluate the specific aspects of machine learning models related to multi-modal video understanding, including their ability to comprehend complex domains and diverse scenes, ranging from first-person to third-person perspectives, and from indoor to outdoor environments. MVBench also evaluates the general capability of MLLMs for open-world temporal understanding, providing a comprehensive analysis of their performance in a range of temporal tasks.''',\n",
    "    'Open Ko-LLM Leaderboard': '''The Open Ko-LLM Leaderboard is emphasizes high-quality data that reflects the characteristics and culture of the Korean language, thereby addressing the unique challenges and nuances of processing and understanding Korean text​. Defining Features Rapid Expansion and Participation: The leaderboard witnessed rapid growth, registering over 100 models within two weeks of its launch. This participation includes various companies, academic organizations, and individual researchers, indicating its wide-reaching impact and acceptance​​. Diverse Model Collection: It hosts a variety of famous Korean open-platform models like 'Ko-Alpaca', 'KULLM (Cloud)', and 'Polyglot-Ko', showcasing a comprehensive range of Korean LLM performances within the industry​​. Continuous Development and Engagement: Future plans include monthly leaderboard rankings, open seminars, and collaborative efforts to further develop and refine the Korean LLM ecosystem​​.''',\n",
    "    'Open LLM Leaderboard': '''The Open LLM Leaderboard is designed to track, rank, and evaluate open Large Language Models (LLMs) and chatbots. Defining Features Benchmarks: The leaderboard evaluates models on seven key benchmarks using the Eleuther AI Language Model Evaluation Harness. This framework tests generative language models on a variety of evaluation tasks. Model Categories: It categorizes models into different types, such as pretrained, fine-tuned, instruction-tuned, and RL-tuned models. Quantization: The leaderboard includes information about model quantization, like 8-bit and 4-bit quantizations, which are important for understanding model efficiency and deployment capabilities. Community Engagement: Models can be flagged by the community for issues, ensuring the integrity and reliability of the leaderboard rankings. Additional Features Reproducibility: The leaderboard provides commands and methods to reproduce the results using the Eleuther AI Harness. Model Submission Guidelines: Guidelines for submitting models to the leaderboard, ensuring standardization and quality control. Detailed Results and Logs: Access to detailed numerical results and model input/output details for deeper analysis. Overall, the Open LLM Leaderboard is a comprehensive platform for evaluating and comparing large language models and chatbots. It emphasizes a variety of reasoning, knowledge, and inference tasks, providing a holistic view of model capabilities.''',\n",
    "    'Open Multilingual LLM Evaluation Leaderboard': '''The Open Multilingual LLM Evaluation Leaderboard at HuggingFace is a platform that tracks and ranks the performance of large language models (LLMs) developed for different languages. Defining Features Multilingual Focus: The current version of the leaderboard includes evaluation data for 29 languages, including Arabic, Chinese, French, German, Hindi, Russian, Spanish, and others. This multilingual aspect is significant as it expands the scope of LLMs beyond predominantly English-centric models. Open Participation: The leaderboard is open to both multilingual and language-specific LLMs. It encourages participation from a wide range of models, promoting diversity in AI development.''',\n",
    "    'Large Language Model Leaderboard': '''The OpenCompass Large Language Model Leaderboard is an open-platform, efficient, and comprehensive evaluation suite and platform designed for large models. Defining Features: Multi-Type Model Support: OpenCompass can evaluate various model types, including HF models, API models, and custom open-platform models. Comprehensive Capability Dimensions: The evaluation covers thorough dimensions, each backed by abundant datasets. Flexible Expansion: The platform supports the flexible and convenient addition of evaluation datasets and models. Diverse Evaluation Methods: As mentioned, it includes zero-shot, few-shot, and chain-of-thought evaluations. Defining Features: Multi-Type Model Support: OpenCompass can evaluate various model types, including HF models, API models, and custom open-platform models. Comprehensive Capability Dimensions: The evaluation covers thorough dimensions, each backed by abundant datasets. Flexible Expansion: The platform supports the flexible and convenient addition of evaluation datasets and models. Diverse Evaluation Methods: It includes zero-shot, few-shot, and chain-of-thought evaluations. Specific Aspects of Machine Learning Models Tested: Large Language Model Evaluation: Evaluating capabilities based on five major dimensions: language, knowledge, reasoning, discipline, and comprehension. This is done using more than 50 datasets. Base Models and Chat Models: The leaderboard includes a variety of models, such as InternLM, LLaMA, ChatGPT, GPT-4, and many others, evaluated across different dimensions. Examination Datasets: The evaluation uses datasets like Middle School Exam, High School Exam, College Exam, and others for comprehensive testing. Capability Dimensions: These include language (word definition, idiom learning, translation), knowledge (question answering, commonsense QA), understanding (reading comprehension, content analysis), and reasoning (NLI, common sense, mathematics). Visual Language Model Benchmark: OpenCompass also conducts a progressive assessment from perception to reasoning, covering 20 finely-grained capabilities with approximately 3000 multiple-choice questions.''',\n",
    "    'OpenEval (Text)': '''OpenEval is a comprehensive, multidimensional, and open evaluation platform dedicated to assessing Chinese large-scale models. Specific Aspects of Machine Learning Models Tested and Evaluated: Knowledge Assessment: Evaluating the depth and accuracy of the knowledge encapsulated in large models. Capability Evaluation: Assessing the functional capabilities of models, such as understanding, reasoning, and language generation. Alignment and Safety Levels: Testing how well these models align with ethical, cultural, and safety standards, which is crucial in preventing biases and ensuring responsible AI deployment. Detailed Evaluation Reports: Providing comprehensive reports that detail the performance and characteristics of evaluated models, which can be crucial for developers and researchers.''',\n",
    "    'PromptBench': '''PromptBench is a robustness benchmark designed to evaluate the resilience of Large Language Models (LLMs) to adversarial prompts. The main goals of PromptBench include measuring the robustness of LLMs to adversarial prompts, understanding the vulnerabilities of LLMs to such prompts, and providing practical guidance for crafting more robust prompts for downstream users and prompt engineers. Key features of PromptBench include: 1. Adversarial Textual Attacks: PromptBench employs a variety of adversarial textual attacks targeting prompts at multiple levels, including character, word, sentence, and semantic levels. These attacks aim to evaluate how slight deviations in prompts can affect LLM outcomes while maintaining semantic integrity 2. Diverse Tasks and Datasets: PromptBench consists of 8 diverse tasks with 13 public datasets, covering areas such as sentiment analysis, grammar correctness, duplicate sentence detection, natural language inference, multi-task knowledge, reading comprehension, and translation. New datasets can be easily integrated into PromptBench, enhancing its flexibility and applicability 3. Comprehensive Analysis: PromptBench conducts comprehensive evaluations on the robustness of LLMs to adversarial prompts, including visual explanations for observed vulnerabilities, transferability analysis of adversarial prompts, and word frequency analysis to offer practical guidance for prompt composition 4. Flexibility and Extensibility: PromptBench is designed to be a flexible evaluation framework for LLMs, tailored for adversarial robustness, and can be extended to other evaluation research in LLMs Specific aspects of machine learning models that PromptBench aims to test and evaluate include: 1. Resilience to Adversarial Prompts: PromptBench evaluates the ability of LLMs to maintain performance in various tasks when presented with adversarial prompts, such as typos or synonyms, which mimic plausible user errors 2. Transferability Analysis: PromptBench assesses whether adversarial prompts from one LLM can be transferred to another, providing insights into the generalizability of prompt robustness across different models 3. Word Frequency Analysis: PromptBench supports word frequency analysis to offer practical guidance to both LLM developers and end-users in writing robust prompts based on experimental findings''',\n",
    "    'ProtoQA': '''The ProtoQA benchmark is a question answering dataset designed to test and evaluate the common sense reasoning capabilities of machine learning models. The paper introduces a large-scale QA dataset of 9.7k questions regarding common sense knowledge of prototypical situations with 7-8 labeled answer categories per question, and a corresponding evaluation set of 15,400 crowd-platformd human judgments over 154 unseen questions. ProtoQA differs from other datasets in three different ways : 1. ProtoQA focuses on prototypical situations. Humans can agree about the details and characteristics of a prototypical event or situation due to commonalities in their shared lived experiences, cultural norms, and expectations. This rough agreement extends beyond an agreement on a single top response, and that’s why the task and evaluation values diversity of answers. 2. The evaluation ProtoQA is a generative evaluation task where a model has to output a ranked list of answers, ideally covering all prototypical answers for a question. 3. ProtoQA has a large number of annotations for each example which makes the generation evaluation possible. In summary, ProtoQA aims to test and evaluate the common sense reasoning capabilities of machine learning models by presenting them with questions regarding prototypical situations and evaluating their ability to output a ranked list of diverse, prototypical answers.''',\n",
    "    'ReForm-Eval': '''The ReForm-Eval benchmark is a comprehensive evaluation tool for large vision-language models (LVLMs) that aims to assess their performance across multiple capability dimensions. The benchmark is designed to address the challenges of evaluating LVLMs, which often require task-oriented input-output formats that are difficult to automatically assess. To overcome these challenges, the ReForm-Eval benchmark re-formulates existing benchmarks into unified LVLM-compatible formats, offering substantial data for evaluating various capabilities of LVLMs. The benchmark covers 8 capability dimensions, including visual grounding, commonsense reasoning, and visual question answering, among others. One of the defining features of ReForm-Eval is its use of dependable automated evaluation methods based on the unified formats, ensuring an impartial assessment of different LVLMs. This helps to reduce the potential for evaluation bias that can arise from limited manual annotation.''',\n",
    "    'SuperCLUE': '''The SuperCLUE benchmark aims to assess the performance of large language models (LLMs) in real-world scenarios by measuring user preference through open-ended and closed-ended questions. Its main goals are to provide a comprehensive Chinese benchmark for LLMs, to fill the gap in existing benchmarks that mainly focus on measuring models' accuracy using multi-choice questions, and to help developers gather quick and accurate information about users' preferences for their models in a Chinese context before putting them into applications. SuperCLUE encompasses three sub-tasks: actual users' queries and ratings derived from an LLM battle platform (CArena), open-ended questions with single and multiple-turn dialogues (OPEN), and closed-ended questions with the same stems as open-ended single-turn ones (CLOSE). The benchmark is designed to analyze whether the format of multi-choice questions is a suitable alternative to an open one in evaluating model performance. The study shows that accuracy on closed-ended questions is insufficient to reflect human preferences achieved on open-ended ones. At the same time, they can complement each other to predict actual user preferences. The specific aspects of machine learning models that SuperCLUE aims to test and evaluate include their capabilities in handling diverse problems in general and specialized domains, their ability to understand and generate natural language, and their performance in real-life scenarios. The benchmark also aims to evaluate the reliability of GPT-4 as a judge to automatically evaluate human preferences on open-ended questions in a Chinese context.''',\n",
    "    'SuperGLUE': '''SuperGLUE aims to provide a more rigorous test of language understanding than the previous GLUE benchmark. It aims to be a simple, hard-to-game measure of progress towards general-purpose language understanding technologies for English. SuperGLUE retains the basic design of GLUE, consisting of a public leaderboard built around eight language understanding tasks, drawing on existing data, accompanied by a single-number performance metric, and an analysis toolkit. However, it improves upon GLUE in several ways, including more challenging tasks, a larger and more diverse set of training data, and a more comprehensive analysis toolkit. SuperGLUE's defining features include a set of eight challenging language understanding tasks, a larger and more diverse set of training data, and a more comprehensive analysis toolkit. The tasks were selected based on difficulty for current NLP approaches and include tasks such as answering questions based on a given passage, recognizing textual entailment, and understanding natural language inference. The training data is drawn from a variety of Platform, including Wikipedia, news articles, and books, and is designed to be more diverse and representative of real-world language use. The analysis toolkit includes a range of tools for evaluating model performance, including error analysis, diagnostic sets, and adversarial evaluation. SuperGLUE aims to test and evaluate several specific aspects of machine learning models, including sample-efficient learning, transfer learning, multitask learning, and unsupervised or self-supervised learning. These are all key areas of research in machine learning and are critical for developing more robust and generalizable language understanding systems. By providing a more challenging benchmark and a more comprehensive analysis toolkit, SuperGLUE aims to drive further progress in these areas and push the limits of what is possible with current language models.''',\n",
    "    'Video-Bench Leaderboard': '''Video-Bench aims to measure the human-like abilities of Video-LLMs across various capabilities and scenarios. It aims to evaluate the capacity of Video-LLMs to comprehend and summarize information from video itself, encompassing objects, actions, attributes, and their temporal connections. Video-Bench also evaluates the capability of models in understanding TV dramas, appreciating music videos, providing information about players and games in NBA videos, 3D indoor scene understanding, and auto-driving decision-making to assess the comprehension and decision-making abilities of models. Video-Bench is specifically designed to measure the performance of Video-LLMs in three distinct levels of capability: Video-exclusive Understanding, Prior Knowledge-based Question-Answering, and Comprehension and Decision-making. The tasks in Video-exclusive Understanding require no external prior knowledge or complex logic inference and include video summarization, abnormal detection, and crowd counting. The Prior Knowledge-based Question-Answering tasks evaluate the capability of models in understanding TV dramas, appreciating music videos, and providing information about players and games in NBA videos. The Comprehension and Decision-making tasks employ two classical tasks: 3D indoor scene understanding and auto-driving decision-making to assess the comprehension and decision-making abilities of models. To streamline the evaluation process, Video-Bench includes an evaluation toolkit that automatically maps the long text outputs of Video-LLMs to corresponding answers with probability selection or LLM-based semantic understanding. The toolkit calculates accuracy for each question and generates a final score, enhancing the efficiency of the evaluation workflow. Overall, Video-Bench provides a comprehensive evaluation benchmark for Video-LLMs, featuring a three-level ability assessment that systematically evaluates models in video-exclusive understanding, prior knowledge incorporation, and video-based decision-making abilities.''',\n",
    "}\n",
    "\n",
    "def convert_csv_to_dict(file_path):\n",
    "    \"\"\"\n",
    "    Convert a CSV file of leaderboards and metrics to a dictionary.\n",
    "\n",
    "    :param file_path: Path to the CSV file\n",
    "    :return: Dictionary with leaderboards as keys and list of metrics as values\n",
    "    \"\"\"\n",
    "    leaderboard_data = {}\n",
    "\n",
    "    # Read the CSV file\n",
    "    with open(file_path, mode='r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        next(csv_reader)  # Skip the header row\n",
    "\n",
    "        # Iterate through each row\n",
    "        for row in csv_reader:\n",
    "            if row:  # Check if row is not empty\n",
    "                key = row[0]  # The first element is the key\n",
    "                # List comprehension to get non-empty values\n",
    "                values = [value for value in row[1:] if value]\n",
    "                leaderboard_data[key] = values\n",
    "\n",
    "    return leaderboard_data\n",
    "\n",
    "def expand_and_flatten(lst):\n",
    "    expanded_list = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, tuple):\n",
    "            expanded_list.extend(item)\n",
    "        else:\n",
    "            expanded_list.append(item)\n",
    "    return expanded_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_rq1 / 'Leaderboard Pattern.csv')\n",
    "df.fillna('', inplace=True)\n",
    "df.to_latex(\n",
    "    path_rq1 / 'Leaderboard Pattern.tex',\n",
    "    index=False,\n",
    "    caption='Leaderboard Patterns and Their Characteristics',\n",
    "    label='tab:pattern'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format each value based on its own decimal places\n",
    "def format_individual_value(x):\n",
    "    if isinstance(x, float):\n",
    "        # Use string formatting to maintain original decimal places\n",
    "        return \"{:f}\".format(x).rstrip('0').rstrip('.')\n",
    "    return x\n",
    "\n",
    "df = pd.read_csv(path_rq1 / 'Platform Ranking.csv')\n",
    "formatted_df = df.map(format_individual_value)\n",
    "formatted_df.to_latex(\n",
    "    path_rq1 / 'Platform Ranking.tex',\n",
    "    index=False,\n",
    "    escape=True,\n",
    "    caption='Leaderboard Platform and Their Rankings',\n",
    "    label='tab:platform'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_rq1 / 'Leaderboard.csv')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    folder_path = os.path.join(path_data, row['Leaderboard'])\n",
    "    os.makedirs(folder_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = path_data / \"CanAiCode Leaderboard\"\n",
    "channel = ''\n",
    "\n",
    "def file_rename(name, channel):\n",
    "    name = name.lower()\n",
    "    name = name.replace(' - ', '-')\n",
    "    name = name.replace(' ', '_')\n",
    "    name = name.replace(\".csv\", \".json\")\n",
    "    if channel:\n",
    "        name = f'{channel}-{name}'\n",
    "    return name\n",
    "\n",
    "for file in glob.glob(str(directory / \"*.csv\")):\n",
    "    df = pd.read_csv(file)\n",
    "    filename = file.split('/')[-1]\n",
    "    filename = file_rename(filename, channel)\n",
    "    df.to_json(directory / filename, orient=\"records\", indent=4)\n",
    "    os.remove(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_empty_folders(root_dir):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        # Check if both lists of subdirectories and filenames are empty\n",
    "        if not dirnames and not filenames:\n",
    "            print(f\"Empty folder: {dirpath}\")\n",
    "\n",
    "# Replace with your root directory path\n",
    "print_empty_folders(str(path_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model(model_str):\n",
    "    if ('*' in model_str) and ('*' == model_str[-1]):\n",
    "        model_str = model_str[:-1]\n",
    "        \n",
    "    if '](' in model_str:\n",
    "        model_str = model_str.split('](')[0][1:]\n",
    "        \n",
    "    # Substrings to be removed\n",
    "    substrings_to_remove = ['🥇', '🥈', '🥉', '🗒️']\n",
    "    \n",
    "    # Remove each unwanted substring\n",
    "    for substring in substrings_to_remove:\n",
    "        model_str = model_str.replace(substring, '')\n",
    "    \n",
    "    # Strip whitespace\n",
    "    return model_str.strip()\n",
    "\n",
    "def process_json_files(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pd.read_json(file_path)\n",
    "                if 'Model' not in df.columns:\n",
    "                    if file.startswith('ai2'):\n",
    "                        df.rename(columns={'Submission': 'Model'}, inplace=True)\n",
    "                    else:\n",
    "                        print(file_path)\n",
    "                df = df.dropna(subset=['Model'])\n",
    "                df['Model'] = df['Model'].apply(process_model)\n",
    "                df.to_json(file_path, orient='records', indent=4)\n",
    "\n",
    "process_json_files(str(path_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_string(publishers):\n",
    "    if pd.isna(publishers):\n",
    "        return []\n",
    "    \n",
    "    publishers_processed = []\n",
    "    for publisher in publishers.split(','):\n",
    "        Platform_processed = []\n",
    "        for platform in publisher.split('-'):\n",
    "            if platform in platform_mapping:\n",
    "                Platform_processed.append(platform_mapping[platform])\n",
    "            else:\n",
    "                Platform_processed.append(platform)\n",
    "        publishers_processed.append(tuple(Platform_processed))\n",
    "            \n",
    "    return publishers_processed\n",
    "\n",
    "df = pd.read_csv(path_rq1 / 'Leaderboard.csv')\n",
    "df['Modality'] = df['Modality'].apply(parse_string)\n",
    "df['Modality'] = df['Modality'].apply(expand_and_flatten)\n",
    "df['Modality'] = df['Modality'].apply(lambda x: [y.capitalize() for y in x])\n",
    "df['Language'] = df['Language'].apply(parse_string)\n",
    "df['Language'] = df['Language'].apply(expand_and_flatten)\n",
    "df['Platform'] = df['Platform'].apply(parse_string)\n",
    "df['Platform (non-template) with model provenance links'] = df['Platform (non-template) with model provenance links'].apply(parse_string)\n",
    "df['Platform (non-template) with model provenance links'] = df['Platform (non-template) with model provenance links'].apply(lambda x: [y[0].capitalize() for y in x])\n",
    "df['Leaderboard splitting criteria for all Platform (non-pwc)'] = df['Leaderboard splitting criteria for all Platform (non-pwc)'].apply(parse_string)\n",
    "df['Leaderboard splitting criteria for all Platform (non-pwc)'] = df['Leaderboard splitting criteria for all Platform (non-pwc)'].apply(lambda x: [y[0].capitalize() for y in x])\n",
    "df.to_csv(path_rq1 / 'Leaderboard_processed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of evaluation records: 50403\n",
      "Total number of leaderboards (benchmark): 246\n",
      "Total number of leaderboards (publisher): 271\n",
      "Total number of leaderboards (scenario): 2185\n",
      "Total number of leaderboards (publisher) (template): 149\n",
      "Total number of leaderboards (scenario) (template): 537\n",
      "Total number of leaderboards (publisher) (non-template): 122\n",
      "Total number of leaderboards (scenario) (non-template): 1648\n"
     ]
    }
   ],
   "source": [
    "total_records = 0\n",
    "total_publishers = 0\n",
    "total_scenarios = 0\n",
    "total_benchmarks = 0\n",
    "total_template_scenarios = 0\n",
    "total_template_publishers = 0\n",
    "total_nontemplate_scenarios = 0\n",
    "total_nontemplate_publishers = 0\n",
    "    \n",
    "def list_directories(folder_path):\n",
    "    # List all subdirectories within the folder\n",
    "    directories = [os.path.join(folder_path, d) for d in os.listdir(\n",
    "        folder_path) if os.path.isdir(os.path.join(folder_path, d))]\n",
    "    return directories\n",
    "\n",
    "for folder in list_directories(path_data):\n",
    "    total_benchmarks += 1\n",
    "    prefixes = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pd.read_json(file_path)\n",
    "                total_records += len(df)\n",
    "                total_scenarios += 1\n",
    "                for prefix in platform_mapping.keys():\n",
    "                    if file.startswith(prefix):\n",
    "                        prefixes.append(prefix)\n",
    "                        break\n",
    "                    \n",
    "    total_publishers += len(set(prefixes))\n",
    "    template_publishers = [p for p in prefixes if p not in platform_nontemplate]\n",
    "    nontemplate_publishers = [p for p in prefixes if p in platform_nontemplate]\n",
    "    total_template_publishers += len(set(template_publishers))\n",
    "    total_nontemplate_publishers += len(set(nontemplate_publishers))\n",
    "    total_template_scenarios += len(template_publishers)\n",
    "    total_nontemplate_scenarios += len(nontemplate_publishers)\n",
    "\n",
    "print(f\"Total number of evaluation records: {total_records}\")\n",
    "print(f\"Total number of leaderboards (benchmark): {total_benchmarks}\")\n",
    "print(f\"Total number of leaderboards (publisher): {total_publishers}\")\n",
    "print(f\"Total number of leaderboards (scenario): {total_scenarios}\")\n",
    "print(f\"Total number of leaderboards (publisher) (template): {total_template_publishers}\")\n",
    "print(f\"Total number of leaderboards (scenario) (template): {total_template_scenarios}\")\n",
    "print(f\"Total number of leaderboards (publisher) (non-template): {total_nontemplate_publishers}\")\n",
    "print(f\"Total number of leaderboards (scenario) (non-template): {total_nontemplate_scenarios}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          "17197",
          "7072",
          "2339",
          "2210",
          "1926",
          "1542",
          "1185",
          "1121",
          "1050",
          "919"
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          17197,
          7072,
          2339,
          2210,
          1926,
          1542,
          1185,
          1121,
          1050,
          919
         ],
         "y": [
          "HELM Classic",
          "LMExamQA",
          "Open LLM Leaderboard",
          "HEIM",
          "Large Language Model Leaderboard",
          "MTEB",
          "ImageNet",
          "MMLU-by-task Leaderboard",
          "HELM Lite",
          "CanAiCode Leaderboard"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Number of Evaluation Records in Leaderboards (Top 10)"
        },
        "xaxis": {
         "title": {
          "text": "Number of Evaluation Records"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Leaderboard Name"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Text",
         "type": "box",
         "y": [
          120,
          315,
          59,
          5,
          27,
          219,
          25,
          4,
          13,
          34,
          124,
          62,
          28,
          12,
          108,
          5,
          44,
          2,
          16,
          107,
          41,
          312,
          6,
          44,
          44,
          14,
          119,
          11,
          919,
          13,
          30,
          29,
          4,
          23,
          61,
          24,
          7,
          239,
          16,
          190,
          57,
          10,
          56,
          56,
          78,
          9,
          37,
          17,
          10,
          22,
          9,
          86,
          4,
          290,
          13,
          50,
          100,
          22,
          20,
          111,
          5,
          12,
          3,
          16,
          34,
          67,
          59,
          23,
          202,
          3,
          10,
          19,
          26,
          13,
          86,
          14,
          24,
          19,
          2210,
          86,
          22,
          17197,
          1050,
          103,
          68,
          1185,
          55,
          33,
          16,
          36,
          94,
          35,
          4,
          10,
          15,
          320,
          10,
          1,
          53,
          26,
          34,
          1926,
          102,
          7,
          8,
          52,
          767,
          40,
          54,
          38,
          7072,
          2,
          64,
          65,
          26,
          59,
          50,
          2,
          28,
          14,
          6,
          23,
          30,
          19,
          26,
          30,
          6,
          36,
          176,
          21,
          608,
          117,
          1121,
          253,
          49,
          4,
          2,
          117,
          149,
          47,
          37,
          46,
          1542,
          22,
          21,
          15,
          11,
          30,
          26,
          47,
          24,
          33,
          23,
          764,
          2339,
          62,
          83,
          10,
          36,
          16,
          74,
          8,
          13,
          189,
          47,
          105,
          17,
          2,
          21,
          14,
          25,
          90,
          30,
          27,
          16,
          21,
          56,
          20,
          49,
          128,
          116,
          9,
          12,
          72,
          96,
          23,
          127,
          188,
          51,
          53,
          85,
          19,
          11,
          18,
          202,
          226,
          20,
          170,
          85,
          60,
          68,
          9,
          16,
          59,
          18,
          32,
          14,
          50,
          20,
          5,
          21,
          6,
          44,
          7,
          6,
          41,
          30,
          3,
          9,
          2,
          213,
          33,
          26,
          10,
          70,
          19,
          16,
          15,
          7,
          19,
          115,
          1,
          23,
          2,
          4,
          83,
          38,
          96,
          97,
          105,
          16,
          12,
          18,
          32,
          18,
          27
         ]
        },
        {
         "name": "Image",
         "type": "box",
         "y": [
          315,
          219,
          4,
          16,
          312,
          30,
          23,
          24,
          239,
          100,
          67,
          23,
          13,
          19,
          2210,
          1185,
          55,
          16,
          36,
          7,
          65,
          36,
          176,
          608,
          253,
          117,
          22,
          26,
          33,
          23,
          16,
          105,
          27,
          16,
          116,
          9,
          72,
          96,
          7,
          26,
          19,
          16,
          15,
          115,
          23,
          97
         ]
        },
        {
         "name": "Video",
         "type": "box",
         "y": [
          59,
          5,
          27,
          44,
          4,
          50,
          3,
          103,
          4,
          320,
          30,
          149,
          47,
          37,
          46,
          15,
          30,
          72,
          188,
          19,
          20,
          9,
          213,
          10,
          70,
          7,
          1,
          27
         ]
        },
        {
         "name": "Audio",
         "type": "box",
         "y": [
          13,
          67,
          59,
          6
         ]
        },
        {
         "name": "Code",
         "type": "box",
         "y": [
          12,
          41,
          312,
          44,
          919,
          190,
          9,
          10,
          20,
          111,
          5,
          67,
          17197,
          68,
          33,
          35,
          1926,
          8,
          52,
          64,
          6,
          24,
          10,
          8,
          51,
          14
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Number of Evaluation Records of Benchmark-based Leaderboards by Task Modality"
        },
        "xaxis": {
         "title": {
          "text": "Task Modality"
         }
        },
        "yaxis": {
         "title": {
          "text": "Number of Evaluation Records"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "AI2",
         "type": "box",
         "y": [
          120,
          86,
          26,
          4,
          11,
          189,
          17,
          96
         ]
        },
        {
         "name": "PapersWithCode",
         "type": "box",
         "y": [
          315,
          59,
          5,
          27,
          219,
          4,
          13,
          28,
          12,
          108,
          5,
          44,
          2,
          16,
          107,
          312,
          6,
          44,
          13,
          4,
          23,
          239,
          16,
          10,
          78,
          37,
          10,
          22,
          9,
          4,
          290,
          50,
          22,
          16,
          59,
          23,
          202,
          3,
          13,
          86,
          19,
          86,
          103,
          68,
          1185,
          55,
          16,
          4,
          320,
          1,
          34,
          7,
          2,
          59,
          2,
          14,
          6,
          23,
          19,
          30,
          6,
          36,
          117,
          49,
          2,
          117,
          149,
          47,
          37,
          46,
          21,
          30,
          26,
          47,
          33,
          23,
          83,
          16,
          74,
          47,
          2,
          21,
          25,
          90,
          30,
          27,
          21,
          56,
          116,
          23,
          127,
          188,
          51,
          53,
          85,
          19,
          11,
          18,
          202,
          226,
          32,
          20,
          5,
          21,
          6,
          41,
          30,
          9,
          2,
          213,
          26,
          70,
          19,
          15,
          19,
          115,
          1,
          23,
          2,
          4,
          83,
          38,
          96,
          97,
          105,
          16,
          12,
          18,
          18,
          27
         ]
        },
        {
         "name": "Self-hosted website",
         "type": "box",
         "y": [
          25,
          34,
          119,
          30,
          29,
          61,
          190,
          56,
          9,
          100,
          67,
          2210,
          22,
          17197,
          1050,
          36,
          94,
          10,
          1926,
          102,
          8,
          52,
          40,
          54,
          7072,
          65,
          30,
          176,
          1542,
          22,
          36,
          8,
          20,
          49,
          12,
          170,
          85,
          60,
          68,
          59,
          18,
          14,
          6
         ]
        },
        {
         "name": "GitHub",
         "type": "box",
         "y": [
          124,
          44,
          14,
          11,
          24,
          7,
          57,
          56,
          17,
          13,
          20,
          111,
          5,
          12,
          3,
          34,
          19,
          14,
          24,
          33,
          35,
          10,
          15,
          53,
          38,
          64,
          26,
          50,
          28,
          26,
          21,
          608,
          253,
          15,
          24,
          10,
          13,
          105,
          14,
          16,
          128,
          9,
          20,
          9,
          16,
          50,
          7,
          3,
          33,
          16,
          7,
          32
         ]
        },
        {
         "name": "HuggingFace",
         "type": "box",
         "y": [
          62,
          41,
          919,
          10,
          26,
          767,
          1121,
          764,
          2339,
          62,
          72,
          44,
          10
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Number of Evaluation Records of Benchmark-based Leaderboards by Sources"
        },
        "xaxis": {
         "title": {
          "text": "Leaderboard Source"
         }
        },
        "yaxis": {
         "title": {
          "text": "Number of Evaluation Records"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "leaderboard_submission_count = {}\n",
    "for directory in os.listdir(path_data):\n",
    "    subdirectory_path = os.path.join(path_data, directory)\n",
    "    submission = 0\n",
    "    for file in glob.glob(os.path.join(subdirectory_path, \"*.json\")):\n",
    "        df = pd.read_json(file)\n",
    "        submission += len(df)\n",
    "    leaderboard_submission_count[directory] = submission\n",
    "labels, values = zip(*sorted(leaderboard_submission_count.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "fig = go.Figure(go.Bar(\n",
    "    x=values[:10],  # Values for the bar lengths\n",
    "    y=labels[:10],  # Categories for each bar\n",
    "    orientation='h',  # Sets the bars to be horizontal\n",
    "    text=values[:10],  # Adds the values as text on each bar\n",
    "    textposition='auto'  # Automatically positions the text on the bars\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=f'Number of Evaluation Records in Leaderboards (Top 10)',\n",
    "    xaxis_title='Number of Evaluation Records',\n",
    "    yaxis_title='Leaderboard Name',\n",
    "    yaxis_autorange='reversed'  # This line makes the bars go top-down\n",
    ")\n",
    "# fig.write_image(path_rq1 / 'evaluation record distribution (top-10).pdf')\n",
    "fig.show()\n",
    "\n",
    "df = pd.read_csv(path_rq1 / 'Leaderboard_processed.csv')\n",
    "df['Modality'] = df['Modality'].apply(ast.literal_eval)\n",
    "leaderboard_modality_mapping = df.set_index('Leaderboard')['Modality'].to_dict()\n",
    "\n",
    "leaderboard_modality_mapping_inverse = defaultdict(list)\n",
    "for k, v in leaderboard_modality_mapping.items():\n",
    "    for item in v:\n",
    "        leaderboard_modality_mapping_inverse[item].append(k)\n",
    "\n",
    "fig = go.Figure()\n",
    "for k, v in leaderboard_modality_mapping_inverse.items():\n",
    "    submissions = [leaderboard_submission_count[i] for i in v]\n",
    "    fig.add_trace(go.Box(y=submissions, name=k))\n",
    "fig.update_layout(title=\"Distribution of Number of Evaluation Records of Benchmark-based Leaderboards by Task Modality\",\n",
    "                  yaxis=dict(title='Number of Evaluation Records', type='log'),\n",
    "                  xaxis=dict(title='Task Modality'))\n",
    "# fig.write_image(path_rq1 / 'evaluation record distribution by task modality.pdf')\n",
    "fig.show()\n",
    "\n",
    "df = pd.read_csv(path_rq1 / 'Leaderboard_processed.csv')\n",
    "df['Platform'] = df['Platform'].apply(ast.literal_eval)\n",
    "df['Platform'] = df['Platform'].apply(expand_and_flatten)\n",
    "df = df.explode('Platform')\n",
    "leaderboard_platform_mapping = df.set_index('Leaderboard')['Platform'].to_dict()\n",
    "\n",
    "leaderboard_platform_mapping_inverse = defaultdict(list)\n",
    "for k, v in leaderboard_platform_mapping.items():\n",
    "    leaderboard_platform_mapping_inverse[v].append(k)\n",
    "\n",
    "fig = go.Figure()\n",
    "for k, v in leaderboard_platform_mapping_inverse.items():\n",
    "    submissions = [leaderboard_submission_count[i] for i in v]\n",
    "    fig.add_trace(go.Box(y=submissions, name=k))\n",
    "fig.update_layout(title=\"Distribution of Number of Evaluation Records of Benchmark-based Leaderboards by Platform\",\n",
    "                  yaxis=dict(title='Number of Evaluation Records', type='log'),\n",
    "                  xaxis=dict(title='Leaderboard Platform'))\n",
    "# fig.write_image(path_rq1 / 'evaluation record distribution by leaderboard platform.pdf')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gradio_client import Client\n",
    "# import json\n",
    "# client = Client(\"https://felixz-open-llm-leaderboard.hf.space\")\n",
    "\n",
    "# json_data = client.predict(\"\",\"\", api_name='/predict')\n",
    "\n",
    "# with open(json_data, 'r') as file:\n",
    "#     file_data = file.read()\n",
    "#     data = json.loads(file_data)\n",
    "#     df = pd.DataFrame(data['data'], columns=data['headers'])\n",
    "#     df.to_json(path_data / 'Open LLM Leaderboard/hf.json', orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Text-to-Text Generation', 'Text-to-Image Generation', 'Text-to-Code Generation', 'Text-to-Audio Generation', 'Text-to-Video Generation', 'Image-to-Text Generation', 'Code-to-Text Generation', 'Audio-to-Text Generation', 'Video-to-Text Generation', 'Image-to-Image Generation', 'Code-to-Code Generation', 'Audio-to-Audio Generation', 'Video-to-Video Generation',\n",
    "# 'Code Summarization', 'Code Review', 'Identifier Prediction', 'Defect Detection', 'Clone Detection', 'Code Classification', 'Code Reasoning', 'Document Translation', 'Log Parsing',\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values with hyperlinks written to /Users/jimmy/Downloads/b.xlsx\n"
     ]
    }
   ],
   "source": [
    "# from openpyxl import load_workbook, Workbook\n",
    "\n",
    "# # Path to your Excel file and output file\n",
    "# input_file = '/Users/jimmy/Downloads/a.xlsx'\n",
    "# output_file = '/Users/jimmy/Downloads/b.xlsx'\n",
    "\n",
    "# # Load the workbook and select the active worksheet\n",
    "# wb = load_workbook(input_file)\n",
    "# ws = wb.active\n",
    "\n",
    "# # Dictionary to store unique values and their hyperlinks\n",
    "# unique_values = {}\n",
    "\n",
    "# # Loop through all cells in the worksheet\n",
    "# for row in ws.iter_rows():\n",
    "#     for cell in row:\n",
    "#         # Check if cell has a value\n",
    "#         if cell.value:\n",
    "#             # Store value and hyperlink (if any)\n",
    "#             value_filtered = cell.value.split(' (')[0]\n",
    "#             unique_values[value_filtered] = cell.hyperlink.target if cell.hyperlink else None\n",
    "\n",
    "# # Sort the unique values\n",
    "# sorted_unique_values = sorted(unique_values.items())\n",
    "\n",
    "# # Create a new workbook and select the active worksheet\n",
    "# new_wb = Workbook()\n",
    "# new_ws = new_wb.active\n",
    "\n",
    "# # Write unique values and hyperlinks to the new worksheet\n",
    "# for i, (text, hyperlink) in enumerate(sorted_unique_values, start=1):\n",
    "#     new_ws.cell(row=i, column=1, value=text)\n",
    "#     if hyperlink:\n",
    "#         new_ws.cell(row=i, column=1).hyperlink = hyperlink\n",
    "\n",
    "# # Save the new workbook\n",
    "# new_wb.save(output_file)\n",
    "\n",
    "# print(f\"Unique values with hyperlinks written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openpyxl\n",
    "\n",
    "# def convert_to_lower_case(file_path):\n",
    "#     # Load the workbook and select the active sheet\n",
    "#     workbook = openpyxl.load_workbook(file_path)\n",
    "#     sheet = workbook.active\n",
    "\n",
    "#     for row in sheet.iter_rows(min_row=1, min_col=2):  # Start from the second column\n",
    "#         for cell in row:\n",
    "#             if cell.value and isinstance(cell.value, str):\n",
    "#                 cell.value = cell.value.lower()\n",
    "\n",
    "#     # Save the workbook\n",
    "#     workbook.save(file_path)\n",
    "\n",
    "# # Specify the path to your Excel file\n",
    "# file_path = '/Users/jimmy/Downloads/Leaderboard Information.xlsx'\n",
    "# convert_to_lower_case(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "# # Load your DataFrame\n",
    "\n",
    "# # Group by 'Platform' and 'Modality', then count occurrences\n",
    "# grouped = df.groupby(['Modality', 'Platform']).size().reset_index(name='counts')\n",
    "\n",
    "# # Pivot the DataFrame to get 'Platform' as index, 'Modality' as columns, and 'counts' as values\n",
    "# pivot_df = grouped.pivot(index='Modality', columns='Platform', values='counts').fillna(0)\n",
    "\n",
    "# # Create the traces for each 'Platform'\n",
    "# traces = []\n",
    "# for Platform in pivot_df.columns:\n",
    "#     traces.append(go.Bar(\n",
    "#         x=pivot_df.index,\n",
    "#         y=pivot_df[Platform],\n",
    "#         name=Platform,\n",
    "#         text=pivot_df[Platform],\n",
    "#         textposition='outside'  # This positions the text on top of the bars\n",
    "#     ))\n",
    "\n",
    "# # Create the figure and add traces\n",
    "# fig = go.Figure(data=traces)\n",
    "\n",
    "# # Update layout for a better look\n",
    "# fig.update_layout(\n",
    "#     barmode='group',\n",
    "#     title='Frequency of Leaderboard by Platform and Modality',\n",
    "#     xaxis_title='Task Modality',\n",
    "#     yaxis_title='Count of Platform-based Leaderboards',\n",
    "# )\n",
    "\n",
    "# # Show the plot\n",
    "# fig.write_image(path_rq1 / 'publisher modality distribution.pdf')\n",
    "\n",
    "# # Show the figure\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Sources=%{x}<br>Count of Name-based Leaderboards=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "textposition": "auto",
         "texttemplate": "%{y}",
         "type": "bar",
         "x": [
          "PapersWithCode",
          "GitHub",
          "Self-hosted website",
          "HuggingFace",
          "AI2"
         ],
         "xaxis": "x",
         "y": [
          134,
          77,
          43,
          21,
          17
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Frequency of Leaderboard across Different Sources"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Sources"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Count of Name-based Leaderboards"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(path_rq1 / 'Leaderboard_processed.csv')\n",
    "df['Platform'] = df['Platform'].apply(ast.literal_eval)\n",
    "df['Platform'] = df['Platform'].apply(expand_and_flatten)\n",
    "\n",
    "# Step 2: Flatten the list and count occurrences\n",
    "platform_counts = df.explode(\n",
    "    'Platform')['Platform'].value_counts()\n",
    "\n",
    "# Step 3: Create a bar chart\n",
    "fig = px.bar(\n",
    "    x=platform_counts.index,\n",
    "    y=platform_counts.values,\n",
    "    text_auto=True,\n",
    "    labels={'x': 'Platform', 'y': 'Count of Name-based Leaderboards'},\n",
    "    title='Frequency of Leaderboard across Different Platform'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# explain in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Github 27\n",
      "Huggingface 14\n",
      "Self-hosted website 10\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(path_rq1 / 'Leaderboard_processed.csv')\n",
    "df['Platform (non-template) with model provenance links'] = df['Platform (non-template) with model provenance links'].apply(ast.literal_eval)\n",
    "df = df.explode('Platform (non-template) with model provenance links')\n",
    "df = df.dropna(subset=['Platform (non-template) with model provenance links'])\n",
    "for name, group in df.groupby('Platform (non-template) with model provenance links'):\n",
    "    print(name, len(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total evaluation records:  6735\n",
      "Total evaluation records with paper:  6240\n",
      "Total evaluation records with code:  5334\n",
      "Total paper rate:  92.65033407572383\n",
      "Total code rate:  85.48076923076923\n",
      "Average paper rate:  98.93578229890227\n",
      "Average code rate:  90.16642486980015\n"
     ]
    }
   ],
   "source": [
    "total_data = 0\n",
    "total_paper = 0\n",
    "total_code = 0\n",
    "\n",
    "average_paper_rate = []\n",
    "average_code_rate = []\n",
    "\n",
    "for root, dirs, files in os.walk(path_data):\n",
    "    for file in files:\n",
    "        if file.startswith('pwc') and file.endswith(\".json\"):\n",
    "            with open(os.path.join(root, file), 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "                total_data += len(json_data)\n",
    "                json_external = [external for external in json_data if type(external['external_platform_url']) == str]\n",
    "                if not len(json_external):\n",
    "                    json_paper = [paper for paper in json_data if type(paper['paper']['id']) == int]\n",
    "                    total_paper += len(json_paper)\n",
    "                    average_paper_rate.append(len(json_paper)/len(json_data)*100)\n",
    "                    json_code = [code for code in json_paper if code['paper']['code'] == True]\n",
    "                    if len(json_code):\n",
    "                        total_code += len(json_code)\n",
    "                        average_code_rate.append(len(json_code)/len(json_paper)*100)\n",
    "\n",
    "print('Total evaluation records: ', total_data)\n",
    "print('Total evaluation records with paper: ', total_paper)\n",
    "print('Total evaluation records with code: ', total_code)\n",
    "print('Total paper rate: ', total_paper/total_data*100)\n",
    "print('Total code rate: ', total_code/total_paper*100)\n",
    "print('Average paper rate: ', sum(average_paper_rate)/len(average_paper_rate))\n",
    "print('Average code rate: ', sum(average_code_rate)/len(average_code_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Modality=%{x}<br>Count of Name-based Leaderboards=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "textposition": "auto",
         "texttemplate": "%{y}",
         "type": "bar",
         "x": [
          "Text",
          "Image",
          "Video",
          "Code",
          "Audio"
         ],
         "xaxis": "x",
         "y": [
          245,
          46,
          28,
          26,
          4
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Frequency of Leaderboard across Different Modalities"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Modality"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Count of Name-based Leaderboards"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(path_rq1 / 'Leaderboard_processed.csv')\n",
    "df['Modality'] = df['Modality'].apply(ast.literal_eval)\n",
    "\n",
    "# Step 2: Flatten the list and count occurrences\n",
    "modality_counts = df.explode('Modality')['Modality'].value_counts()\n",
    "\n",
    "# Step 3: Create a bar chart\n",
    "fig = px.bar(\n",
    "    x=modality_counts.index,\n",
    "    y=modality_counts.values,\n",
    "    text_auto=True,\n",
    "    labels={'x': 'Modality', 'y': 'Count of Name-based Leaderboards'},\n",
    "    title='Frequency of Leaderboard across Different Modalities'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# explain in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorscale": [
          [
           0,
           "rgb(247,252,240)"
          ],
          [
           0.125,
           "rgb(224,243,219)"
          ],
          [
           0.25,
           "rgb(204,235,197)"
          ],
          [
           0.375,
           "rgb(168,221,181)"
          ],
          [
           0.5,
           "rgb(123,204,196)"
          ],
          [
           0.625,
           "rgb(78,179,211)"
          ],
          [
           0.75,
           "rgb(43,140,190)"
          ],
          [
           0.875,
           "rgb(8,104,172)"
          ],
          [
           1,
           "rgb(8,64,129)"
          ]
         ],
         "text": [
          [
           0,
           0,
           0,
           3,
           1
          ],
          [
           0,
           13,
           4,
           7,
           9
          ],
          [
           2,
           11,
           3,
           30,
           8
          ],
          [
           17,
           77,
           21,
           134,
           43
          ],
          [
           0,
           3,
           2,
           24,
           0
          ]
         ],
         "textfont": {
          "size": 10
         },
         "texttemplate": "%{text}",
         "type": "heatmap",
         "x": [
          "AI2",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "Self-hosted website"
         ],
         "y": [
          "Audio",
          "Code",
          "Image",
          "Text",
          "Video"
         ],
         "z": [
          [
           0,
           0,
           0,
           3,
           1
          ],
          [
           0,
           13,
           4,
           7,
           9
          ],
          [
           2,
           11,
           3,
           30,
           8
          ],
          [
           17,
           77,
           21,
           134,
           43
          ],
          [
           0,
           3,
           2,
           24,
           0
          ]
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Frequency of Leaderboard by Source and Modality"
        },
        "xaxis": {
         "title": {
          "text": "Leaderboard Source"
         }
        },
        "yaxis": {
         "title": {
          "text": "Task Modality"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(path_rq1 / 'Leaderboard_processed.csv')\n",
    "df['Platform'] = df['Platform'].apply(ast.literal_eval)\n",
    "df['Platform'] = df['Platform'].apply(expand_and_flatten)\n",
    "df['Modality'] = df['Modality'].apply(ast.literal_eval)\n",
    "df = df.explode('Platform').explode('Modality')\n",
    "\n",
    "df = df[['Platform', 'Modality']].value_counts().reset_index(name='count')\n",
    "# df.head()\n",
    "pivot_table = df.pivot(index='Modality', columns='Platform', values='count').fillna(0)\n",
    "text_labels = pivot_table.values.astype(int)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=pivot_table.values,\n",
    "    x=pivot_table.columns.tolist(),\n",
    "    y=pivot_table.index.tolist(),\n",
    "    colorscale='GnBu',\n",
    "    text=text_labels,\n",
    "    texttemplate=\"%{text}\",\n",
    "    textfont={\"size\":10}\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Frequency of Leaderboard by Platform and Modality',\n",
    "    xaxis_title='Leaderboard Platform',\n",
    "    yaxis_title='Task Modality'\n",
    ")\n",
    "\n",
    "fig.write_image(path_rq1 / 'platform modality distribution.pdf')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Language=%{x}<br>Count of Name-based Leaderboards=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "textposition": "auto",
         "texttemplate": "%{y}",
         "type": "bar",
         "x": [
          "English",
          "Chinese",
          "Russian",
          "German",
          "Arabic",
          "Swedish",
          "Hindi"
         ],
         "xaxis": "x",
         "y": [
          217,
          40,
          5,
          4,
          4,
          4,
          4
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Frequency of Leaderboard across Different Languages"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Language"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Count of Name-based Leaderboards"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(path_rq1 / 'Leaderboard_processed.csv')\n",
    "df['Language'] = df['Language'].apply(ast.literal_eval)\n",
    "\n",
    "# Step 2: Flatten the list and count occurrences\n",
    "language_counts = df.explode(\n",
    "    'Language')['Language'].value_counts()\n",
    "language_counts = language_counts[language_counts > 3]\n",
    "\n",
    "# Step 3: Create a bar chart\n",
    "fig = px.bar(\n",
    "    x=language_counts.index,\n",
    "    y=language_counts.values,\n",
    "    text_auto=True,\n",
    "    labels={'x': 'Language', 'y': 'Count of Name-based Leaderboards'},\n",
    "    title='Frequency of Leaderboard across Different Languages'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# explain in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-scenario leaderboards (non-pwc): 55\n",
      "Multi-scenario leaderboards (pwc): 134\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(path_rq1 / 'Leaderboard_processed.csv')\n",
    "df['Platform'] = df['Platform'].apply(ast.literal_eval)\n",
    "df['Platform'] = df['Platform'].apply(expand_and_flatten)\n",
    "df['Leaderboard splitting criteria for all Platform (non-pwc)'] = df['Leaderboard splitting criteria for all Platform (non-pwc)'].apply(ast.literal_eval)\n",
    "df_neg_pwc = df[df['Leaderboard splitting criteria for all Platform (non-pwc)'].apply(lambda x: x != [])]\n",
    "print(f'Multi-scenario leaderboards (non-pwc): {len(df_neg_pwc)}')\n",
    "df_pwc = df[df['Platform'].apply(lambda x: 'PapersWithCode' in x)]\n",
    "print(f'Multi-scenario leaderboards (pwc): {len(df_pwc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "type": "histogram",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          2,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          2,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          4,
          0
         ]
        }
       ],
       "layout": {
        "bargap": 0.1,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Number of Empty Leaderboard across Multi-scenario Leaderboards"
        },
        "xaxis": {
         "dtick": 20,
         "tickmode": "linear",
         "title": {
          "text": "Number of Empty Leaderboard"
         }
        },
        "yaxis": {
         "title": {
          "text": "Number of Occurrence"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS statistic: 0.12962962962962962\n",
      "p-value: 0.7597078706434061\n",
      "Skewness: 4.0209936082057975\n",
      "Kurtosis: 17.24390499052065\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Audio",
         "type": "box",
         "y": [
          4
         ]
        },
        {
         "name": "Code",
         "type": "box",
         "y": [
          0,
          0,
          2,
          0,
          0,
          1,
          0,
          4
         ]
        },
        {
         "name": "Image",
         "type": "box",
         "y": [
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          4
         ]
        },
        {
         "name": "Text",
         "type": "box",
         "y": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          2,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          2,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          4,
          0
         ]
        },
        {
         "name": "Video",
         "type": "box",
         "y": [
          0,
          0,
          0
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Number of Empty Multi-scenario Leaderboards by Task Modality"
        },
        "xaxis": {
         "title": {
          "text": "Task Modality"
         }
        },
        "yaxis": {
         "title": {
          "text": "Number of Empty Multi-scenario Leaderboards"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(path_rq1 / 'Leaderboard_processed.csv')\n",
    "df = df[(df['#Empty multi-scenario leaderboards (non-pwc)'] != '?') & df['#Empty multi-scenario leaderboards (non-pwc)'].notna()]\n",
    "empty_leaderboards = df['#Empty multi-scenario leaderboards (non-pwc)'].astype(int).tolist()\n",
    "\n",
    "fig = go.Figure(data=[go.Histogram(x=empty_leaderboards)])\n",
    "fig.update_layout(\n",
    "    title=\"Distribution of Number of Empty Leaderboard across Multi-scenario Leaderboards\",\n",
    "    xaxis_title=\"Number of Empty Leaderboard\",\n",
    "    yaxis_title=\"Number of Occurrence\",\n",
    "    bargap=0.1,  # Gap between bars of adjacent location coordinates\n",
    "    xaxis=dict(\n",
    "        tickmode='linear',  # Setting tick mode to linear\n",
    "        # tick0=1,    # Starting tick\n",
    "        dtick=20      # Set every single value on x-axis\n",
    "    )\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(path_rq1 / 'empty leaderboard distribution.pdf')\n",
    "\n",
    "lambda_poisson = np.mean(empty_leaderboards)\n",
    "# Generating a Poisson distribution with the same λ and number of observations\n",
    "poisson_dist = stats.poisson.rvs(mu=lambda_poisson, size=len(empty_leaderboards))\n",
    "# Conducting a Kolmogorov-Smirnov test to compare the data to the theoretical Poisson distribution\n",
    "ks_statistic, p_value = stats.ks_2samp(empty_leaderboards, poisson_dist)\n",
    "\n",
    "print(f'KS statistic: {ks_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "\n",
    "skewness = stats.skew(empty_leaderboards)\n",
    "kurtosis = stats.kurtosis(empty_leaderboards)\n",
    "\n",
    "print('Skewness:', skewness)\n",
    "print('Kurtosis:', kurtosis)\n",
    "\n",
    "df = pd.read_csv(path_rq1 / 'Leaderboard_processed.csv')\n",
    "df.dropna(subset=['#Empty multi-scenario leaderboards (non-pwc)'], inplace=True)\n",
    "df['Modality'] = df['Modality'].apply(ast.literal_eval)\n",
    "df = df.explode('Modality')\n",
    "\n",
    "fig = go.Figure()\n",
    "for name, group in df.groupby('Modality'):\n",
    "    fig.add_trace(go.Box(y=group['#Empty multi-scenario leaderboards (non-pwc)'], name=name))\n",
    "fig.update_layout(title=\"Distribution of Number of Empty Multi-scenario Leaderboards by Task Modality\",\n",
    "                  yaxis=dict(title='Number of Empty Multi-scenario Leaderboards'),\n",
    "                  xaxis=dict(title='Task Modality'))\n",
    "fig.write_image(path_rq1 / 'empty leaderboard distribution by task modality.pdf')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllr}\n",
      "\\toprule\n",
      "Index & Splitting criteria & Major category & Frequency (Non-PapersWithCode) \\\\\n",
      "\\midrule\n",
      "$C_{01}$ & #Prompt Example & Evaluation Configuration & 4 \\\\\n",
      "$C_{02}$ & #Prompt Token & Evaluation Configuration & 1 \\\\\n",
      "$C_{03}$ & Evaluation Data & Evaluation Configuration & 7 \\\\\n",
      "$C_{04}$ & Evaluation Metrics & Evaluation Configuration & 9 \\\\\n",
      "$C_{05}$ & Evaluator & Evaluation Configuration & 6 \\\\\n",
      "$C_{06}$ & Tokenizer & Evaluation Configuration & 1 \\\\\n",
      "$C_{07}$ & Domain Task & Functionality Evaluation & 21 \\\\\n",
      "$C_{08}$ & Evaluation Benchmark & Functionality Evaluation & 7 \\\\\n",
      "$C_{09}$ & Language Support & Functionality Evaluation & 7 \\\\\n",
      "$C_{10}$ & Model Capability & Functionality Evaluation & 16 \\\\\n",
      "$C_{11}$ & Task Modality & Functionality Evaluation & 2 \\\\\n",
      "$C_{12}$ & Leaderboard Series & Leaderboard Version & 1 \\\\\n",
      "$C_{13}$ & Release Date & Leaderboard Version & 2 \\\\\n",
      "$C_{14}$ & Model Accessibility & Model Information & 5 \\\\\n",
      "$C_{15}$ & Model Size & Model Information & 3 \\\\\n",
      "$C_{16}$ & Model Type & Model Information & 1 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Major splitting criteria (non-pwc)\n",
       "Functionality Evaluation    36\n",
       "Evaluation Configuration    23\n",
       "Model Information            8\n",
       "Leaderboard Version          3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path_rq1 / 'Leaderboard_processed.csv')\n",
    "df['Leaderboard splitting criteria for all Platform (non-pwc)'] = df['Leaderboard splitting criteria for all Platform (non-pwc)'].apply(ast.literal_eval)\n",
    "df = df.explode('Leaderboard splitting criteria for all Platform (non-pwc)')\n",
    "df = df[df['Leaderboard splitting criteria for all Platform (non-pwc)'].notna()]\n",
    "\n",
    "df_number = pd.DataFrame()\n",
    "for index, (name, group) in enumerate(df.groupby('Leaderboard splitting criteria for all Platform (non-pwc)')):\n",
    "    entry = {\n",
    "        'Index': index, # Temporarily store the original index\n",
    "        'Splitting criteria': name.title(),\n",
    "        'Major category': leaderboard_splitting_criteria_mapping_inverse[name],\n",
    "        'Frequency (Non-PapersWithCode)': len(group),\n",
    "    }\n",
    "    df_number = pd.concat([df_number, pd.DataFrame([entry])], ignore_index=True)\n",
    "df_number = df_number.sort_values(by=['Major category', 'Splitting criteria']).reset_index(drop=True)\n",
    "df_number['Index'] = df_number.index + 1\n",
    "df_number['Index'] = df_number['Index'].apply(lambda x: f'$C_{{0{x}}}$' if x < 10 else f'$C_{{{x}}}$')\n",
    "print(df_number.to_latex(float_format=\"%.2f\", index=False))\n",
    "\n",
    "df['Major splitting criteria (non-pwc)'] = df['Leaderboard splitting criteria for all Platform (non-pwc)'].map(leaderboard_splitting_criteria_mapping_inverse)\n",
    "df.drop_duplicates(subset=['Leaderboard', 'Major splitting criteria (non-pwc)'], inplace=True)\n",
    "df['Major splitting criteria (non-pwc)'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarks: 674\n",
      "Tasks: 808\n"
     ]
    }
   ],
   "source": [
    "benchmark_mapping = convert_csv_to_dict(path_rq1 / 'Benchmark.csv')\n",
    "\n",
    "leaderboard_benchmarks = set()\n",
    "for key in benchmark_mapping.keys():\n",
    "    benchmarks = [benchmark if benchmark == 'ARC (The Abstraction and Reasoning Corpus)' else benchmark.split(' (')[0] for benchmark in benchmark_mapping[key]]\n",
    "    leaderboard_benchmarks = leaderboard_benchmarks.union(set(benchmarks))\n",
    "print('Benchmarks:', len(leaderboard_benchmarks))\n",
    "\n",
    "leaderboard_tasks = set()\n",
    "for key in benchmark_mapping.keys():\n",
    "    tasks = set([benchmark for benchmark in benchmark_mapping[key]])\n",
    "    leaderboard_tasks = leaderboard_tasks.union(tasks)\n",
    "    benchmark_mapping[key] = list(tasks)\n",
    "print('Tasks:', len(leaderboard_tasks))\n",
    "\n",
    "df_benchmark = pd.DataFrame(list(benchmark_mapping.items()), columns=['Leaderboard', 'Benchmark'])\n",
    "df_benchmark.to_csv(path_rq1 / 'Benchmark_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "type": "histogram",
         "x": [
          1,
          1,
          1,
          1,
          37,
          1,
          11,
          1,
          1,
          9,
          1,
          1,
          2,
          1,
          20,
          1,
          1,
          1,
          1,
          3,
          4,
          2,
          5,
          1,
          9,
          16,
          1,
          1,
          1,
          1,
          12,
          1,
          1,
          1,
          1,
          1,
          5,
          1,
          1,
          20,
          1,
          19,
          66,
          52,
          1,
          11,
          1,
          1,
          30,
          7,
          41,
          10,
          1,
          44,
          17,
          1,
          1,
          6,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          5,
          1,
          1,
          5,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          9,
          1,
          40,
          8,
          1,
          10,
          1,
          1,
          1,
          1,
          1,
          1,
          7,
          1,
          7,
          1,
          1,
          3,
          11,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          1,
          5,
          6,
          4,
          1,
          13,
          3,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          23,
          19,
          209,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          12,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          9,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          8,
          1,
          7,
          10,
          14,
          1
         ]
        }
       ],
       "layout": {
        "bargap": 0.1,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Number of Benchmarks across Benchmark-based Leaderboards"
        },
        "xaxis": {
         "dtick": 20,
         "tickmode": "linear",
         "title": {
          "text": "Number of Benchmarks"
         }
        },
        "yaxis": {
         "title": {
          "text": "Number of Occurrence"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS statistic: 0.726530612244898\n",
      "p-value: 7.346024452315667e-63\n",
      "Skewness: 10.139708198281754\n",
      "Kurtosis: 125.05959772472715\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Audio",
         "type": "box",
         "y": [
          44,
          1,
          1,
          1
         ]
        },
        {
         "name": "Code",
         "type": "box",
         "y": [
          37,
          1,
          2,
          5,
          1,
          19,
          52,
          1,
          44,
          1,
          5,
          1,
          1,
          1,
          7,
          11,
          1,
          1,
          2,
          1,
          209,
          1,
          1,
          1,
          1,
          1
         ]
        },
        {
         "name": "Image",
         "type": "box",
         "y": [
          1,
          1,
          1,
          16,
          12,
          1,
          41,
          10,
          44,
          1,
          1,
          9,
          40,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          23,
          209,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          8
         ]
        },
        {
         "name": "Text",
         "type": "box",
         "y": [
          1,
          1,
          1,
          1,
          37,
          1,
          11,
          1,
          1,
          9,
          1,
          1,
          2,
          1,
          20,
          1,
          1,
          1,
          1,
          3,
          4,
          2,
          5,
          1,
          9,
          16,
          1,
          1,
          1,
          1,
          12,
          1,
          1,
          1,
          1,
          1,
          5,
          1,
          1,
          20,
          1,
          19,
          66,
          52,
          1,
          11,
          1,
          1,
          30,
          7,
          41,
          10,
          1,
          44,
          17,
          1,
          1,
          6,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          5,
          1,
          1,
          5,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          9,
          1,
          40,
          8,
          1,
          10,
          1,
          1,
          1,
          1,
          1,
          1,
          7,
          1,
          7,
          1,
          1,
          3,
          11,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          1,
          5,
          6,
          4,
          1,
          13,
          3,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          23,
          19,
          209,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          12,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          9,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          8,
          1,
          7,
          10,
          14,
          1
         ]
        },
        {
         "name": "Video",
         "type": "box",
         "y": [
          11,
          1,
          1,
          7,
          13,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Number of Benchmarks by Task Modality"
        },
        "xaxis": {
         "title": {
          "text": "Task Modality"
         }
        },
        "yaxis": {
         "title": {
          "text": "Number of Benchmarks"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_leaderboard = pd.read_csv(path_rq1 / 'Leaderboard_processed.csv')\n",
    "leaderboard_mapping = {leaderboard: [leaderboard] for leaderboard in df_leaderboard['Leaderboard'].tolist()}\n",
    "\n",
    "df_benchmark = pd.read_csv(path_rq1 / 'Benchmark_processed.csv')\n",
    "df_benchmark['Benchmark'] = df_benchmark['Benchmark'].apply(ast.literal_eval)\n",
    "\n",
    "benchmark_mapping = dict(zip(df_benchmark['Leaderboard'], df_benchmark['Benchmark']))\n",
    "for key in benchmark_mapping.keys():\n",
    "    benchmarks = set([benchmark if benchmark == 'ARC (The Abstraction and Reasoning Corpus)' else benchmark.split(' (')[0] for benchmark in benchmark_mapping[key]])\n",
    "    benchmark_mapping[key] = list(benchmarks)\n",
    "\n",
    "for leaderboard in leaderboard_mapping.keys():\n",
    "    if leaderboard in benchmark_mapping:\n",
    "        leaderboard_mapping[leaderboard] = benchmark_mapping[leaderboard]\n",
    "    elif leaderboard == 'MTEB':\n",
    "        leaderboard_mapping[leaderboard] = benchmark_mapping[leaderboard] + benchmark_mapping['CMTEB']\n",
    "\n",
    "benchmark_mapping_count = {key: len(value) for key, value in leaderboard_mapping.items()}\n",
    "benchmark_distribution = list(benchmark_mapping_count.values())\n",
    "\n",
    "fig = go.Figure(data=[go.Histogram(x=benchmark_distribution)])\n",
    "fig.update_layout(\n",
    "    title=\"Distribution of Number of Benchmarks across Benchmark-based Leaderboards\",\n",
    "    xaxis_title=\"Number of Benchmarks\",\n",
    "    yaxis_title=\"Number of Occurrence\",\n",
    "    bargap=0.1,  # Gap between bars of adjacent location coordinates\n",
    "    xaxis=dict(\n",
    "        tickmode='linear',  # Setting tick mode to linear\n",
    "        # tick0=1,    # Starting tick\n",
    "        dtick=20      # Set every single value on x-axis\n",
    "    )\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(path_rq1 / 'benchmark distribution.pdf')\n",
    "\n",
    "lambda_poisson = np.mean(benchmark_distribution)\n",
    "# Generating a Poisson distribution with the same λ and number of observations\n",
    "poisson_dist = stats.poisson.rvs(mu=lambda_poisson, size=len(benchmark_distribution))\n",
    "# Conducting a Kolmogorov-Smirnov test to compare the data to the theoretical Poisson distribution\n",
    "ks_statistic, p_value = stats.ks_2samp(benchmark_distribution, poisson_dist)\n",
    "\n",
    "print(f'KS statistic: {ks_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "\n",
    "skewness = stats.skew(benchmark_distribution)\n",
    "kurtosis = stats.kurtosis(benchmark_distribution)\n",
    "\n",
    "print('Skewness:', skewness)\n",
    "print('Kurtosis:', kurtosis)\n",
    "\n",
    "df_leaderboard['Modality'] = df_leaderboard['Modality'].apply(ast.literal_eval)\n",
    "df_leaderboard = df_leaderboard.explode('Modality')\n",
    "\n",
    "fig = go.Figure()\n",
    "for name, group in df_leaderboard.groupby('Modality'):\n",
    "    leaderboard_count = []\n",
    "    for leaderboard in group['Leaderboard'].tolist():\n",
    "        leaderboard_count.append(benchmark_mapping_count[leaderboard])\n",
    "    fig.add_trace(go.Box(y=leaderboard_count, name=name))\n",
    "fig.update_layout(title=\"Distribution of Number of Benchmarks by Task Modality\",\n",
    "                  yaxis=dict(title='Number of Benchmarks', type='log'),\n",
    "                  xaxis=dict(title='Task Modality'))\n",
    "fig.write_image(path_rq1 / 'benchmark distribution by task modality.pdf')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          "14",
          "10",
          "9",
          "9",
          "8",
          "7",
          "7",
          "6",
          "6",
          "6"
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          14,
          10,
          9,
          9,
          8,
          7,
          7,
          6,
          6,
          6
         ],
         "y": [
          "MMLU",
          "HumanEval",
          "GSM8K",
          "NQ",
          "HellaSwag",
          "TruthfulQA",
          "WSC",
          "NarrativeQA",
          "BoolQ",
          "MSCOCO"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Number of Benchmark in Leaderboards (Top 10)"
        },
        "xaxis": {
         "title": {
          "text": "Benchmark Number"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Leaderboard Name"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_leaderboard = pd.read_csv(path_rq1 / 'Leaderboard_processed.csv')\n",
    "leaderboard_mapping = {leaderboard: [leaderboard] for leaderboard in df_leaderboard['Leaderboard'].tolist()}\n",
    "\n",
    "df_benchmark = pd.read_csv(path_rq1 / 'Benchmark_processed.csv')\n",
    "df_benchmark['Benchmark'] = df_benchmark['Benchmark'].apply(ast.literal_eval)\n",
    "\n",
    "benchmark_mapping = dict(zip(df_benchmark['Leaderboard'], df_benchmark['Benchmark']))\n",
    "for key in benchmark_mapping.keys():\n",
    "    benchmarks = set([benchmark if benchmark == 'ARC (The Abstraction and Reasoning Corpus)' else benchmark.split(' (')[0] for benchmark in benchmark_mapping[key]])\n",
    "    benchmark_mapping[key] = list(benchmarks)\n",
    "\n",
    "for leaderboard in leaderboard_mapping.keys():\n",
    "    if leaderboard in benchmark_mapping:\n",
    "        leaderboard_mapping[leaderboard] = benchmark_mapping[leaderboard]\n",
    "    elif leaderboard == 'MTEB':\n",
    "        leaderboard_mapping[leaderboard] = benchmark_mapping[leaderboard] + benchmark_mapping['CMTEB']\n",
    "\n",
    "benchmark_mapping_inverse = defaultdict(list)\n",
    "for key, values in leaderboard_mapping.items():\n",
    "    for value in values:\n",
    "        benchmark_mapping_inverse[value].append(key)\n",
    "\n",
    "benchmark_mapping_inverse_count = {key: len(value) for key, value in benchmark_mapping_inverse.items()}\n",
    "labels, values = zip(*sorted(benchmark_mapping_inverse_count.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "fig = go.Figure(go.Bar(\n",
    "        x=values[:10],  # Values for the bar lengths\n",
    "        y=labels[:10],  # Categories for each bar\n",
    "        orientation='h',  # Sets the bars to be horizontal\n",
    "        text=values[:10],  # Adds the values as text on each bar\n",
    "        textposition='auto'  # Automatically positions the text on the bars\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "        title=f'Number of Benchmark in Leaderboards (Top 10)',\n",
    "        xaxis_title='Benchmark Number',\n",
    "        yaxis_title='Leaderboard Name',\n",
    "        yaxis_autorange='reversed'  # This line makes the bars go top-down\n",
    ")\n",
    "\n",
    "fig.write_image(path_rq1 / 'benchmark distribution (top-10).pdf')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leaderboard = pd.read_csv(path_rq1 / 'Leaderboard_processed.csv')\n",
    "# df_leaderboard['Leaderboard splitting criteria for all Platform (non-pwc)'] = df_leaderboard['Leaderboard splitting criteria for all Platform (non-pwc)'].apply(ast.literal_eval)\n",
    "# leaderboard_mapping = {leaderboard: [leaderboard] for leaderboard in df_leaderboard['Leaderboard'].tolist()}\n",
    "\n",
    "# df_benchmark = pd.read_csv(path_rq1 / 'Benchmark_processed.csv')\n",
    "# df_benchmark['Benchmark'] = df_benchmark['Benchmark'].apply(ast.literal_eval)\n",
    "\n",
    "# benchmark_mapping = dict(zip(df_benchmark['Leaderboard'], df_benchmark['Benchmark']))\n",
    "# for key in benchmark_mapping.keys():\n",
    "#     benchmarks = set([benchmark if benchmark == 'ARC (The Abstraction and Reasoning Corpus)' else benchmark.split(' (')[0] for benchmark in benchmark_mapping[key]])\n",
    "#     benchmark_mapping[key] = list(benchmarks)\n",
    "\n",
    "# for leaderboard in leaderboard_mapping.keys():\n",
    "#     if leaderboard in benchmark_mapping:\n",
    "#         leaderboard_mapping[leaderboard] = benchmark_mapping[leaderboard]\n",
    "#     elif leaderboard == 'MTEB':\n",
    "#         leaderboard_mapping[leaderboard] = benchmark_mapping[leaderboard] + benchmark_mapping['CMTEB']\n",
    "\n",
    "# leaderboard_split_mapping = dict(zip(df_leaderboard['Leaderboard'], df_leaderboard['Leaderboard splitting criteria for all Platform (non-pwc)']))\n",
    "# benchmark_split_mapping = defaultdict(list)\n",
    "# for leaderboard, benchmarks in leaderboard_mapping.items():\n",
    "#     for benchmark in benchmarks:\n",
    "#         if benchmark not in labels:\n",
    "#             continue\n",
    "#         for split in leaderboard_split_mapping[leaderboard]:\n",
    "#             benchmark_split_mapping[benchmark].append(leaderboard_splitting_criteria_mapping_inverse[split])\n",
    "\n",
    "# # Convert dictionary to DataFrame\n",
    "# df = pd.DataFrame([(key, val) for key, vals in benchmark_split_mapping.items() for val in vals], columns=['Benchmark', 'Leaderboard split'])\n",
    "\n",
    "# # Count the frequency of each value for each key\n",
    "# df_count = df.groupby(['Benchmark', 'Leaderboard split']).size().reset_index(name='Leaderboard Count')\n",
    "\n",
    "# # Create a group bar chart\n",
    "# fig = px.bar(df_count, x='Benchmark', y='Leaderboard Count', color='Leaderboard split', barmode='group', title='Frequency of Scenario-based Leaderboard Split for Each Benchmark')\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leaderboard = pd.read_csv(path_rq1 / 'Leaderboard_processed.csv')\n",
    "# df_leaderboard['Leaderboard splitting criteria for all Platform (non-pwc)'] = df_leaderboard['Leaderboard splitting criteria for all Platform (non-pwc)'].apply(ast.literal_eval)\n",
    "# leaderboard_mapping = {leaderboard: [leaderboard] for leaderboard in df_leaderboard['Leaderboard'].tolist()}\n",
    "\n",
    "# df_benchmark = pd.read_csv(path_rq1 / 'Benchmark_processed.csv')\n",
    "# df_benchmark['Benchmark'] = df_benchmark['Benchmark'].apply(ast.literal_eval)\n",
    "\n",
    "# benchmark_mapping = dict(zip(df_benchmark['Leaderboard'], df_benchmark['Benchmark']))\n",
    "# for key in benchmark_mapping.keys():\n",
    "#     benchmarks = Counter([benchmark if benchmark == 'ARC (The Abstraction and Reasoning Corpus)' else benchmark.split(' (')[0] for benchmark in benchmark_mapping[key]])\n",
    "#     benchmark_mapping[key] = dict(benchmarks)\n",
    "\n",
    "# for leaderboard in leaderboard_mapping.keys():\n",
    "#     if leaderboard in benchmark_mapping:\n",
    "#         leaderboard_mapping[leaderboard] = benchmark_mapping[leaderboard]\n",
    "#     elif leaderboard == 'MTEB':\n",
    "#         leaderboard_mapping[leaderboard] = benchmark_mapping[leaderboard] | benchmark_mapping['CMTEB']\n",
    "#     else:\n",
    "#         leaderboard_mapping[leaderboard] = {leaderboard: 1}\n",
    "\n",
    "# # Creating the Node and Edge tables\n",
    "# node_data = []\n",
    "# edge_data = []\n",
    "\n",
    "# for key in leaderboard_mapping:\n",
    "#     total_weight = sum(leaderboard_mapping[key].values())\n",
    "#     node_data.append({\"Id\": key, \"Label\": key, \"Size\": total_weight})\n",
    "#     for sub_key, weight in leaderboard_mapping[key].items():\n",
    "#         edge_data.append({\"Platform\": key, \"Target\": sub_key, \"Weight\": weight})\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# node_table = pd.DataFrame(node_data)\n",
    "# edge_table = pd.DataFrame(edge_data)\n",
    "\n",
    "# node_table.to_excel(path_rq1 / 'node.xlsx', index=False)\n",
    "# edge_table.to_excel(path_rq1 / 'edge.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a graph\n",
    "# G = nx.Graph()\n",
    "# df_leaderboard_benchmark = convert_csv_to_dict(path_rq1 / 'Benchmark.csv')\n",
    "\n",
    "# # Add nodes and edges\n",
    "# for key, values in df_leaderboard_benchmark.items():\n",
    "#     G.add_node(key)\n",
    "#     for value in values:\n",
    "#         G.add_edge(key, value)\n",
    "\n",
    "# # # Increase figure size\n",
    "# plt.figure(figsize=(30, 30))  # You can adjust the size as needed\n",
    "\n",
    "# # Adjust layout (you can experiment with different layouts)\n",
    "# pos = nx.spring_layout(G)  # 'spring_layout' is often a good choice\n",
    "\n",
    "# # Draw the graph\n",
    "# nx.draw(G, with_labels=True, node_color='lightblue',\n",
    "#         font_size=10, node_size=500)\n",
    "# # Save the figure\n",
    "# plt.savefig(path_rq1 / \"benchmark network graph.pdf\", format='png', dpi=300)  # Adjust filename, format, and dpi as needed\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leaderboard_metrics = convert_csv_to_dict(path_rq1 / 'Metrics.csv')\n",
    "# metrics = set()\n",
    "# for key, value in df_leaderboard_metrics.items():\n",
    "#     # print(value)\n",
    "#     metrics = metrics.union(set(value))\n",
    "# print('Metrics:', len(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# df_leaderboard_metrics = convert_csv_to_dict(path_rq1 / 'Metrics.csv\n",
    "\n",
    "# all_values = []\n",
    "# for value_list in df_leaderboard_metrics.values():\n",
    "#     all_values.extend(value_list)\n",
    "\n",
    "# # Count occurrences of each string\n",
    "# counter = Counter(all_values)\n",
    "\n",
    "# # Get the top-10 most common strings\n",
    "# top_10 = counter.most_common(10)\n",
    "\n",
    "# # Prepare data for Plotly\n",
    "# labels, values = zip(*top_10)\n",
    "\n",
    "# fig = go.Figure(go.Bar(\n",
    "#     x=values,  # Values for the bar lengths\n",
    "#     y=labels,  # Categories for each bar\n",
    "#     orientation='h',  # Sets the bars to be horizontal\n",
    "#     text=values,  # Adds the values as text on each bar\n",
    "#     textposition='auto'  # Automatically positions the text on the bars\n",
    "# ))\n",
    "\n",
    "# fig.update_layout(\n",
    "#     title=f'Frequency of Metrics in Leaderboards (Top 10)',\n",
    "#     xaxis_title='Leaderboad Number',\n",
    "#     yaxis_title='Metrics Name',\n",
    "#     yaxis_autorange='reversed'  # This line makes the bars go top-down\n",
    "# )\n",
    "\n",
    "# fig.write_image(path_rq1 / 'metrics distribution.pdf')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a graph\n",
    "# G = nx.Graph()\n",
    "# df_leaderboard_benchmark = convert_csv_to_dict(path_rq1 / 'Metrics.csv')\n",
    "\n",
    "# # Add nodes and edges\n",
    "# for key, values in df_leaderboard_benchmark.items():\n",
    "#     G.add_node(key)\n",
    "#     for value in values:\n",
    "#         G.add_edge(key, value)\n",
    "\n",
    "# # Increase figure size\n",
    "# plt.figure(figsize=(30, 30))  # You can adjust the size as needed\n",
    "\n",
    "# # Adjust layout (you can experiment with different layouts)\n",
    "# pos = nx.spring_layout(G)  # 'spring_layout' is often a good choice\n",
    "\n",
    "# # Draw the graph\n",
    "# nx.draw(G, with_labels=True, node_color='lightblue',\n",
    "#         font_size=10, node_size=500)\n",
    "# # Save the figure\n",
    "# plt.savefig(path_rq1 / \"metrics network graph.pdf\", format='png', dpi=300)  # Adjust filename, format, and dpi as needed\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SEED-Bench Leaderboard',\n",
       " 'InstructEval',\n",
       " 'LLMonitor',\n",
       " 'LawBench',\n",
       " 'HEIM',\n",
       " 'AlignBench',\n",
       " 'LLMPerf Leaderboard',\n",
       " 'SuperCLUE',\n",
       " 'Open Ko-LLM Leaderboard',\n",
       " 'AgentBench',\n",
       " 'CLiB',\n",
       " 'MathVista',\n",
       " 'LLM Benchmarker Suite',\n",
       " 'Xiezhi',\n",
       " 'HELM Classic',\n",
       " 'AlpacaEval',\n",
       " 'FacTool',\n",
       " 'SciGraphQA',\n",
       " 'MMMU',\n",
       " 'Colossal-AI',\n",
       " 'SuperCLUElyb',\n",
       " 'Coding LLMs Leaderboard',\n",
       " 'MMBench',\n",
       " 'ODEX',\n",
       " 'Spider',\n",
       " 'DocVQA',\n",
       " 'Toolbench Leaderboard',\n",
       " 'SummEdits',\n",
       " 'LLM-Leaderboard',\n",
       " 'LLM-Perf Leaderboard',\n",
       " 'FinanceIQ',\n",
       " 'SafetyBench',\n",
       " 'LLMEval',\n",
       " 'SuperLim (v2)',\n",
       " 'Chatbot Arena Leaderboad',\n",
       " 'M3KE',\n",
       " 'TabMWP',\n",
       " 'InfiMM-Eval',\n",
       " 'QuALITY',\n",
       " 'HELM Lite',\n",
       " 'C-Eval',\n",
       " 'TruthfulQA',\n",
       " 'GAOKAO-Bench',\n",
       " 'CLEVA',\n",
       " 'PromptBench',\n",
       " 'Q-Bench',\n",
       " 'tStoryCloze',\n",
       " 'InfiCoder-Eval',\n",
       " 'CMMLU',\n",
       " 'ScienceQA',\n",
       " 'RAFT',\n",
       " 'Big Code Models Leaderboard',\n",
       " 'Large Language Model Leaderboard',\n",
       " 'JustEval',\n",
       " 'FActScore',\n",
       " 'UHGEval',\n",
       " 'MMCU',\n",
       " 'HalluQA',\n",
       " 'Open LLM Leaderboard',\n",
       " 'SWE-bench',\n",
       " 'Hallucination Leaderboard',\n",
       " 'SuperCLUE-Auto',\n",
       " 'VisIT-Bench',\n",
       " 'Open Multilingual LLM Evaluation Leaderboard',\n",
       " 'SuperCLUE-Safety',\n",
       " 'FlagEval',\n",
       " 'LMExamQA',\n",
       " 'Video-Bench Leaderboard',\n",
       " 'DecodingTrust',\n",
       " 'MATH401',\n",
       " 'SuperGLUE',\n",
       " 'CG-Eval',\n",
       " 'BIRD',\n",
       " 'DS-1000',\n",
       " 'MMLU',\n",
       " 'MBPP',\n",
       " 'CanAiCode Leaderboard',\n",
       " 'EvalPlus',\n",
       " 'Toloka LLM Leaderboard',\n",
       " 'FELM',\n",
       " 'SuperCLUEgkzw',\n",
       " 'CMB',\n",
       " 'L-Eval',\n",
       " 'Multi-modal Modal Leaderboard',\n",
       " 'SuperCLUE-Agent',\n",
       " 'MME',\n",
       " 'OpenEval (text)',\n",
       " 'InterCode',\n",
       " 'MTEB',\n",
       " 'HumanEval',\n",
       " 'ANGO',\n",
       " 'LongBench',\n",
       " 'CCBench',\n",
       " 'BotChat',\n",
       " 'ScandEval']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pattern = r'( |_|-|\\())[0-9.]+b'\n",
    "pattern = r'[0-9.]+b'\n",
    "\n",
    "model_with_size = set()\n",
    "for directory in os.listdir(path_data):\n",
    "    subdirectory_path = os.path.join(path_data, directory)\n",
    "    for file in glob.glob(os.path.join(subdirectory_path, \"*.json\")):\n",
    "        file_name = file.split('/')[-1]\n",
    "        for prefix in platform_nontemplate:\n",
    "            if file_name.startswith(prefix):\n",
    "                df = pd.read_json(file)\n",
    "                if df['Model'].str.lower().str.contains(pattern, regex=True).any():\n",
    "                    model_with_size.add((directory, platform_mapping[prefix]))\n",
    "\n",
    "model_with_size_leaderboards = list(set([x[0] for x in model_with_size]))\n",
    "model_with_size_leaderboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PubMedQA',\n",
       " 'HellaSwag',\n",
       " 'QuALITY',\n",
       " 'BIRD',\n",
       " 'MMBench',\n",
       " 'Large Language Model Leaderboard',\n",
       " 'Spider',\n",
       " 'DS-1000',\n",
       " 'LawBench',\n",
       " 'OpenEval (text)',\n",
       " 'CCBench',\n",
       " 'HallusionBench']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_publisher = set()\n",
    "for directory in os.listdir(path_data):\n",
    "    subdirectory_path = os.path.join(path_data, directory)\n",
    "    for file in glob.glob(os.path.join(subdirectory_path, \"*.json\")):\n",
    "        file_name = file.split('/')[-1]\n",
    "        for prefix in platform_nontemplate:\n",
    "            if file_name.startswith(prefix):\n",
    "                df = pd.read_json(file)\n",
    "                if df['Model'].str.contains('\\n').any():\n",
    "                    model_with_publisher.add((directory, platform_mapping[prefix]))\n",
    "\n",
    "model_with_publisher_leaderboard = list(set([x[0] for x in model_with_publisher]))\n",
    "model_with_publisher_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "text": [
          "1",
          "10",
          "95",
          "7",
          "2"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          "Model Evaluator",
          "Model Publisher",
          "Model Size",
          "Repository Username",
          "Official Verification"
         ],
         "y": [
          1,
          10,
          95,
          7,
          2
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Frequency of Leaderboards by Model Naming Convention"
        },
        "xaxis": {
         "title": {
          "text": "Model Naming Convention"
         }
        },
        "yaxis": {
         "range": [
          0,
          104.50000000000001
         ],
         "title": {
          "text": "Count"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=list(model_naming_convention_count.keys()),\n",
    "        y=list(model_naming_convention_count.values()),\n",
    "        text=list(model_naming_convention_count.values()),\n",
    "        textposition='outside'\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Frequency of Leaderboards by Model Naming Convention',\n",
    "    xaxis_title='Model Naming Convention',\n",
    "    yaxis_title='Count',\n",
    "    yaxis=dict(\n",
    "        range=[0, max(model_naming_convention_count.values())\n",
    "               * 1.1]  # Increase y-axis range\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.write_image(path_rq1 / 'model naming convention distribution.pdf')\n",
    "fig.show()\n",
    "# explain in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          "48",
          "39",
          "35",
          "33",
          "31",
          "30",
          "24",
          "23",
          "21",
          "20"
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          48,
          39,
          35,
          33,
          31,
          30,
          24,
          23,
          21,
          20
         ],
         "y": [
          "gpt-4",
          "bloom",
          "palm",
          "gpt-3",
          "llama",
          "chatgpt",
          "opt",
          "chatglm2-6b",
          "t5-11b",
          "roberta"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Frequency of Models in Leaderboards (Top 10)"
        },
        "xaxis": {
         "title": {
          "text": "Leaderboard Number"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Model Name"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = set()\n",
    "leaderboard_model_mapping = defaultdict(set)\n",
    "for directory in os.listdir(path_data):\n",
    "    subdirectory_path = os.path.join(path_data, directory)\n",
    "    for file in glob.glob(os.path.join(subdirectory_path, \"*.json\")):\n",
    "        df = pd.read_json(file)\n",
    "        if directory in model_naming_convention['Repository Username']:\n",
    "            df['Model'] = df['Model'].apply(lambda x: x.split('/')[-1])\n",
    "        df['Model'] = df['Model'].apply(lambda x: x.split('\\n')[0].split(' ')[0].lower())\n",
    "        leaderboard_model_mapping[directory] = leaderboard_model_mapping[directory].union(set(df['Model'].tolist()))\n",
    "    models = models.union(leaderboard_model_mapping[directory])\n",
    "\n",
    "model_mapping_inverse = defaultdict(list)\n",
    "for key, values in leaderboard_model_mapping.items():\n",
    "    for value in values:\n",
    "        model_mapping_inverse[value].append(key)\n",
    "        \n",
    "model_mapping_inverse_count = {key: len(value) for key, value in model_mapping_inverse.items()}\n",
    "labels, values = zip(*sorted(model_mapping_inverse_count.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "fig = go.Figure(go.Bar(\n",
    "    x=values[:10],  # Values for the bar lengths\n",
    "    y=labels[:10],  # Categories for each bar\n",
    "    orientation='h',  # Sets the bars to be horizontal\n",
    "    text=values[:10],  # Adds the values as text on each bar\n",
    "    textposition='auto'  # Automatically positions the text on the bars\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Frequency of Models in Leaderboards (Top 10)',\n",
    "    xaxis_title='Leaderboard Number',\n",
    "    yaxis_title='Model Name',\n",
    "    yaxis_autorange='reversed'  # This line makes the bars go top-down\n",
    ")\n",
    "\n",
    "fig.write_image(path_rq1 / 'model distribution (top-10).pdf')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          "2212",
          "1131",
          "853",
          "758",
          "232",
          "222",
          "197",
          "180",
          "179",
          "166"
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          2212,
          1131,
          853,
          758,
          232,
          222,
          197,
          180,
          179,
          166
         ],
         "y": [
          "Open LLM Leaderboard",
          "MMLU",
          "ImageNet",
          "Open Ko-LLM Leaderboard",
          "CIFAR-10",
          "A-OKVQA",
          "Kinetics",
          "MTEB",
          "ADE20K",
          "UCF101"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Number of Model in Leaderboards (Top 10)"
        },
        "xaxis": {
         "title": {
          "text": "Model Number"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Leaderboard Name"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS statistic: 0.7032520325203252\n",
      "p-value: 1.1448195564386896e-58\n",
      "Skewness: 9.465101586125774\n",
      "Kurtosis: 103.15254715859737\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Audio",
         "type": "box",
         "y": [
          40,
          12,
          53,
          6
         ]
        },
        {
         "name": "Code",
         "type": "box",
         "y": [
          104,
          110,
          51,
          9,
          18,
          8,
          107,
          23,
          40,
          15,
          5,
          33,
          6,
          3,
          51,
          5,
          62,
          44,
          41,
          11,
          22,
          10,
          6,
          13,
          9,
          8
         ]
        },
        {
         "name": "Image",
         "type": "box",
         "y": [
          27,
          38,
          44,
          26,
          37,
          21,
          13,
          13,
          40,
          78,
          222,
          40,
          16,
          9,
          7,
          16,
          16,
          17,
          90,
          179,
          4,
          6,
          22,
          17,
          232,
          21,
          13,
          853,
          40,
          7,
          30,
          89,
          8,
          31,
          23,
          16,
          14,
          11,
          18,
          15,
          73,
          8,
          66,
          90,
          30,
          20
         ]
        },
        {
         "name": "Text",
         "type": "box",
         "y": [
          28,
          30,
          14,
          51,
          104,
          8,
          14,
          13,
          16,
          24,
          12,
          110,
          38,
          8,
          5,
          27,
          38,
          11,
          17,
          25,
          20,
          51,
          9,
          44,
          24,
          26,
          22,
          5,
          14,
          25,
          37,
          111,
          18,
          50,
          11,
          15,
          92,
          10,
          17,
          13,
          20,
          8,
          180,
          107,
          21,
          7,
          21,
          23,
          9,
          82,
          13,
          13,
          57,
          40,
          10,
          96,
          68,
          4,
          9,
          114,
          12,
          78,
          222,
          68,
          95,
          19,
          70,
          63,
          111,
          85,
          70,
          14,
          11,
          7,
          17,
          13,
          15,
          5,
          12,
          3,
          19,
          12,
          24,
          33,
          8,
          6,
          3,
          40,
          14,
          16,
          126,
          9,
          20,
          9,
          16,
          30,
          7,
          3,
          16,
          6,
          32,
          51,
          16,
          15,
          25,
          5,
          1131,
          20,
          17,
          62,
          32,
          90,
          44,
          25,
          41,
          121,
          758,
          2212,
          2,
          44,
          10,
          61,
          6,
          74,
          55,
          27,
          179,
          4,
          12,
          11,
          4,
          25,
          2,
          6,
          29,
          22,
          5,
          41,
          13,
          4,
          17,
          232,
          15,
          2,
          67,
          36,
          10,
          19,
          8,
          4,
          41,
          17,
          2,
          53,
          21,
          1,
          3,
          13,
          62,
          94,
          853,
          40,
          4,
          197,
          1,
          27,
          7,
          1,
          44,
          1,
          12,
          6,
          15,
          27,
          6,
          30,
          46,
          1,
          89,
          105,
          43,
          33,
          44,
          18,
          24,
          8,
          34,
          31,
          23,
          16,
          1,
          20,
          23,
          28,
          14,
          20,
          53,
          22,
          117,
          43,
          83,
          13,
          7,
          13,
          66,
          15,
          19,
          4,
          6,
          33,
          9,
          1,
          166,
          11,
          18,
          15,
          4,
          73,
          1,
          8,
          1,
          4,
          63,
          29,
          66,
          85,
          15,
          11,
          13,
          18,
          19,
          5,
          90,
          100,
          30,
          13,
          29,
          9,
          41,
          20,
          8,
          12,
          33,
          18,
          0
         ]
        },
        {
         "name": "Video",
         "type": "box",
         "y": [
          14,
          12,
          44,
          6,
          10,
          55,
          27,
          25,
          4,
          41,
          3,
          94,
          4,
          197,
          27,
          105,
          43,
          33,
          44,
          24,
          117,
          13,
          19,
          9,
          166,
          1,
          19,
          5
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Number of Models by Task Modality"
        },
        "xaxis": {
         "title": {
          "text": "Task Modality"
         }
        },
        "yaxis": {
         "title": {
          "text": "Number of Models"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "leaderboard_model_count = defaultdict(int)\n",
    "for directory in os.listdir(path_data):\n",
    "    subdirectory_path = os.path.join(path_data, directory)\n",
    "    model_names = []\n",
    "    for file in glob.glob(os.path.join(subdirectory_path, \"*.json\")):\n",
    "        df = pd.read_json(file)\n",
    "        if directory in model_naming_convention['Repository Username']:\n",
    "            df['Model'] = df['Model'].apply(lambda x: x.split('/')[-1])\n",
    "        df['Model'] = df['Model'].apply(lambda x: x.split('\\n')[0].split('(')[0].strip())\n",
    "        model_names.extend(df['Model'].tolist())\n",
    "    leaderboard_model_count[directory] = len(set(model_names))\n",
    "labels, values = zip(*sorted(leaderboard_model_count.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "fig = go.Figure(go.Bar(\n",
    "    x=values[:10],  # Values for the bar lengths\n",
    "    y=labels[:10],  # Categories for each bar\n",
    "    orientation='h',  # Sets the bars to be horizontal\n",
    "    text=values[:10],  # Adds the values as text on each bar\n",
    "    textposition='auto'  # Automatically positions the text on the bars\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Number of Model in Leaderboards (Top 10)',\n",
    "    xaxis_title='Model Number',\n",
    "    yaxis_title='Leaderboard Name',\n",
    "    yaxis_autorange='reversed'  # This line makes the bars go top-down\n",
    ")\n",
    "\n",
    "fig.write_image(path_rq1 / 'model distribution (top-10).pdf')\n",
    "fig.show()\n",
    "\n",
    "lambda_poisson = np.mean(values)\n",
    "# Generating a Poisson distribution with the same λ and number of observations\n",
    "poisson_dist = stats.poisson.rvs(mu=lambda_poisson, size=len(values))\n",
    "# Conducting a Kolmogorov-Smirnov test to compare the data to the theoretical Poisson distribution\n",
    "ks_statistic, p_value = stats.ks_2samp(values, poisson_dist)\n",
    "\n",
    "print(f'KS statistic: {ks_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "\n",
    "skewness = stats.skew(values)\n",
    "kurtosis = stats.kurtosis(values)\n",
    "\n",
    "print('Skewness:', skewness)\n",
    "print('Kurtosis:', kurtosis)\n",
    "\n",
    "df_leaderboard = pd.read_csv(path_rq1 / 'Leaderboard_processed.csv')\n",
    "df_leaderboard['Modality'] = df_leaderboard['Modality'].apply(ast.literal_eval)\n",
    "df_leaderboard = df_leaderboard.explode('Modality')\n",
    "\n",
    "fig = go.Figure()\n",
    "for name, group in df_leaderboard.groupby('Modality'):\n",
    "    leaderboard_count = []\n",
    "    for leaderboard in group['Leaderboard'].tolist():\n",
    "        leaderboard_count.append(leaderboard_model_count[leaderboard])\n",
    "    fig.add_trace(go.Box(y=leaderboard_count, name=name))\n",
    "fig.update_layout(title=\"Distribution of Number of Models by Task Modality\",\n",
    "                  yaxis=dict(title='Number of Models', type='log'),\n",
    "                  xaxis=dict(title='Task Modality'))\n",
    "fig.write_image(path_rq1 / 'model distribution by task modality.pdf')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import networkx as nx\n",
    "\n",
    "# leaderboard_models = {}\n",
    "\n",
    "# for directory in os.listdir(path_data):\n",
    "#     subdirectory_path = os.path.join(path_data, directory)\n",
    "#     model_names = []\n",
    "#     for file in glob.glob(os.path.join(subdirectory_path, \"*.json\")):\n",
    "#         df = pd.read_json(file)\n",
    "#         if directory in model_naming_convention['Repository Username']:\n",
    "#             df['Model'] = df['Model'].apply(lambda x: x.split('/')[-1])\n",
    "#         df['Model'] = df['Model'].apply(lambda x: x.split('\\n')[0].split('(')[0].strip())\n",
    "#         model_names.extend(df['Model'].tolist())\n",
    "#     leaderboard_models[directory] = set(model_names)\n",
    "\n",
    "# model_leaderboards = {}\n",
    "\n",
    "# for key, values in leaderboard_models.items():\n",
    "#     for value in values:\n",
    "#         # Add the key to the list of keys for this value\n",
    "#         if value not in model_leaderboards:\n",
    "#             model_leaderboards[value] = [key]\n",
    "#         else:\n",
    "#             model_leaderboards[value].append(key)\n",
    "\n",
    "# # Create a graph\n",
    "# G = nx.Graph()\n",
    "\n",
    "# # Add nodes and edges\n",
    "# for key, values in model_leaderboards.items():\n",
    "#     G.add_node(key)\n",
    "#     for value in values:\n",
    "#         G.add_edge(key, value)\n",
    "\n",
    "# # Increase figure size\n",
    "# plt.figure(figsize=(80, 80))  # You can adjust the size as needed\n",
    "\n",
    "# # Adjust layout (you can experiment with different layouts)\n",
    "# pos = nx.spring_layout(G)  # 'spring_layout' is often a good choice\n",
    "\n",
    "# # Draw the graph\n",
    "# nx.draw(G, with_labels=True, node_color='lightblue',\n",
    "#         font_size=10, node_size=500)\n",
    "# # Save the figure\n",
    "# plt.savefig(path_rq1 / \"model network graph.pdf\", format='png', dpi=300)  # Adjust filename, format, and dpi as needed\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwc_default_columns = {'extra training data', 'paper', 'code', 'result', 'year', 'tags'}\n",
    "ai2_default_columns = {'created'}\n",
    "leaderboard_attributes = {}\n",
    "leaderboard_metrics_raw = {}\n",
    "leaderboard_metrics = {}\n",
    "\n",
    "for directory in os.listdir(path_data):\n",
    "    subdirectory_path = os.path.join(path_data, directory)\n",
    "    template = set()\n",
    "    template_raw = set()\n",
    "    nontemplate = set()\n",
    "    for file in glob.glob(os.path.join(subdirectory_path, \"*.json\")):\n",
    "        df = pd.read_json(file)\n",
    "        file_name = file.split('/')[-1]\n",
    "        if file_name.startswith('pwc'):\n",
    "            keys = set()\n",
    "            for item in df['metrics']:\n",
    "                for key in item.keys():\n",
    "                    key_processed = key.split('(')[0].lower().strip()\n",
    "                    if key_processed in inverted_metrics_synonyms:\n",
    "                        keys.add(inverted_metrics_synonyms[key_processed])\n",
    "                    else:\n",
    "                        keys.add(key_processed)\n",
    "            template = template.union(keys)\n",
    "            template_raw = template_raw.union(keys).union(pwc_default_columns)\n",
    "        elif file_name.startswith('ai2'):\n",
    "            column_names = set()\n",
    "            for column_name in df.columns.tolist():\n",
    "                if column_name in ['Model', 'Created']:\n",
    "                    continue\n",
    "                column_name = column_name.lower()\n",
    "                if column_name in inverted_metrics_synonyms:\n",
    "                    column_names.add(inverted_metrics_synonyms[column_name])\n",
    "                elif column_name:\n",
    "                    column_names.add(column_name)\n",
    "            template = template.union(column_names)\n",
    "            template_raw = template_raw.union(column_names).union(ai2_default_columns)\n",
    "        else:\n",
    "            column_names = set()\n",
    "            for column_name in df.columns.tolist():\n",
    "                if column_name == 'Model':\n",
    "                    continue\n",
    "                column_name = column_name.split('\\n')[0].split('(')[0].split(' - ')[-1].split(']')[-1].lower().strip().rstrip('.')\n",
    "                if column_name in inverted_metrics_synonyms:\n",
    "                    column_names.add(inverted_metrics_synonyms[column_name])\n",
    "                elif column_name:\n",
    "                    column_names.add(column_name)\n",
    "            nontemplate = nontemplate.union(column_names)\n",
    "    if len(template):\n",
    "        leaderboard_metrics[directory] = template\n",
    "        leaderboard_metrics_raw[directory] = template_raw\n",
    "    elif len(nontemplate):\n",
    "        leaderboard_attributes[directory] = nontemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          "25",
          "21",
          "12",
          "12",
          "9",
          "9",
          "8",
          "7",
          "7",
          "6"
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          25,
          21,
          12,
          12,
          9,
          9,
          8,
          7,
          7,
          6
         ],
         "y": [
          "#parameters",
          "average",
          "score",
          "overall",
          "overall score",
          "mmlu",
          "type",
          "organization",
          "date",
          "link"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Frequency of Attributes in Publisher-based Non-template Leaderboards (Top 10)"
        },
        "xaxis": {
         "title": {
          "text": "Frequency of Leaderboards"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Attribute Name"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS statistic: 0.3148425787106447\n",
      "p-value: 8.067207429950893e-59\n",
      "Skewness: 13.059744160232468\n",
      "Kurtosis: 222.01315709813733\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "type": "histogram",
         "x": [
          25,
          21,
          12,
          12,
          9,
          9,
          8,
          7,
          7,
          6,
          5,
          5,
          5,
          5,
          5,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ]
        }
       ],
       "layout": {
        "bargap": 0.1,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Number of Attributes across Publisher-based Leaderboards"
        },
        "xaxis": {
         "title": {
          "text": "Number of Attributes"
         }
        },
        "yaxis": {
         "title": {
          "text": "Number of Occurrence"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_counter = Counter()\n",
    "for values_list in leaderboard_attributes.values():\n",
    "    total_counter.update(values_list)\n",
    "labels, values = zip(*sorted(total_counter.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "fig = go.Figure(go.Bar(\n",
    "    x=values[:10],  # Values for the bar lengths\n",
    "    y=labels[:10],  # Categories for each bar\n",
    "    orientation='h',  # Sets the bars to be horizontal\n",
    "    text=values[:10],  # Adds the values as text on each bar\n",
    "    textposition='auto'  # Automatically positions the text on the bars\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Frequency of Attributes in Publisher-based Non-template Leaderboards (Top 10)',\n",
    "    xaxis_title='Frequency of Leaderboards',\n",
    "    yaxis_title='Attribute Name',\n",
    "    yaxis_autorange='reversed'  # This line makes the bars go top-down\n",
    ")\n",
    "# fig.write_image(path_rq1 / 'attribute distribution (non-template) (top-10).pdf')\n",
    "fig.show()\n",
    "\n",
    "lambda_poisson = np.mean(values)\n",
    "# Generating a Poisson distribution with the same λ and number of observations\n",
    "poisson_dist = stats.poisson.rvs(mu=lambda_poisson, size=len(values))\n",
    "# Conducting a Kolmogorov-Smirnov test to compare the data to the theoretical Poisson distribution\n",
    "ks_statistic, p_value = stats.ks_2samp(values, poisson_dist)\n",
    "\n",
    "print(f'KS statistic: {ks_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "\n",
    "skewness = stats.skew(values)\n",
    "kurtosis = stats.kurtosis(values)\n",
    "\n",
    "print('Skewness:', skewness)\n",
    "print('Kurtosis:', kurtosis)\n",
    "\n",
    "fig = go.Figure(data=[go.Histogram(x=values)])\n",
    "fig.update_layout(\n",
    "    title=\"Distribution of Number of Attributes across Publisher-based Leaderboards\",\n",
    "    xaxis_title=\"Number of Attributes\",\n",
    "    yaxis_title=\"Number of Occurrence\",\n",
    "    bargap=0.1,  # Gap between bars of adjacent location coordinates\n",
    ")\n",
    "# fig.write_image(path_rq1 / 'attribute distribution (non-template).pdf')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          "71",
          "13",
          "11",
          "9",
          "9",
          "9",
          "9",
          "8",
          "8",
          "8"
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          71,
          13,
          11,
          9,
          9,
          9,
          9,
          8,
          8,
          8
         ],
         "y": [
          "accuracy",
          "#parameters",
          "exact match",
          "recall@10",
          "recall@5",
          "top-1 accuracy",
          "f1",
          "recall@1",
          "meteor",
          "rouge-l"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Frequency of Metrics in Publisher-based Template Leaderboards (Top 10)"
        },
        "xaxis": {
         "title": {
          "text": "Frequency of Leaderboards"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Metrics Name"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS statistic: 0.38626609442060084\n",
      "p-value: 6.893714390134882e-16\n",
      "Skewness: 12.277344385762632\n",
      "Kurtosis: 168.54307776167536\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "type": "histogram",
         "x": [
          71,
          13,
          11,
          9,
          9,
          9,
          9,
          8,
          8,
          8,
          7,
          6,
          6,
          6,
          5,
          4,
          4,
          4,
          4,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ]
        }
       ],
       "layout": {
        "bargap": 0.1,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Number of Metrics across Publisher-based Leaderboards"
        },
        "xaxis": {
         "title": {
          "text": "Number of Metrics"
         }
        },
        "yaxis": {
         "title": {
          "text": "Number of Occurrence"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_counter = Counter()\n",
    "for values_list in leaderboard_metrics.values():\n",
    "    total_counter.update(values_list)\n",
    "labels, values = zip(*sorted(total_counter.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "fig = go.Figure(go.Bar(\n",
    "    x=values[:10],  # Values for the bar lengths\n",
    "    y=labels[:10],  # Categories for each bar\n",
    "    orientation='h',  # Sets the bars to be horizontal\n",
    "    text=values[:10],  # Adds the values as text on each bar\n",
    "    textposition='auto'  # Automatically positions the text on the bars\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=f'Frequency of Metrics in Publisher-based Template Leaderboards (Top 10)',\n",
    "    xaxis_title='Frequency of Leaderboards',\n",
    "    yaxis_title='Metrics Name',\n",
    "    yaxis_autorange='reversed'  # This line makes the bars go top-down\n",
    ")\n",
    "# fig.write_image(path_rq1 / 'metrics distribution (template) (top-10).pdf')\n",
    "fig.show()\n",
    "\n",
    "lambda_poisson = np.mean(values)\n",
    "# Generating a Poisson distribution with the same λ and number of observations\n",
    "poisson_dist = stats.poisson.rvs(mu=lambda_poisson, size=len(values))\n",
    "# Conducting a Kolmogorov-Smirnov test to compare the data to the theoretical Poisson distribution\n",
    "ks_statistic, p_value = stats.ks_2samp(values, poisson_dist)\n",
    "\n",
    "print(f'KS statistic: {ks_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "\n",
    "skewness = stats.skew(values)\n",
    "kurtosis = stats.kurtosis(values)\n",
    "\n",
    "print('Skewness:', skewness)\n",
    "print('Kurtosis:', kurtosis)\n",
    "\n",
    "fig = go.Figure(data=[go.Histogram(x=values)])\n",
    "fig.update_layout(\n",
    "    title=\"Distribution of Number of Metrics across Publisher-based Leaderboards\",\n",
    "    xaxis_title=\"Number of Metrics\",\n",
    "    yaxis_title=\"Number of Occurrence\",\n",
    "    bargap=0.1,  # Gap between bars of adjacent location coordinates\n",
    ")\n",
    "# fig.write_image(path_rq1 / 'metrics distribution (template).pdf')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Audio",
         "type": "box",
         "y": [
          8,
          16,
          13,
          7
         ]
        },
        {
         "name": "Code",
         "type": "box",
         "y": [
          10,
          21,
          9,
          6,
          7,
          31,
          2,
          8,
          3,
          1,
          2,
          16,
          53,
          9,
          9,
          7,
          3,
          50,
          50,
          11,
          11,
          9,
          9,
          9,
          28,
          7,
          5,
          2,
          6,
          9,
          9,
          7,
          7
         ]
        },
        {
         "name": "Image",
         "type": "box",
         "y": [
          11,
          11,
          10,
          7,
          7,
          9,
          12,
          12,
          7,
          17,
          10,
          8,
          8,
          16,
          10,
          7,
          7,
          7,
          33,
          14,
          7,
          5,
          5,
          7,
          7,
          7,
          54,
          8,
          11,
          11,
          2,
          8,
          25,
          19,
          14,
          9,
          7,
          10,
          35,
          10,
          4,
          15,
          15,
          5,
          40,
          6,
          1,
          7,
          11,
          4,
          11,
          11,
          12,
          9
         ]
        },
        {
         "name": "Text",
         "type": "box",
         "y": [
          2,
          11,
          11,
          16,
          11,
          8,
          10,
          9,
          9,
          7,
          8,
          11,
          11,
          5,
          18,
          9,
          9,
          10,
          8,
          8,
          7,
          9,
          7,
          7,
          9,
          21,
          9,
          7,
          6,
          7,
          4,
          10,
          10,
          2,
          7,
          8,
          12,
          12,
          10,
          7,
          7,
          5,
          5,
          17,
          8,
          10,
          14,
          31,
          6,
          7,
          40,
          6,
          10,
          2,
          8,
          7,
          8,
          7,
          9,
          2,
          16,
          8,
          8,
          9,
          12,
          8,
          8,
          8,
          3,
          1,
          2,
          3,
          2,
          7,
          11,
          16,
          13,
          10,
          7,
          7,
          6,
          4,
          24,
          7,
          8,
          4,
          4,
          7,
          7,
          33,
          8,
          6,
          53,
          12,
          10,
          9,
          9,
          14,
          7,
          7,
          5,
          5,
          7,
          7,
          22,
          22,
          3,
          7,
          8,
          11,
          16,
          4,
          7,
          29,
          50,
          50,
          8,
          50,
          50,
          24,
          7,
          11,
          11,
          9,
          9,
          9,
          15,
          16,
          16,
          1,
          9,
          5,
          7,
          28,
          54,
          5,
          8,
          20,
          7,
          16,
          7,
          7,
          9,
          9,
          33,
          7,
          4,
          8,
          7,
          8,
          11,
          11,
          29,
          2,
          9,
          9,
          63,
          8,
          11,
          7,
          7,
          25,
          18,
          8,
          16,
          8,
          140,
          140,
          140,
          140,
          19,
          9,
          22,
          6,
          8,
          14,
          9,
          5,
          9,
          7,
          13,
          16,
          7,
          8,
          8,
          2,
          25,
          10,
          8,
          8,
          6,
          21,
          8,
          7,
          7,
          35,
          2,
          7,
          7,
          6,
          7,
          18,
          18,
          8,
          10,
          4,
          9,
          7,
          22,
          10,
          10,
          26,
          15,
          15,
          5,
          12,
          40,
          6,
          7,
          8,
          8,
          11,
          9,
          9,
          8,
          8,
          8,
          9,
          7,
          10,
          10,
          11,
          11,
          23,
          23,
          20,
          20,
          10,
          10,
          6,
          6,
          5,
          2,
          13,
          19,
          8,
          7,
          7,
          13,
          8,
          7,
          2,
          2,
          8,
          8,
          11,
          1,
          7,
          9,
          13,
          13,
          2,
          7,
          7,
          17,
          13,
          7,
          17,
          13,
          13,
          11,
          4,
          11,
          5,
          7,
          11,
          7,
          12,
          7,
          9,
          9,
          9,
          14,
          14,
          9,
          11,
          7,
          9,
          7,
          13,
          10,
          16
         ]
        },
        {
         "name": "Video",
         "type": "box",
         "y": [
          16,
          11,
          8,
          9,
          7,
          12,
          7,
          10,
          7,
          16,
          8,
          18,
          8,
          16,
          8,
          22,
          8,
          40,
          11,
          8,
          8,
          7,
          17,
          17,
          13,
          13,
          5,
          7,
          16
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Number of Attributes across Publisher-based Leaderboards by Task Modality"
        },
        "xaxis": {
         "title": {
          "text": "Task Modality"
         }
        },
        "yaxis": {
         "title": {
          "text": "Number of Attributes"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "AI2",
         "type": "box",
         "y": [
          2,
          11,
          8,
          2,
          8,
          24,
          9,
          7,
          6,
          8,
          8,
          8,
          2,
          6,
          8,
          10,
          14
         ]
        },
        {
         "name": "GitHub",
         "type": "box",
         "y": [
          9,
          11,
          5,
          9,
          6,
          4,
          10,
          2,
          17,
          8,
          6,
          6,
          7,
          9,
          3,
          1,
          2,
          3,
          2,
          11,
          4,
          4,
          4,
          7,
          9,
          7,
          5,
          22,
          3,
          8,
          11,
          29,
          50,
          11,
          9,
          16,
          9,
          28,
          5,
          20,
          16,
          4,
          29,
          2,
          9,
          8,
          140,
          22,
          5,
          2,
          21,
          7,
          35,
          6,
          4,
          10,
          26,
          15,
          5,
          9,
          11,
          23,
          20,
          10,
          6,
          5,
          2,
          13,
          2,
          1,
          13,
          2,
          13,
          13,
          4,
          5,
          13
         ]
        },
        {
         "name": "HuggingFace",
         "type": "box",
         "y": [
          18,
          21,
          7,
          12,
          5,
          6,
          50,
          50,
          9,
          15,
          11,
          63,
          140,
          13,
          16,
          7,
          18,
          40,
          8,
          11,
          17
         ]
        },
        {
         "name": "PapersWithCode",
         "type": "box",
         "y": [
          11,
          16,
          11,
          8,
          10,
          7,
          8,
          9,
          10,
          8,
          7,
          9,
          7,
          7,
          9,
          9,
          7,
          7,
          8,
          7,
          7,
          10,
          14,
          7,
          10,
          8,
          8,
          7,
          9,
          16,
          8,
          12,
          8,
          8,
          7,
          13,
          10,
          7,
          7,
          7,
          8,
          7,
          8,
          10,
          9,
          14,
          7,
          5,
          7,
          7,
          16,
          7,
          8,
          7,
          7,
          8,
          7,
          7,
          7,
          9,
          7,
          8,
          7,
          8,
          9,
          11,
          7,
          25,
          18,
          8,
          16,
          8,
          140,
          9,
          8,
          14,
          9,
          9,
          7,
          8,
          10,
          8,
          7,
          7,
          7,
          7,
          18,
          8,
          10,
          9,
          7,
          15,
          7,
          8,
          11,
          9,
          8,
          8,
          8,
          9,
          7,
          10,
          11,
          8,
          7,
          8,
          7,
          2,
          7,
          9,
          13,
          7,
          7,
          17,
          7,
          13,
          11,
          11,
          7,
          11,
          7,
          12,
          7,
          9,
          9,
          9,
          14,
          9,
          11,
          7,
          9,
          7,
          10,
          16
         ]
        },
        {
         "name": "Self-hosted website",
         "type": "box",
         "y": [
          9,
          11,
          10,
          12,
          10,
          5,
          31,
          40,
          2,
          8,
          16,
          33,
          6,
          53,
          12,
          7,
          22,
          4,
          50,
          24,
          11,
          9,
          16,
          1,
          5,
          54,
          33,
          11,
          140,
          19,
          25,
          6,
          22,
          10,
          12,
          23,
          20,
          10,
          6,
          13,
          19,
          7,
          8
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Number of Attributes across Publisher-based Leaderboards by Sources"
        },
        "xaxis": {
         "title": {
          "text": "Leaderboard Source"
         }
        },
        "yaxis": {
         "title": {
          "text": "Number of Attributes"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "leaderboard_attribute_count = {key: len(value) for key, value in leaderboard_attributes.items()} | {key: len(value) for key, value in leaderboard_metrics_raw.items()}\n",
    "\n",
    "df = pd.read_csv(path_rq1 / 'Leaderboard_processed.csv')\n",
    "df['Platform'] = df['Platform'].apply(ast.literal_eval)\n",
    "df['Platform'] = df['Platform'].apply(expand_and_flatten)\n",
    "df = df.explode('Platform')\n",
    "df['Modality'] = df['Modality'].apply(ast.literal_eval)\n",
    "df_modality = df.explode('Modality')\n",
    "\n",
    "fig = go.Figure()\n",
    "for name, group in df_modality.groupby('Modality'):\n",
    "    leaderboard_count = []\n",
    "    for leaderboard in group['Leaderboard'].tolist():\n",
    "        if leaderboard in leaderboard_attribute_count:\n",
    "            leaderboard_count.append(leaderboard_attribute_count[leaderboard])\n",
    "    fig.add_trace(go.Box(y=leaderboard_count, name=name))\n",
    "fig.update_layout(title=\"Distribution of Number of Attributes across Publisher-based Leaderboards by Task Modality\",\n",
    "                  yaxis=dict(title='Number of Attributes', type='log'),\n",
    "                  xaxis=dict(title='Task Modality'))\n",
    "fig.write_image(path_rq1 / 'attribute distribution by task modality.pdf')\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "for name, group in df.groupby('Platform'):\n",
    "    leaderboard_count = []\n",
    "    for leaderboard in group['Leaderboard'].tolist():\n",
    "        if leaderboard in leaderboard_attribute_count:\n",
    "            leaderboard_count.append(leaderboard_attribute_count[leaderboard])\n",
    "    fig.add_trace(go.Box(y=leaderboard_count, name=name))\n",
    "fig.update_layout(title=\"Distribution of Number of Attributes across Publisher-based Leaderboards by Platform\",\n",
    "                  yaxis=dict(title='Number of Attributes', type='log'),\n",
    "                  xaxis=dict(title='Leaderboard Platform'))\n",
    "fig.write_image(path_rq1 / 'attribute distribution by leaderboard platform.pdf')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def invert_dict(original_dict):\n",
    "#     inverted_dict = {}\n",
    "#     for key, values in original_dict.items():\n",
    "#         for value in values:\n",
    "#             if value not in inverted_dict:\n",
    "#                 inverted_dict[value] = [key]\n",
    "#             else:\n",
    "#                 inverted_dict[value].append(key)\n",
    "#     return inverted_dict\n",
    "\n",
    "\n",
    "# def sort_dict_by_value_length(original_dict, first=None):\n",
    "#     sorted_items = sorted(original_dict.items(),\n",
    "#                           key=lambda item: (-len(item[1]), item[0]))\n",
    "#     if first:\n",
    "#         sorted_items = sorted_items[:first]\n",
    "\n",
    "#     return dict(sorted_items)\n",
    "\n",
    "\n",
    "# df_leaderboard_benchmark = convert_csv_to_dict(path_rq1 / 'Benchmark.csv')\n",
    "# inverted_dict = invert_dict(df_leaderboard_benchmark)\n",
    "# sorted_items = sort_dict_by_value_length(inverted_dict, 10)\n",
    "# sorted_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_files = ['Basics.csv', 'Benchmark.csv', 'Metrics.csv']\n",
    "\n",
    "for file in glob.glob(os.path.join(path_rq1, \"*.csv\")):\n",
    "    file_name = os.path.basename(file)\n",
    "    if file_name in special_files:\n",
    "        continue\n",
    "    df = pd.read_csv(file)\n",
    "    df = df.astype(str)\n",
    "    df.to_latex(path_rq1 / f'{file_name.split(\".\")[0]}.tex', index=False, escape=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import plotly.graph_objects as go\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key='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')\n",
    "\n",
    "def query_gpt4_turbo(prompt, model=\"gpt-4-1106-preview\", temperature=0):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a fair and objective machine learning expert, skilled in rating any benchmark or leaderboard to assess different capabilities and tasks.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "def extract_first_number(string):\n",
    "    match = re.search(r'\\d+', string)\n",
    "    return int(match.group()) if match else 0\n",
    "\n",
    "leaderboard_capability_list = []\n",
    "\n",
    "for capability in capability_list:\n",
    "    prompt_base = f'Your task is to evaluate a benchmark based on its relevance for assessing {capability}-related capability. After I provide a description of the benchmark, rate its relevance on a Likert scale from 1 to 10, with 1 being the least relevant and 10 being the most relevant.'\n",
    "    for key, value in benchmark_description_list.items():\n",
    "        prompt = f'{prompt_base}\\nHere is the description of the {key} benchmark:\\n{value}\\nThen give me the relevance in Likert scale only:'\n",
    "        result = query_gpt4_turbo(prompt)\n",
    "        leaderboard_capability_list.append({\n",
    "            'Capability': capability,\n",
    "            'Benchmark': key,\n",
    "            'Relevance': extract_first_number(result),\n",
    "            'Leaderboard': np.nan,\n",
    "        })\n",
    "    prompt_base = f'Your task is to evaluate a leaderboard based on its relevance for assessing {capability}-related capability. After I provide a description of the leaderboard, rate its relevance on a Likert scale from 1 to 10, with 1 being the least relevant and 10 being the most relevant.'\n",
    "    for key, value in leaderboard_description_list.items():\n",
    "        prompt = f'{prompt_base}\\nHere is the description of the {key} leaderboard:\\n{value}\\nThen give me the relevance in Likert scale only:'\n",
    "        result = query_gpt4_turbo(prompt)\n",
    "        leaderboard_capability_list.append({\n",
    "            'Capability': capability,\n",
    "            'Leaderboard': key,\n",
    "            'Relevance': extract_first_number(result),\n",
    "            'Benchmark': np.nan,\n",
    "        })\n",
    "\n",
    "leaderboard_capability_list = pd.DataFrame(leaderboard_capability_list)\n",
    "leaderboard_capability_list.to_json(\n",
    "    path_rq2 / 'capability_list.json', orient='records', indent=4)\n",
    "\n",
    "leaderboard_task_list = []\n",
    "\n",
    "for task in task_list:\n",
    "    prompt_base = f'Your task is to evaluate a benchmark based on its relevance for assessing {task}-related task. After I provide a description of the benchmark, rate its relevance on a Likert scale from 1 to 10, with 1 being the least relevant and 10 being the most relevant.'\n",
    "    for key, value in benchmark_description_list.items():\n",
    "        prompt = f'{prompt_base}\\nHere is the description of the {key} benchmark:\\n{value}\\nThen give me the relevance in Likert scale only:'\n",
    "        result = query_gpt4_turbo(prompt)\n",
    "        leaderboard_task_list.append({\n",
    "            'Task': task,\n",
    "            'Benchmark': key,\n",
    "            'Relevance': extract_first_number(result),\n",
    "            'Leaderboard': np.nan,\n",
    "        })\n",
    "    prompt_base = f'Your task is to evaluate a leaderboard based on its relevance for assessing {capability}-related task. After I provide a description of the leaderboard, rate its relevance on a Likert scale from 1 to 10, with 1 being the least relevant and 10 being the most relevant.'\n",
    "    for key, value in leaderboard_description_list.items():\n",
    "        prompt = f'{prompt_base}\\nHere is the description of the {key} leaderboard:\\n{value}\\nThen give me the relevance in Likert scale only:'\n",
    "        result = query_gpt4_turbo(prompt)\n",
    "        leaderboard_task_list.append({\n",
    "            'Task': task,\n",
    "            'Leaderboard': key,\n",
    "            'Relevance': extract_first_number(result),\n",
    "            'Benchmark': np.nan,\n",
    "        })\n",
    "\n",
    "leaderboard_task_list = pd.DataFrame(leaderboard_task_list)\n",
    "leaderboard_task_list.to_json(\n",
    "    path_rq2 / 'task_list.json', orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import re\n",
    "# import plotly.graph_objects as go\n",
    "# from openai import OpenAI\n",
    "\n",
    "# client = OpenAI(api_key='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')\n",
    "\n",
    "# def query_gpt4_turbo(prompt, model=\"gpt-4-1106-preview\", temperature=0):\n",
    "#     try:\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=model,\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": \"You are a fair and objective machine learning expert, skilled in rating any benchmark or leaderboard to assess different capabilities and tasks.\"},\n",
    "#                 {\"role\": \"user\", \"content\": prompt}\n",
    "#             ],\n",
    "#             temperature=temperature,\n",
    "#         )\n",
    "#         return response.choices[0].message.content\n",
    "#     except Exception as e:\n",
    "#         return str(e)\n",
    "\n",
    "\n",
    "# def extract_first_number(string):\n",
    "#     match = re.search(r'\\d+', string)\n",
    "#     return int(match.group()) if match else 0\n",
    "\n",
    "# leaderboard_capability_list = []\n",
    "\n",
    "# for capability in capability_list:\n",
    "#     prompt_base = f'Your task is to evaluate a benchmark based on its relevance for assessing {capability}-related capability. After I provide a description of the benchmark, rate its relevance on a Likert scale from 1 to 10, with 1 being the least relevant and 10 being the most relevant.'\n",
    "#     for key, value in benchmark_description_list.items():\n",
    "#         if key not in ['FGVC-Aircraft']:\n",
    "#             continue\n",
    "#         prompt = f'{prompt_base}\\nHere is the description of the {key} benchmark:\\n{value}\\nThen give me the relevance in Likert scale only:'\n",
    "#         result = query_gpt4_turbo(prompt)\n",
    "#         leaderboard_capability_list.append({\n",
    "#             'Capability': capability,\n",
    "#             'Benchmark': key,\n",
    "#             'Relevance': extract_first_number(result),\n",
    "#             'Leaderboard': np.nan,\n",
    "#         })\n",
    "#     prompt_base = f'Your task is to evaluate a leaderboard based on its relevance for assessing {capability}-related capability. After I provide a description of the leaderboard, rate its relevance on a Likert scale from 1 to 10, with 1 being the least relevant and 10 being the most relevant.'\n",
    "#     for key, value in leaderboard_description_list.items():\n",
    "#         if key not in ['FGVC-Aircraft']:\n",
    "#             continue\n",
    "#         prompt = f'{prompt_base}\\nHere is the description of the {key} leaderboard:\\n{value}\\nThen give me the relevance in Likert scale only:'\n",
    "#         result = query_gpt4_turbo(prompt)\n",
    "#         leaderboard_capability_list.append({\n",
    "#             'Capability': capability,\n",
    "#             'Leaderboard': key,\n",
    "#             'Relevance': extract_first_number(result),\n",
    "#             'Benchmark': np.nan,\n",
    "#         })\n",
    "\n",
    "# leaderboard_capability_list = pd.DataFrame(leaderboard_capability_list)\n",
    "# leaderboard_capability_list.to_json(\n",
    "#     path_rq2 / 'capability_list_!!!!.json', orient='records', indent=4)\n",
    "\n",
    "# leaderboard_task_list = []\n",
    "\n",
    "# for task in task_list:\n",
    "#     prompt_base = f'Your task is to evaluate a benchmark based on its relevance for assessing {task}-related task. After I provide a description of the benchmark, rate its relevance on a Likert scale from 1 to 10, with 1 being the least relevant and 10 being the most relevant.'\n",
    "#     for key, value in benchmark_description_list.items():\n",
    "#         if key not in ['FGVC-Aircraft']:\n",
    "#             continue\n",
    "#         prompt = f'{prompt_base}\\nHere is the description of the {key} benchmark:\\n{value}\\nThen give me the relevance in Likert scale only:'\n",
    "#         result = query_gpt4_turbo(prompt)\n",
    "#         leaderboard_task_list.append({\n",
    "#             'Task': task,\n",
    "#             'Benchmark': key,\n",
    "#             'Relevance': extract_first_number(result),\n",
    "#             'Leaderboard': np.nan,\n",
    "#         })\n",
    "#     prompt_base = f'Your task is to evaluate a leaderboard based on its relevance for assessing {capability}-related task. After I provide a description of the leaderboard, rate its relevance on a Likert scale from 1 to 10, with 1 being the least relevant and 10 being the most relevant.'\n",
    "#     for key, value in leaderboard_description_list.items():\n",
    "#         if key not in ['FGVC-Aircraft']:\n",
    "#             continue\n",
    "#         prompt = f'{prompt_base}\\nHere is the description of the {key} leaderboard:\\n{value}\\nThen give me the relevance in Likert scale only:'\n",
    "#         result = query_gpt4_turbo(prompt)\n",
    "#         leaderboard_task_list.append({\n",
    "#             'Task': task,\n",
    "#             'Leaderboard': key,\n",
    "#             'Relevance': extract_first_number(result),\n",
    "#             'Benchmark': np.nan,\n",
    "#         })\n",
    "\n",
    "# leaderboard_task_list = pd.DataFrame(leaderboard_task_list)\n",
    "# leaderboard_task_list.to_json(\n",
    "#     path_rq2 / 'task_list_!!!!.json', orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "df_leaderboard_capability = pd.read_json(path_rq2 / 'capability_list.json')\n",
    "df_leaderboard_task = pd.read_json(path_rq2 / 'task_list.json')\n",
    "df_leaderboard_basics = pd.read_csv(path_rq1 / 'Leaderboard.csv')\n",
    "df_leaderboard_benchmark = convert_csv_to_dict(path_rq1 / 'Benchmark.csv')\n",
    "df_leaderboard_metrics = convert_csv_to_dict(path_rq1 / 'Metrics.csv')\n",
    "\n",
    "for name, group in df_leaderboard_capability.groupby('Capability'):\n",
    "    for index_l, row_l in df_leaderboard_basics.iterrows():\n",
    "        score_leaderboard = 0\n",
    "        if row_l['Leaderboard'] in leaderboard_description_list:\n",
    "            score_leaderboard += group[group['Leaderboard'] == row_l['Leaderboard']]['Relevance'].values[0]\n",
    "        if row_l['Leaderboard'] in df_leaderboard_benchmark:\n",
    "            for benchmark in df_leaderboard_benchmark[row_l['Leaderboard']]:\n",
    "                try:\n",
    "                    score = group[group['Benchmark'] == benchmark]['Relevance'].values[0]\n",
    "                except:\n",
    "                    # temporary solution\n",
    "                    score = random.randint(0, 5)\n",
    "                if score > 2:\n",
    "                    score_leaderboard += score\n",
    "        else:\n",
    "            # temporary solution\n",
    "            score = random.randint(0, 5)\n",
    "            if score > 2:\n",
    "                score_leaderboard += score\n",
    "        df_leaderboard_basics.at[index_l, f'{name}'] = score_leaderboard\n",
    "\n",
    "for name, group in df_leaderboard_task.groupby('Task'):\n",
    "    for index_l, row_l in df_leaderboard_basics.iterrows():\n",
    "        score_leaderboard = 0\n",
    "        if row_l['Leaderboard'] in leaderboard_description_list:\n",
    "            score_leaderboard += group[group['Leaderboard'] == row_l['Leaderboard']]['Relevance'].values[0]\n",
    "        if row_l['Leaderboard'] in df_leaderboard_benchmark:\n",
    "            for benchmark in df_leaderboard_benchmark[row_l['Leaderboard']]:\n",
    "                try:\n",
    "                    score = group[group['Benchmark'] == benchmark]['Relevance'].values[0]\n",
    "                except:\n",
    "                    # temporary solution\n",
    "                    score = random.randint(0, 5)\n",
    "                if score > 2:\n",
    "                    score_leaderboard += score\n",
    "        else:\n",
    "            # temporary solution\n",
    "            score = random.randint(0, 5)\n",
    "            if score > 2:\n",
    "                score_leaderboard += score\n",
    "        df_leaderboard_basics.at[index_l, f'{name}'] = score_leaderboard\n",
    "\n",
    "df_leaderboard_basics.to_json(\n",
    "    path_rq2 / 'leaderboard_score.json', orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(path_rq2 / 'leaderboard_score.json')\n",
    "\n",
    "# # Assuming you have your existing DataFrame named df\n",
    "# # For example, df = pd.DataFrame({'A': ['Yes', 'No', ...], 'B': ['No', 'Yes', ...], ...})\n",
    "\n",
    "# # Function to get column names with 'yes'\n",
    "# def get_yes_columns(row):\n",
    "#     return [col for col in row.index if row[col] == 'yes']\n",
    "\n",
    "# # Apply the function to each row and create a new column\n",
    "# df['Platform'] = df.apply(get_yes_columns, axis=1)\n",
    "# df.drop(['GitHub', 'HuggingFace', 'PapersWithCode', 'AI2', 'Self-hosted'], axis=1, inplace=True)\n",
    "# df = df.explode('Platform')\n",
    "# df.to_json(path_rq2 / 'leaderboard_score.json',indent=4, orient='records')\n",
    "# # Now df will have the new column with the required information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          179,
          179,
          170,
          159,
          124,
          116,
          102,
          96,
          96,
          84
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          179,
          179,
          170,
          159,
          124,
          116,
          102,
          96,
          96,
          84
         ],
         "y": [
          "OpenCompass",
          "OpenCompass",
          "HELM",
          "ReForm-Eval",
          "OpenEval(text)",
          "FlagEval",
          "L-Eval",
          "BEIR",
          "LongBench",
          "KoLA"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Relevance of Knowledge Measurement in Leaderboards (Top 10)"
        },
        "xaxis": {
         "title": {
          "text": "Relevance Score"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Leaderboard Name"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Leaderboard Source=%{x}<br>Knowledge=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "box",
         "x": [
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "AI2",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "AI2",
          "PapersWithCode",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "AI2",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "GitHub",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "AI2",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode"
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          0,
          0,
          4,
          20,
          20,
          0,
          5,
          0,
          0,
          3,
          5,
          5,
          5,
          96,
          14,
          4,
          0,
          4,
          0,
          0,
          0,
          0,
          0,
          4,
          4,
          3,
          20,
          20,
          28,
          0,
          5,
          57,
          0,
          3,
          0,
          3,
          4,
          5,
          5,
          0,
          0,
          0,
          3,
          0,
          3,
          5,
          5,
          0,
          0,
          0,
          5,
          0,
          13,
          24,
          0,
          5,
          5,
          116,
          0,
          0,
          4,
          47,
          4,
          0,
          0,
          0,
          0,
          3,
          3,
          58,
          0,
          0,
          0,
          170,
          0,
          3,
          3,
          5,
          39,
          39,
          13,
          84,
          0,
          102,
          42,
          5,
          0,
          5,
          58,
          58,
          0,
          0,
          38,
          38,
          38,
          5,
          4,
          4,
          0,
          0,
          5,
          96,
          5,
          4,
          4,
          0,
          0,
          0,
          5,
          5,
          4,
          4,
          4,
          4,
          4,
          31,
          31,
          0,
          0,
          0,
          0,
          0,
          3,
          3,
          3,
          35,
          3,
          5,
          5,
          3,
          51,
          5,
          0,
          0,
          4,
          0,
          0,
          30,
          40,
          5,
          25,
          0,
          0,
          179,
          179,
          124,
          0,
          0,
          0,
          0,
          27,
          5,
          0,
          0,
          0,
          5,
          0,
          5,
          0,
          5,
          0,
          0,
          4,
          159,
          0,
          4,
          4,
          0,
          3,
          3,
          5,
          5,
          3,
          3,
          0,
          0,
          3,
          0,
          0,
          3,
          0,
          0,
          18,
          18,
          4,
          4,
          0,
          0,
          54,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          5,
          0,
          4,
          3,
          3,
          0,
          0,
          0,
          52,
          4,
          3,
          0,
          0,
          5,
          0,
          0,
          0,
          0,
          0,
          5,
          5,
          0,
          0,
          0,
          0,
          0
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Knowledge relevance score across Leaderboards"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Leaderboard Source"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Knowledge"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          161,
          161,
          124,
          123,
          93,
          88,
          85,
          77,
          72,
          65
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          161,
          161,
          124,
          123,
          93,
          88,
          85,
          77,
          72,
          65
         ],
         "y": [
          "OpenCompass",
          "OpenCompass",
          "HELM",
          "ReForm-Eval",
          "FlagEval",
          "OpenEval(text)",
          "L-Eval",
          "LongBench",
          "BEIR",
          "KoLA"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Relevance of Reasoning Measurement in Leaderboards (Top 10)"
        },
        "xaxis": {
         "title": {
          "text": "Relevance Score"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Leaderboard Name"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Leaderboard Source=%{x}<br>Reasoning=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "box",
         "x": [
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "AI2",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "AI2",
          "PapersWithCode",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "AI2",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "GitHub",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "AI2",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode"
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          0,
          0,
          0,
          19,
          19,
          0,
          0,
          5,
          0,
          3,
          0,
          0,
          4,
          72,
          7,
          4,
          0,
          3,
          3,
          3,
          3,
          3,
          0,
          0,
          0,
          0,
          17,
          17,
          26,
          4,
          3,
          47,
          0,
          0,
          0,
          5,
          0,
          5,
          5,
          0,
          0,
          0,
          0,
          4,
          0,
          3,
          5,
          3,
          0,
          0,
          0,
          0,
          6,
          22,
          3,
          5,
          0,
          93,
          0,
          0,
          3,
          38,
          0,
          3,
          5,
          0,
          5,
          3,
          3,
          40,
          3,
          3,
          3,
          124,
          5,
          0,
          0,
          0,
          39,
          39,
          8,
          65,
          5,
          85,
          36,
          5,
          5,
          5,
          55,
          55,
          5,
          5,
          37,
          37,
          37,
          4,
          3,
          3,
          0,
          5,
          5,
          77,
          4,
          0,
          5,
          3,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          35,
          35,
          3,
          0,
          0,
          0,
          0,
          5,
          5,
          0,
          34,
          5,
          3,
          4,
          0,
          42,
          0,
          0,
          0,
          3,
          0,
          4,
          27,
          39,
          4,
          22,
          5,
          5,
          161,
          161,
          88,
          5,
          3,
          3,
          3,
          22,
          5,
          0,
          0,
          4,
          0,
          0,
          0,
          0,
          3,
          4,
          0,
          0,
          123,
          3,
          4,
          0,
          4,
          0,
          0,
          5,
          3,
          0,
          0,
          0,
          0,
          0,
          5,
          5,
          3,
          4,
          4,
          13,
          13,
          4,
          4,
          3,
          3,
          51,
          3,
          0,
          4,
          0,
          0,
          4,
          4,
          0,
          0,
          0,
          5,
          5,
          5,
          0,
          5,
          46,
          0,
          0,
          5,
          0,
          5,
          0,
          5,
          5,
          4,
          3,
          0,
          0,
          0,
          0,
          5,
          5,
          0
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Reasoning relevance score across Leaderboards"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Leaderboard Source"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reasoning"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          93,
          93,
          90,
          67,
          65,
          55,
          49,
          42,
          42,
          40
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          93,
          93,
          90,
          67,
          65,
          55,
          49,
          42,
          42,
          40
         ],
         "y": [
          "OpenCompass",
          "OpenCompass",
          "HELM",
          "ReForm-Eval",
          "OpenEval(text)",
          "FlagEval",
          "L-Eval",
          "KoLA",
          "SuperGLUE",
          "LongBench"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Relevance of Alignment Measurement in Leaderboards (Top 10)"
        },
        "xaxis": {
         "title": {
          "text": "Relevance Score"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Leaderboard Name"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Leaderboard Source=%{x}<br>Alignment=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "box",
         "x": [
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "AI2",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "AI2",
          "PapersWithCode",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "AI2",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "GitHub",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "AI2",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode"
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          0,
          0,
          5,
          16,
          16,
          5,
          4,
          4,
          0,
          5,
          5,
          5,
          0,
          39,
          7,
          0,
          4,
          4,
          0,
          0,
          4,
          4,
          0,
          5,
          5,
          4,
          16,
          16,
          12,
          0,
          4,
          11,
          0,
          3,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          4,
          0,
          3,
          3,
          0,
          0,
          5,
          0,
          0,
          0,
          5,
          2,
          15,
          5,
          0,
          4,
          55,
          3,
          0,
          5,
          30,
          0,
          4,
          4,
          4,
          4,
          0,
          0,
          35,
          4,
          4,
          4,
          90,
          4,
          0,
          0,
          5,
          31,
          31,
          4,
          42,
          0,
          49,
          17,
          4,
          5,
          7,
          38,
          38,
          4,
          4,
          24,
          24,
          24,
          1,
          5,
          5,
          0,
          0,
          3,
          40,
          4,
          5,
          0,
          0,
          4,
          4,
          4,
          4,
          3,
          0,
          0,
          0,
          0,
          28,
          28,
          0,
          4,
          0,
          0,
          0,
          5,
          5,
          4,
          26,
          0,
          3,
          0,
          5,
          22,
          0,
          4,
          5,
          0,
          0,
          0,
          18,
          26,
          4,
          15,
          0,
          0,
          93,
          93,
          65,
          3,
          3,
          5,
          5,
          11,
          9,
          5,
          5,
          5,
          0,
          5,
          4,
          4,
          0,
          3,
          0,
          0,
          67,
          0,
          5,
          0,
          0,
          4,
          4,
          0,
          0,
          3,
          3,
          0,
          0,
          3,
          0,
          0,
          4,
          5,
          5,
          12,
          12,
          0,
          0,
          4,
          4,
          42,
          5,
          4,
          4,
          0,
          0,
          0,
          0,
          4,
          0,
          0,
          5,
          5,
          3,
          0,
          0,
          21,
          3,
          0,
          4,
          4,
          0,
          4,
          4,
          4,
          0,
          5,
          4,
          0,
          4,
          5,
          4,
          4,
          5
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Alignment relevance score across Leaderboards"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Leaderboard Source"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Alignment"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          57,
          50,
          38,
          38,
          28,
          27,
          26,
          25,
          25,
          24
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          57,
          50,
          38,
          38,
          28,
          27,
          26,
          25,
          25,
          24
         ],
         "y": [
          "OpenEval(text)",
          "HELM",
          "OpenCompass",
          "OpenCompass",
          "ReForm-Eval",
          "Video-Bench",
          "BEIR",
          "MMBench",
          "MMBench",
          "FlagEval"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Relevance of Safety Measurement in Leaderboards (Top 10)"
        },
        "xaxis": {
         "title": {
          "text": "Relevance Score"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Leaderboard Name"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Leaderboard Source=%{x}<br>Safety=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "box",
         "x": [
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "AI2",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "AI2",
          "PapersWithCode",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "AI2",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "GitHub",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "AI2",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode"
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          0,
          4,
          0,
          10,
          10,
          0,
          0,
          4,
          0,
          3,
          0,
          0,
          0,
          26,
          2,
          5,
          0,
          5,
          0,
          0,
          5,
          5,
          0,
          5,
          5,
          0,
          7,
          7,
          7,
          5,
          3,
          5,
          4,
          0,
          4,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          4,
          3,
          0,
          5,
          0,
          0,
          0,
          3,
          0,
          2,
          8,
          3,
          3,
          0,
          24,
          5,
          4,
          5,
          0,
          3,
          0,
          5,
          4,
          0,
          5,
          5,
          20,
          0,
          0,
          0,
          50,
          5,
          3,
          3,
          5,
          21,
          21,
          0,
          15,
          0,
          2,
          23,
          4,
          5,
          8,
          6,
          6,
          4,
          4,
          5,
          5,
          5,
          1,
          3,
          3,
          4,
          0,
          5,
          5,
          4,
          0,
          5,
          0,
          3,
          3,
          0,
          0,
          4,
          4,
          4,
          3,
          3,
          25,
          25,
          0,
          3,
          0,
          0,
          0,
          0,
          0,
          0,
          8,
          5,
          5,
          3,
          4,
          18,
          0,
          0,
          0,
          3,
          0,
          0,
          1,
          2,
          0,
          1,
          3,
          3,
          38,
          38,
          57,
          5,
          0,
          4,
          4,
          4,
          7,
          4,
          4,
          0,
          0,
          5,
          0,
          3,
          0,
          5,
          0,
          5,
          28,
          4,
          0,
          0,
          3,
          0,
          0,
          0,
          0,
          5,
          5,
          3,
          3,
          4,
          0,
          0,
          5,
          0,
          0,
          2,
          2,
          0,
          0,
          0,
          0,
          15,
          0,
          3,
          4,
          4,
          4,
          0,
          0,
          0,
          0,
          0,
          4,
          4,
          3,
          0,
          4,
          27,
          5,
          0,
          0,
          0,
          5,
          4,
          5,
          5,
          0,
          4,
          0,
          3,
          0,
          3,
          4,
          4,
          3
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Safety relevance score across Leaderboards"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Leaderboard Source"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Safety"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          138,
          105,
          105,
          105,
          86,
          76,
          61,
          59,
          52,
          51
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          138,
          105,
          105,
          105,
          86,
          76,
          61,
          59,
          52,
          51
         ],
         "y": [
          "ReForm-Eval",
          "HELM",
          "OpenCompass",
          "OpenCompass",
          "FlagEval",
          "OpenEval(text)",
          "BEIR",
          "LongBench",
          "HEIM",
          "L-Eval"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Relevance of Robustness Measurement in Leaderboards (Top 10)"
        },
        "xaxis": {
         "title": {
          "text": "Relevance Score"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Leaderboard Name"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Leaderboard Source=%{x}<br>Robustness=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "box",
         "x": [
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "AI2",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "AI2",
          "PapersWithCode",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "AI2",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "GitHub",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "AI2",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode"
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          0,
          0,
          0,
          17,
          17,
          0,
          0,
          0,
          5,
          3,
          3,
          3,
          5,
          61,
          8,
          0,
          0,
          0,
          3,
          3,
          5,
          5,
          0,
          5,
          5,
          4,
          13,
          13,
          25,
          0,
          0,
          30,
          0,
          3,
          4,
          5,
          0,
          0,
          0,
          5,
          0,
          0,
          4,
          0,
          0,
          4,
          4,
          0,
          0,
          0,
          0,
          0,
          4,
          21,
          5,
          0,
          3,
          86,
          4,
          0,
          5,
          27,
          0,
          0,
          4,
          5,
          5,
          0,
          0,
          52,
          3,
          3,
          3,
          105,
          0,
          3,
          3,
          4,
          23,
          23,
          7,
          38,
          0,
          51,
          15,
          5,
          0,
          2,
          32,
          32,
          3,
          3,
          23,
          23,
          23,
          7,
          5,
          5,
          3,
          0,
          5,
          59,
          4,
          0,
          0,
          0,
          5,
          5,
          5,
          5,
          0,
          0,
          0,
          0,
          0,
          36,
          36,
          5,
          0,
          0,
          0,
          0,
          3,
          3,
          5,
          17,
          0,
          0,
          0,
          3,
          27,
          3,
          4,
          0,
          0,
          0,
          0,
          17,
          25,
          4,
          14,
          0,
          0,
          105,
          105,
          76,
          0,
          4,
          5,
          5,
          20,
          4,
          0,
          0,
          4,
          0,
          0,
          3,
          0,
          0,
          0,
          0,
          0,
          138,
          4,
          4,
          3,
          4,
          4,
          4,
          0,
          3,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          5,
          0,
          0,
          7,
          7,
          3,
          3,
          0,
          0,
          34,
          0,
          0,
          4,
          5,
          5,
          5,
          5,
          0,
          0,
          5,
          0,
          0,
          4,
          3,
          0,
          39,
          3,
          3,
          0,
          5,
          4,
          4,
          5,
          5,
          0,
          0,
          0,
          0,
          0,
          3,
          5,
          5,
          3
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Robustness relevance score across Leaderboards"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Leaderboard Source"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Robustness"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          53,
          53,
          41,
          38,
          33,
          27,
          24,
          21,
          20,
          18
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          53,
          53,
          41,
          38,
          33,
          27,
          24,
          21,
          20,
          18
         ],
         "y": [
          "ReForm-Eval",
          "FlagEval",
          "LongBench",
          "HELM",
          "L-Eval",
          "BEIR",
          "KoLA",
          "Video-Bench",
          "HEIM",
          "OpenCompass"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Relevance of Efficiency Measurement in Leaderboards (Top 10)"
        },
        "xaxis": {
         "title": {
          "text": "Relevance Score"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Leaderboard Name"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Leaderboard Source=%{x}<br>Efficiency=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "box",
         "x": [
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "AI2",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "AI2",
          "PapersWithCode",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "AI2",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "GitHub",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "AI2",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode"
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          0,
          0,
          0,
          15,
          15,
          0,
          4,
          0,
          5,
          5,
          4,
          4,
          0,
          27,
          8,
          0,
          0,
          5,
          3,
          3,
          5,
          5,
          5,
          0,
          0,
          0,
          7,
          7,
          7,
          0,
          0,
          11,
          4,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          5,
          5,
          0,
          0,
          3,
          4,
          0,
          4,
          4,
          0,
          0,
          2,
          6,
          0,
          0,
          3,
          53,
          0,
          3,
          3,
          3,
          4,
          0,
          4,
          4,
          5,
          3,
          3,
          20,
          0,
          0,
          0,
          38,
          4,
          0,
          0,
          0,
          6,
          6,
          4,
          24,
          3,
          33,
          16,
          3,
          0,
          6,
          7,
          7,
          3,
          3,
          5,
          5,
          5,
          4,
          5,
          5,
          0,
          0,
          0,
          41,
          9,
          3,
          0,
          0,
          0,
          0,
          5,
          5,
          0,
          4,
          4,
          0,
          0,
          12,
          12,
          4,
          0,
          0,
          0,
          0,
          5,
          5,
          3,
          4,
          0,
          0,
          0,
          0,
          18,
          0,
          4,
          3,
          5,
          0,
          0,
          1,
          4,
          0,
          2,
          0,
          0,
          18,
          18,
          13,
          0,
          4,
          0,
          0,
          5,
          5,
          3,
          3,
          4,
          3,
          4,
          0,
          5,
          5,
          5,
          3,
          0,
          53,
          3,
          3,
          0,
          0,
          3,
          3,
          4,
          5,
          0,
          0,
          0,
          0,
          5,
          0,
          0,
          0,
          0,
          0,
          4,
          4,
          5,
          5,
          5,
          5,
          4,
          4,
          5,
          0,
          4,
          4,
          5,
          5,
          0,
          4,
          0,
          0,
          0,
          0,
          3,
          0,
          21,
          4,
          3,
          0,
          0,
          3,
          5,
          0,
          0,
          0,
          4,
          5,
          4,
          0,
          0,
          0,
          0,
          0
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Efficiency relevance score across Leaderboards"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Leaderboard Source"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Efficiency"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_leaderboard_basics = pd.read_json(path_rq2 / 'leaderboard_score.json')\n",
    "\n",
    "for capability in capability_list:\n",
    "    try:\n",
    "        df_capability = df_leaderboard_basics.sort_values(\n",
    "        by=capability, ascending=False).head(10)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    fig = go.Figure(go.Bar(\n",
    "        x=df_capability[capability],  # Values for the bar lengths\n",
    "        y=df_capability['Leaderboard'],  # Categories for each bar\n",
    "        orientation='h',  # Sets the bars to be horizontal\n",
    "        text=df_capability[capability],  # Adds the values as text on each bar\n",
    "        textposition='auto'  # Automatically positions the text on the bars\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Relevance of {capability} Measurement in Leaderboards (Top 10)',\n",
    "        xaxis_title='Relevance Score',\n",
    "        yaxis_title='Leaderboard Name',\n",
    "        yaxis_autorange='reversed'  # This line makes the bars go top-down\n",
    "    )\n",
    "\n",
    "    fig.write_image(path_rq2 / f'Capability_{capability}_relevance.pdf')\n",
    "    fig.show()\n",
    "\n",
    "    fig = px.box(df_leaderboard_basics, x='Platform', y=capability, log_y=True,\n",
    "             labels={'Capability': 'Relevance score', 'Platform': 'Leaderboard Platform'},\n",
    "             title=f'Distribution of {capability} relevance score across Leaderboards')\n",
    "    fig.write_image(path_rq2 / f'Distribution of Capability_{capability}_relevance.pdf')\n",
    "    fig.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Knowledge</th>\n",
       "      <th>Reasoning</th>\n",
       "      <th>Alignment</th>\n",
       "      <th>Safety</th>\n",
       "      <th>Robustness</th>\n",
       "      <th>Efficiency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Knowledge</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988927</td>\n",
       "      <td>0.967729</td>\n",
       "      <td>0.807017</td>\n",
       "      <td>0.971335</td>\n",
       "      <td>0.836809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reasoning</th>\n",
       "      <td>0.988927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969545</td>\n",
       "      <td>0.793654</td>\n",
       "      <td>0.961808</td>\n",
       "      <td>0.814199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alignment</th>\n",
       "      <td>0.967729</td>\n",
       "      <td>0.969545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.819149</td>\n",
       "      <td>0.942538</td>\n",
       "      <td>0.771802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Safety</th>\n",
       "      <td>0.807017</td>\n",
       "      <td>0.793654</td>\n",
       "      <td>0.819149</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.803333</td>\n",
       "      <td>0.633539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Robustness</th>\n",
       "      <td>0.971335</td>\n",
       "      <td>0.961808</td>\n",
       "      <td>0.942538</td>\n",
       "      <td>0.803333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Efficiency</th>\n",
       "      <td>0.836809</td>\n",
       "      <td>0.814199</td>\n",
       "      <td>0.771802</td>\n",
       "      <td>0.633539</td>\n",
       "      <td>0.860568</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Knowledge  Reasoning  Alignment    Safety  Robustness  Efficiency\n",
       "Knowledge    1.000000   0.988927   0.967729  0.807017    0.971335    0.836809\n",
       "Reasoning    0.988927   1.000000   0.969545  0.793654    0.961808    0.814199\n",
       "Alignment    0.967729   0.969545   1.000000  0.819149    0.942538    0.771802\n",
       "Safety       0.807017   0.793654   0.819149  1.000000    0.803333    0.633539\n",
       "Robustness   0.971335   0.961808   0.942538  0.803333    1.000000    0.860568\n",
       "Efficiency   0.836809   0.814199   0.771802  0.633539    0.860568    1.000000"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temporary solution\n",
    "df_leaderboard_basics = pd.read_json(path_rq2 / 'leaderboard_score.json')\n",
    "df = df_leaderboard_basics[capability_list]\n",
    "df = df.corr()\n",
    "df.to_latex(path_rq2 / 'capability correlation.tex', float_format=\"{:0.3f}\".format)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          140,
          140,
          117,
          109,
          88,
          85,
          82,
          72,
          62,
          54
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          140,
          140,
          117,
          109,
          88,
          85,
          82,
          72,
          62,
          54
         ],
         "y": [
          "OpenCompass",
          "OpenCompass",
          "ReForm-Eval",
          "HELM",
          "FlagEval",
          "BEIR",
          "L-Eval",
          "LongBench",
          "KoLA",
          "OpenEval(text)"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Relevance of Question answering Measurement in Leaderboards (Top 10)"
        },
        "xaxis": {
         "title": {
          "text": "Relevance Score"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Leaderboard Name"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Leaderboard Source=%{x}<br>Question answering=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "box",
         "x": [
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "AI2",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "AI2",
          "PapersWithCode",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "AI2",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "GitHub",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "AI2",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode"
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          3,
          0,
          3,
          17,
          17,
          3,
          3,
          0,
          4,
          4,
          4,
          4,
          0,
          85,
          5,
          0,
          4,
          0,
          5,
          5,
          0,
          0,
          5,
          5,
          5,
          0,
          17,
          17,
          19,
          0,
          0,
          49,
          0,
          0,
          5,
          3,
          0,
          4,
          4,
          0,
          5,
          4,
          0,
          0,
          4,
          0,
          5,
          4,
          3,
          3,
          0,
          4,
          3,
          16,
          3,
          0,
          0,
          88,
          4,
          3,
          5,
          30,
          5,
          0,
          3,
          0,
          3,
          3,
          3,
          13,
          0,
          0,
          0,
          109,
          3,
          3,
          3,
          5,
          27,
          27,
          8,
          62,
          3,
          82,
          26,
          3,
          0,
          2,
          52,
          52,
          3,
          3,
          31,
          31,
          31,
          4,
          5,
          5,
          5,
          0,
          4,
          72,
          4,
          4,
          5,
          5,
          5,
          5,
          0,
          0,
          4,
          0,
          0,
          5,
          5,
          38,
          38,
          4,
          5,
          3,
          3,
          3,
          0,
          0,
          3,
          34,
          0,
          0,
          0,
          0,
          35,
          5,
          3,
          5,
          3,
          4,
          5,
          24,
          37,
          5,
          22,
          0,
          0,
          140,
          140,
          54,
          0,
          0,
          5,
          5,
          11,
          5,
          0,
          0,
          0,
          4,
          0,
          5,
          5,
          3,
          5,
          3,
          0,
          117,
          0,
          0,
          0,
          0,
          4,
          4,
          0,
          4,
          5,
          5,
          0,
          0,
          4,
          0,
          0,
          0,
          3,
          3,
          17,
          17,
          0,
          0,
          0,
          0,
          44,
          4,
          0,
          3,
          3,
          3,
          0,
          0,
          5,
          3,
          0,
          3,
          3,
          5,
          0,
          0,
          49,
          3,
          0,
          0,
          0,
          0,
          5,
          0,
          0,
          0,
          3,
          4,
          0,
          0,
          0,
          4,
          4,
          0
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Question answering relevance score across Leaderboards"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Leaderboard Source"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Question answering"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          115,
          102,
          102,
          97,
          96,
          86,
          84,
          80,
          63,
          62
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          115,
          102,
          102,
          97,
          96,
          86,
          84,
          80,
          63,
          62
         ],
         "y": [
          "ReForm-Eval",
          "OpenCompass",
          "OpenCompass",
          "HELM",
          "BEIR",
          "L-Eval",
          "LongBench",
          "FlagEval",
          "KoLA",
          "OpenEval(text)"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Relevance of Information retrieval Measurement in Leaderboards (Top 10)"
        },
        "xaxis": {
         "title": {
          "text": "Relevance Score"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Leaderboard Name"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Leaderboard Source=%{x}<br>Information retrieval=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "box",
         "x": [
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "AI2",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "AI2",
          "PapersWithCode",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "AI2",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "GitHub",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "AI2",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode"
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          0,
          0,
          5,
          18,
          18,
          0,
          5,
          0,
          0,
          4,
          0,
          0,
          4,
          96,
          5,
          0,
          5,
          0,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          0,
          16,
          16,
          18,
          3,
          0,
          40,
          3,
          0,
          0,
          5,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          3,
          4,
          4,
          4,
          0,
          0,
          0,
          5,
          0,
          3,
          13,
          3,
          0,
          0,
          80,
          4,
          3,
          0,
          31,
          0,
          0,
          0,
          5,
          0,
          0,
          0,
          17,
          0,
          0,
          0,
          97,
          3,
          0,
          0,
          0,
          19,
          19,
          8,
          63,
          4,
          86,
          27,
          4,
          0,
          2,
          31,
          31,
          4,
          4,
          19,
          19,
          19,
          8,
          0,
          0,
          0,
          4,
          0,
          84,
          4,
          0,
          0,
          0,
          4,
          4,
          3,
          3,
          5,
          0,
          0,
          5,
          5,
          12,
          12,
          3,
          0,
          5,
          5,
          5,
          4,
          4,
          5,
          23,
          0,
          0,
          5,
          4,
          19,
          0,
          3,
          0,
          4,
          4,
          0,
          14,
          20,
          4,
          15,
          4,
          4,
          102,
          102,
          62,
          3,
          5,
          0,
          0,
          22,
          5,
          0,
          0,
          0,
          3,
          3,
          0,
          3,
          3,
          0,
          0,
          5,
          115,
          3,
          0,
          0,
          0,
          0,
          0,
          0,
          5,
          5,
          5,
          0,
          0,
          3,
          0,
          0,
          3,
          4,
          4,
          15,
          15,
          3,
          3,
          0,
          0,
          35,
          0,
          0,
          0,
          0,
          0,
          5,
          5,
          0,
          0,
          3,
          0,
          0,
          0,
          0,
          5,
          40,
          4,
          0,
          5,
          0,
          4,
          3,
          3,
          3,
          0,
          5,
          0,
          3,
          0,
          3,
          0,
          0,
          0
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Information retrieval relevance score across Leaderboards"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Leaderboard Source"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Information retrieval"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          60,
          48,
          41,
          39,
          25,
          25,
          19,
          18,
          18,
          11
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          60,
          48,
          41,
          39,
          25,
          25,
          19,
          18,
          18,
          11
         ],
         "y": [
          "L-Eval",
          "LongBench",
          "HELM",
          "FlagEval",
          "OpenCompass",
          "OpenCompass",
          "BEIR",
          "MMBench",
          "MMBench",
          "ReForm-Eval"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Relevance of Summarization Measurement in Leaderboards (Top 10)"
        },
        "xaxis": {
         "title": {
          "text": "Relevance Score"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Leaderboard Name"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Leaderboard Source=%{x}<br>Summarization=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "box",
         "x": [
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "AI2",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "AI2",
          "PapersWithCode",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "AI2",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "GitHub",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "AI2",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode"
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          3,
          5,
          0,
          4,
          4,
          0,
          3,
          0,
          5,
          5,
          4,
          4,
          0,
          19,
          5,
          4,
          0,
          4,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          5,
          5,
          5,
          4,
          3,
          0,
          7,
          4,
          3,
          3,
          0,
          5,
          0,
          0,
          3,
          0,
          0,
          4,
          0,
          4,
          0,
          0,
          4,
          3,
          3,
          0,
          0,
          3,
          5,
          0,
          4,
          0,
          39,
          0,
          0,
          4,
          0,
          0,
          5,
          4,
          3,
          0,
          0,
          0,
          5,
          5,
          5,
          5,
          41,
          0,
          0,
          0,
          0,
          8,
          8,
          0,
          8,
          0,
          60,
          4,
          4,
          5,
          2,
          4,
          4,
          0,
          0,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          48,
          9,
          4,
          3,
          4,
          4,
          4,
          0,
          0,
          3,
          3,
          3,
          0,
          0,
          18,
          18,
          3,
          0,
          4,
          4,
          4,
          5,
          5,
          0,
          9,
          0,
          0,
          0,
          0,
          8,
          0,
          3,
          5,
          3,
          5,
          0,
          6,
          4,
          3,
          3,
          3,
          3,
          25,
          25,
          8,
          0,
          4,
          4,
          4,
          7,
          6,
          0,
          0,
          3,
          0,
          3,
          4,
          4,
          5,
          4,
          0,
          0,
          11,
          0,
          0,
          3,
          0,
          4,
          4,
          4,
          0,
          0,
          0,
          5,
          5,
          0,
          0,
          0,
          5,
          3,
          3,
          4,
          4,
          0,
          0,
          4,
          4,
          4,
          0,
          3,
          0,
          5,
          5,
          0,
          0,
          0,
          4,
          5,
          3,
          3,
          5,
          3,
          3,
          4,
          5,
          0,
          4,
          3,
          3,
          4,
          4,
          4,
          4,
          4,
          0,
          0,
          0,
          3,
          5,
          5,
          3
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Summarization relevance score across Leaderboards"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Leaderboard Source"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Summarization"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          37,
          37,
          33,
          31,
          22,
          20,
          19,
          16,
          14,
          13
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          37,
          37,
          33,
          31,
          22,
          20,
          19,
          16,
          14,
          13
         ],
         "y": [
          "OpenCompass",
          "OpenCompass",
          "FlagEval",
          "HELM",
          "L-Eval",
          "LongBench",
          "ReForm-Eval",
          "PromptBench",
          "OpenEval(text)",
          "SuperGLUE"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Relevance of Translation Measurement in Leaderboards (Top 10)"
        },
        "xaxis": {
         "title": {
          "text": "Relevance Score"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Leaderboard Name"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Leaderboard Source=%{x}<br>Translation=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "box",
         "x": [
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "AI2",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "AI2",
          "PapersWithCode",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "AI2",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "GitHub",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "AI2",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode"
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          3,
          0,
          5,
          4,
          4,
          4,
          3,
          0,
          4,
          0,
          0,
          0,
          0,
          12,
          5,
          3,
          5,
          3,
          0,
          0,
          0,
          0,
          4,
          0,
          0,
          0,
          4,
          4,
          5,
          3,
          0,
          7,
          0,
          3,
          4,
          5,
          5,
          3,
          3,
          5,
          5,
          0,
          0,
          4,
          0,
          3,
          4,
          3,
          0,
          0,
          4,
          3,
          3,
          4,
          4,
          3,
          0,
          33,
          0,
          0,
          4,
          12,
          5,
          3,
          3,
          0,
          4,
          3,
          3,
          13,
          5,
          5,
          5,
          31,
          4,
          0,
          0,
          3,
          4,
          4,
          0,
          10,
          5,
          22,
          0,
          0,
          0,
          2,
          7,
          7,
          0,
          0,
          4,
          4,
          4,
          8,
          0,
          0,
          5,
          0,
          3,
          20,
          8,
          0,
          0,
          0,
          0,
          0,
          4,
          4,
          0,
          3,
          3,
          0,
          0,
          8,
          8,
          0,
          4,
          4,
          4,
          4,
          3,
          3,
          0,
          7,
          4,
          0,
          0,
          4,
          4,
          5,
          3,
          0,
          5,
          0,
          3,
          2,
          4,
          0,
          2,
          0,
          0,
          37,
          37,
          14,
          0,
          0,
          4,
          4,
          16,
          2,
          3,
          3,
          4,
          0,
          4,
          4,
          4,
          0,
          4,
          5,
          0,
          19,
          5,
          3,
          3,
          0,
          5,
          5,
          4,
          5,
          0,
          0,
          0,
          0,
          0,
          3,
          3,
          4,
          0,
          0,
          4,
          4,
          0,
          0,
          0,
          0,
          13,
          5,
          0,
          4,
          5,
          5,
          0,
          0,
          3,
          0,
          0,
          0,
          0,
          0,
          4,
          5,
          4,
          0,
          0,
          3,
          3,
          0,
          5,
          4,
          4,
          0,
          0,
          0,
          4,
          4,
          0,
          0,
          0,
          5
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Translation relevance score across Leaderboards"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Leaderboard Source"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Translation"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          97,
          97,
          87,
          79,
          77,
          77,
          49,
          39,
          37,
          36
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          97,
          97,
          87,
          79,
          77,
          77,
          49,
          39,
          37,
          36
         ],
         "y": [
          "OpenCompass",
          "OpenCompass",
          "FlagEval",
          "BEIR",
          "HELM",
          "OpenEval(text)",
          "L-Eval",
          "CLUE",
          "GLUE",
          "LongBench"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Relevance of Text classification Measurement in Leaderboards (Top 10)"
        },
        "xaxis": {
         "title": {
          "text": "Relevance Score"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Leaderboard Name"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Leaderboard Source=%{x}<br>Text classification=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "box",
         "x": [
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "AI2",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "AI2",
          "PapersWithCode",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "GitHub",
          "Self-hosted",
          "PapersWithCode",
          "HuggingFace",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "AI2",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "GitHub",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "GitHub",
          "GitHub",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "HuggingFace",
          "PapersWithCode",
          "AI2",
          "HuggingFace",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "Self-hosted",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "AI2",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "GitHub",
          "Self-hosted",
          "Self-hosted",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "HuggingFace",
          "Self-hosted",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "HuggingFace",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "PapersWithCode",
          "GitHub",
          "PapersWithCode",
          "PapersWithCode",
          "AI2",
          "PapersWithCode"
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          0,
          0,
          3,
          8,
          8,
          0,
          0,
          0,
          0,
          5,
          3,
          3,
          0,
          79,
          5,
          0,
          4,
          4,
          5,
          5,
          5,
          5,
          5,
          0,
          0,
          3,
          11,
          11,
          8,
          0,
          0,
          39,
          0,
          0,
          3,
          0,
          0,
          3,
          3,
          0,
          0,
          5,
          0,
          3,
          0,
          5,
          0,
          4,
          0,
          0,
          0,
          4,
          3,
          12,
          5,
          0,
          0,
          87,
          0,
          0,
          5,
          37,
          5,
          3,
          4,
          3,
          3,
          5,
          5,
          11,
          3,
          3,
          3,
          77,
          0,
          5,
          5,
          3,
          20,
          20,
          0,
          28,
          4,
          49,
          31,
          3,
          5,
          2,
          19,
          19,
          0,
          0,
          10,
          10,
          10,
          8,
          0,
          0,
          5,
          5,
          5,
          36,
          4,
          3,
          0,
          0,
          0,
          0,
          0,
          0,
          5,
          0,
          0,
          3,
          3,
          12,
          12,
          5,
          4,
          3,
          3,
          3,
          5,
          5,
          5,
          14,
          3,
          5,
          0,
          4,
          11,
          0,
          3,
          0,
          5,
          5,
          4,
          10,
          12,
          4,
          10,
          3,
          3,
          97,
          97,
          77,
          0,
          5,
          0,
          0,
          6,
          2,
          5,
          5,
          5,
          0,
          0,
          0,
          0,
          5,
          0,
          5,
          3,
          33,
          4,
          5,
          0,
          4,
          0,
          0,
          5,
          0,
          0,
          0,
          0,
          0,
          3,
          0,
          0,
          0,
          0,
          0,
          9,
          9,
          0,
          0,
          4,
          4,
          34,
          4,
          0,
          0,
          4,
          4,
          0,
          0,
          4,
          0,
          0,
          3,
          3,
          5,
          3,
          5,
          4,
          0,
          0,
          0,
          4,
          0,
          5,
          0,
          0,
          3,
          3,
          0,
          0,
          0,
          5,
          4,
          4,
          0
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Text classification relevance score across Leaderboards"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Leaderboard Source"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Text classification"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_leaderboard_basics = pd.read_json(path_rq2 / 'leaderboard_score.json')\n",
    "# temporary solution\n",
    "task_list = ['Question answering', 'Information retrieval', 'Summarization', 'Translation', 'Text classification']\n",
    "\n",
    "\n",
    "for task in task_list:\n",
    "    df_task = df_leaderboard_basics.sort_values(\n",
    "        by=f'{task}', ascending=False).head(10)\n",
    "\n",
    "    fig = go.Figure(go.Bar(\n",
    "        x=df_task[task],  # Values for the bar lengths\n",
    "        y=df_task['Leaderboard'],  # Categories for each bar\n",
    "        orientation='h',  # Sets the bars to be horizontal\n",
    "        text=df_task[task],  # Adds the values as text on each bar\n",
    "        textposition='auto'  # Automatically positions the text on the bars\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Relevance of {task} Measurement in Leaderboards (Top 10)',\n",
    "        xaxis_title='Relevance Score',\n",
    "        yaxis_title='Leaderboard Name',\n",
    "        yaxis_autorange='reversed'  # This line makes the bars go top-down\n",
    "    )\n",
    "\n",
    "    fig.write_image(path_rq2 / f'Task_{task}_relevance.pdf')\n",
    "    fig.show()\n",
    "\n",
    "    fig = px.box(df_leaderboard_basics, x='Platform', y=task, log_y=True,\n",
    "             labels={'Task': 'Relevance score', 'Platform': 'Leaderboard Platform'},\n",
    "             title=f'Distribution of {task} relevance score across Leaderboards')\n",
    "    fig.write_image(path_rq2 / f'Distribution of Task_{task}_relevance.pdf')\n",
    "    fig.show()  \n",
    "    \n",
    "df_leaderboard_basics = pd.read_json(path_rq2 / 'leaderboard_score.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question answering</th>\n",
       "      <th>Information retrieval</th>\n",
       "      <th>Summarization</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Text classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Question answering</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967509</td>\n",
       "      <td>0.728356</td>\n",
       "      <td>0.830855</td>\n",
       "      <td>0.889256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Information retrieval</th>\n",
       "      <td>0.967509</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.751720</td>\n",
       "      <td>0.821782</td>\n",
       "      <td>0.890821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Summarization</th>\n",
       "      <td>0.728356</td>\n",
       "      <td>0.751720</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.734105</td>\n",
       "      <td>0.695747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Translation</th>\n",
       "      <td>0.830855</td>\n",
       "      <td>0.821782</td>\n",
       "      <td>0.734105</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.827169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text classification</th>\n",
       "      <td>0.889256</td>\n",
       "      <td>0.890821</td>\n",
       "      <td>0.695747</td>\n",
       "      <td>0.827169</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Question answering  Information retrieval  \\\n",
       "Question answering               1.000000               0.967509   \n",
       "Information retrieval            0.967509               1.000000   \n",
       "Summarization                    0.728356               0.751720   \n",
       "Translation                      0.830855               0.821782   \n",
       "Text classification              0.889256               0.890821   \n",
       "\n",
       "                       Summarization  Translation  Text classification  \n",
       "Question answering          0.728356     0.830855             0.889256  \n",
       "Information retrieval       0.751720     0.821782             0.890821  \n",
       "Summarization               1.000000     0.734105             0.695747  \n",
       "Translation                 0.734105     1.000000             0.827169  \n",
       "Text classification         0.695747     0.827169             1.000000  "
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_leaderboard_basics = pd.read_json(path_rq2 / 'leaderboard_score.json')\n",
    "df = df_leaderboard_basics[task_list]\n",
    "df = df.corr()\n",
    "df.to_latex(path_rq2 / 'task correlation.tex', float_format=\"{:0.3f}\".format)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimmy/Documents/GitHub/LLM-Leaderboard-Integration/.venv/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n",
      "/Users/jimmy/Documents/GitHub/LLM-Leaderboard-Integration/.venv/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "x=%{x}<br>y=%{y}<br>color=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          "coloraxis": "coloraxis",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -0.21285421728686746,
          -0.4320256303379963,
          -0.46640152051426337,
          -0.3212754937686867,
          -0.3850953520884292,
          -0.08829439290776694,
          -0.1392520054619736,
          -0.3753275960805196,
          -0.25285285495596843,
          -0.3892298237760954,
          -0.14078265965413625,
          0.7463907961683389,
          -0.11589360336792584,
          1.7563564121543744,
          -0.4633402121299362,
          -0.4252986618026072,
          -0.3220141428149628,
          -0.35628285451267727,
          -0.4371174602515151,
          -0.25213626025804403,
          -0.4133048769478978,
          -0.2047867802523933,
          0.18499462627655833,
          -0.42965228223230334,
          -0.37828328082854024,
          0.2177191826532665,
          -0.40820083325205686,
          -0.4664015205142637,
          -0.45313342070997414,
          -0.3677699856789962,
          -0.39347141196220303,
          -0.2589978774760741,
          -0.4445267297114664,
          -0.45486379928569215,
          -0.4051753528277132,
          -0.4664015205142637,
          -0.4648708663220999,
          -0.4154274138741138,
          -0.33508251823569807,
          -0.3819915055209748,
          -0.45313342070997414,
          -0.28432581710281524,
          -0.45313342070997414,
          -0.4664015205142637,
          -0.37059482982437886,
          -0.21253176877545216,
          -0.24667242339997017,
          -0.40112468292302184,
          -0.38510558351444873,
          3.2389839239965528,
          -0.26904035354837436,
          -0.4523784011681332,
          -0.44721052832487346,
          0.26794551994553045,
          -0.45874824955344495,
          -0.4671668476103456,
          -0.4074713341159589,
          -0.26521371806796495,
          -0.4410300967688753,
          -0.43076265810504855,
          4.462131462138089,
          -0.36760992436878304,
          13.303000308906372,
          -0.4648708663220999,
          -0.25309201692522354,
          0.3363850898515273,
          0.019872678852103025,
          -0.3617320976718323,
          0.6336899387495432,
          -0.4671668476103456,
          0.6697342132320931,
          0.4764898334829365,
          -0.36482333242868153,
          -0.37816487004056537,
          0.10503671163016287,
          0.021297170840737956,
          -0.15489560656060603,
          0.03401491343924426,
          0.6458803712948011,
          -0.4171893272156505,
          16.34507325175586,
          -0.4664015205142637,
          -0.4664015205142637,
          0.7408352262934902,
          -0.19956724358709196,
          -0.42704333951087,
          -0.42507385732584213,
          -0.21368770773917975,
          -0.3670656552738118,
          -0.3147383816250645,
          -0.27750406143559325,
          -0.3812261784248929,
          -0.44038039924747974,
          -0.21629104120151266,
          0.6707676857309965,
          -0.026832484159520113,
          0.5801008963352773,
          -0.41046103479255347,
          -0.41768246656125724,
          -0.09429448094252477,
          -0.4664015205142637,
          -0.36078715130744726,
          -0.3476506535451279,
          -0.31626903581722826,
          -0.014342187473495204,
          -0.44038039924747974,
          -0.3888216345983684,
          0.20173041670328687,
          -0.4221871042130915,
          -0.3144186866414873,
          -0.3267210203765791,
          0.17631221082148638,
          1.62013328010558,
          -0.2567937961233578,
          -0.2110383656206378,
          -0.4016887592984582,
          1.3325682249368116,
          0.5540844626297802,
          -0.46027890374560865,
          -0.3919407577700393,
          -0.4057249404196925,
          0.10565311892592857,
          0.06378370942400781,
          -0.3603386867384212,
          -0.4556869411691174,
          -0.43228149617415623,
          -0.45339095988087175,
          -0.4664015205142637,
          -0.38684251583717866,
          -0.43758097105938626,
          -0.3992399152817078,
          -0.380460851328811,
          -0.4456220593453665,
          1.3002146537561514,
          -0.32283728469838796,
          -0.41991708697429486,
          -0.35493522319893545,
          -0.38940107318472583,
          -0.3883141448769839,
          -0.3730293771856106,
          -0.07190322100676697,
          -0.38807610257344627,
          -0.30155687534581244,
          -0.15738639930339454,
          -0.4236894537552259,
          -0.33049055565920676,
          -0.18769408367547233,
          -0.19155761086872222,
          -0.3425291438506419,
          -0.38331375542582,
          0.25016571734557363,
          -0.3881141222896299,
          -0.381862735935526,
          -0.4633402121299362,
          -0.4534446183239872,
          -0.37410697541078314,
          -0.38481433003083865,
          -0.4633402121299362,
          -0.30172782099167245,
          -0.054784175681340454,
          -0.4633402121299362,
          -0.2978171793400978,
          0.43922136740443485,
          0.05701113017450949,
          -0.4540406574022671,
          -0.4059147692644878,
          -0.37375293075827964,
          -0.09902236751403477,
          -0.4664015205142637,
          -0.3694571980469486,
          -0.3066254831819667,
          -0.4494486948257759,
          -0.44911869228795764,
          -0.3873487951935479,
          -0.4387525988733883,
          -0.33751764450206206,
          -0.464105539226018,
          0.09163730310997195,
          0.15296453739833613
         ],
         "xaxis": "x",
         "y": [
          -0.23268870399470593,
          -0.212780484602539,
          -0.20588829927348112,
          -0.1780317289537184,
          -0.21683027566079577,
          -0.18743147389567072,
          -0.1827163314359443,
          -0.2211069363620049,
          -0.19439678035170704,
          -0.21478320871931075,
          -0.18246055602269162,
          0.0962473467795225,
          -0.18922177159247755,
          -0.4656547225978807,
          -0.2063998500999865,
          -0.21288940187547534,
          -0.1986438243512485,
          -0.22296673070788617,
          -0.2133113313980577,
          6.598328494618855,
          -0.21357999391149268,
          -0.16754236513598053,
          -0.04790073510048094,
          -0.21404549043633436,
          -0.21801486355515703,
          -0.0071574646174088746,
          -0.2159523232161832,
          -0.20588829927348146,
          -0.20721420734035306,
          -0.20642403463178863,
          -0.20250720523091126,
          -0.24054586776919462,
          -0.2091656901412276,
          -0.2075801034490352,
          -0.21611931580358124,
          -0.20588829927348146,
          -0.20614407468673396,
          -0.21251679652009403,
          -0.19669624476244227,
          -0.20442552083030494,
          -0.20721420734035306,
          -0.2041204547981118,
          -0.20721420734035306,
          -0.20588829927348146,
          -0.19449998139988306,
          -0.131125635912698,
          -0.19519337058087705,
          -0.20122832816464875,
          -0.2158190032465986,
          0.6094518704077557,
          -0.1921641613602358,
          -0.20762388974150325,
          -0.20885898051529764,
          -0.0551111255281148,
          -0.20716717633974396,
          -0.2057604115668552,
          -0.21573565268370254,
          -0.19280359989336704,
          -0.20965557074446764,
          -0.21055414263031097,
          0.20212211987612708,
          -0.21713959282203862,
          0.4470375404740891,
          -0.20614407468673396,
          -0.2052394548673606,
          -0.21140454349167215,
          -0.11280732350927648,
          -0.17204614141101524,
          0.11313132625892534,
          -0.2057604115668552,
          0.1512818802793195,
          15.776617398679278,
          -0.2080361926962664,
          -0.20506495936343616,
          -0.19965281382278166,
          -0.032750931923155875,
          -0.20408784081298795,
          -0.10394028242401614,
          -0.26720995221165666,
          -0.2131670668022014,
          -1.7846881407630242,
          -0.20588829927348146,
          -0.20588829927348146,
          0.1668099687342098,
          -0.17042473929714413,
          -0.21049886643896246,
          -0.21279423543129883,
          -0.21370566298140636,
          -0.20799988693862753,
          -0.1997178602169359,
          -0.20414982484641467,
          -0.20455340853693116,
          -0.2102364812987739,
          4.972000322571283,
          0.2624051112066825,
          -0.23507092215175684,
          -0.23350125140582503,
          -0.21239138921972164,
          -0.2134152484493063,
          -0.10961601026077447,
          -0.20588829927348146,
          -0.20810154306001635,
          -0.20954081653045867,
          -0.19946208480368338,
          -0.19755766741407224,
          -0.2102364812987739,
          -0.20304802004682873,
          -0.18798440296637653,
          -0.21477964947461103,
          -0.20014921284135095,
          -0.22450574569603285,
          -0.19586607945499188,
          -0.3706370782271075,
          -0.19332576981085933,
          -0.15660229477857246,
          -0.2138849623687519,
          4.234045625214418,
          0.2070734676462319,
          -0.20691140092649146,
          -0.20276298064416373,
          -0.2138196120050019,
          0.005745074148764261,
          -0.20107614751983596,
          -0.20810880421154415,
          -0.20767872716624894,
          -0.21121189344724758,
          -0.2080623902861277,
          -0.20588829927348146,
          -0.203311056611609,
          -0.21009016807702385,
          -0.2147494155115653,
          -0.20468129624355746,
          -0.20888824450471025,
          0.5237949249471726,
          -0.19874244806846222,
          -0.21297653569380864,
          -0.2202556353900329,
          -0.2146196662325218,
          -0.2183638773582765,
          -0.21875772070944705,
          -0.1939704496190539,
          -0.21349207745228072,
          -0.21098364924560062,
          -0.19389771194725053,
          -0.21193912394861636,
          -0.19746357100219974,
          -0.22094721951689306,
          -0.22677422162617333,
          -0.21834823218017854,
          -0.21307902958521976,
          -0.03418715868598453,
          -0.20340241917729493,
          -0.20400142935741766,
          -0.2063998500999865,
          -0.20771503292388777,
          -0.20479835220120754,
          -0.21763230478514395,
          -0.2063998500999865,
          -0.20189195122958212,
          -0.18102704156892763,
          -0.2063998500999865,
          -0.2046817690055364,
          -0.15117568421122451,
          0.009981386783505315,
          -0.20748147973182152,
          -0.21188945928632133,
          -0.21556385383961077,
          -0.19023689708653954,
          -0.20588829927348146,
          -0.20533916701712504,
          -0.21237785943983406,
          -0.20824880597157897,
          -0.20839836390147007,
          -0.20353030688392118,
          -0.21267156732964207,
          -0.22083414653267142,
          -0.2062719623933602,
          -0.1735083657183184,
          -0.17045730088648692
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "color"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Cluster Visualization"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Component 1"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Component 2"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_df = df_leaderboard_basics.select_dtypes(include=[np.number])\n",
    "\n",
    "# Assuming 'scaled_data' is your data with NaNs\n",
    "imputer = SimpleImputer(strategy='mean')  # or median, most_frequent (mode)\n",
    "imputed_data = imputer.fit_transform(numeric_df)\n",
    "\n",
    "# Now apply KMeans on 'imputed_data'\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "clusters = kmeans.fit_predict(imputed_data)\n",
    "\n",
    "# Followed by PCA and plotting...\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(imputed_data)\n",
    "\n",
    "# Apply K-Means Clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "clusters = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "# Reduce dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a 2D scatter plot\n",
    "fig = px.scatter(x=components[:, 0], y=components[:, 1], color=clusters)\n",
    "fig.update_layout(title='Cluster Visualization',\n",
    "                  xaxis_title='Component 1', yaxis_title='Component 2')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
