{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimmy/Documents/GitHub/LLM-Leaderboard-Integration/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path(\"../data\")\n",
    "path_llm = path_data / \"llm\"\n",
    "path_lvlm = path_data / \"lvlm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://felixz-open-llm-leaderboard.hf.space/ ✔\n"
     ]
    }
   ],
   "source": [
    "client = Client(\"https://felixz-open-llm-leaderboard.hf.space/\")\n",
    "json_data = client.predict(\"\",\"\", api_name='/predict')\n",
    "\n",
    "with open(json_data, 'r') as file:\n",
    "    file_data = file.read()\n",
    "    data = json.loads(file_data)\n",
    "    df = pd.DataFrame(data['data'], columns=data['headers'])\n",
    "    df.drop(columns=['Model'], inplace=True)\n",
    "    df.to_json(path_llm / 'HuggingFace-Open-llm-leaderboard-20231116.json', orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path_llm / 'a.csv')\n",
    "df.to_json(path_llm / 'Sherlock.json', orient='records', indent=4)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Text-to-Text Generation', 'Text-to-Image Generation', 'Text-to-Code Generation', 'Text-to-Audio Generation', 'Text-to-Video Generation', 'Image-to-Text Generation', 'Code-to-Text Generation', 'Audio-to-Text Generation', 'Video-to-Text Generation', 'Image-to-Image Generation', 'Code-to-Code Generation', 'Audio-to-Audio Generation', 'Video-to-Video Generation',\n",
    "\n",
    "'Code Summarization', 'Code Review', 'Identifier Prediction', 'Defect Detection', 'Clone Detection', 'Code Classification', 'Code Reasoning', 'Document Translation', 'Log Parsing',\n",
    "\n",
    "# 'Image Captioning', 'Sign Language Recognition', 'Emotion Recognition', 'Video Processing', 'Digital Human', 'Multimodality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [\n",
    "    \"Natural Language Understanding\",\n",
    "    \"Natural Language Inference\",\n",
    "    \"Question Answering\",\n",
    "    \"Summarization\",\n",
    "    \"Translation\",\n",
    "    \"Sentiment Analysis\",\n",
    "    \"Dialogue\",\n",
    "    \"Text Classification\",\n",
    "    \"Information Retrieval\",\n",
    "    \"Knowledge Completion\",\n",
    "    \"Relation Extraction\",\n",
    "    \"Ethics and Morality\",\n",
    "    \"Societal Bias\",\n",
    "    \"Toxicity\",\n",
    "    \"Fairness\",\n",
    "    \"Hallucination\",\n",
    "    \"Calibration\",\n",
    "    \"Efficiency\",\n",
    "    \"Sequence Tagging\",\n",
    "    \"Coreference Resolution\",\n",
    "    \"Fact Extraction\",\n",
    "    \"Multilinguality\",\n",
    "    \"Hate Detection\",\n",
    "    \"Recommendation\",\n",
    "    \"Healthcare\",\n",
    "    \"Education\",\n",
    "    \"Honestness\",\n",
    "    \"Helpfulness\",\n",
    "    \"Harmlessness\",\n",
    "    \"Robustness\",\n",
    "    \"Quality\",\n",
    "    \"Alignment\",\n",
    "    \"Aesthetics\",\n",
    "    \"Originality\",\n",
    "    \"Fidelity\",\n",
    "    \"Risk\",\n",
    "    \"Law\",\n",
    "    \"Mathematics\",\n",
    "    \"Social Science\",\n",
    "    \"Natural Science\",\n",
    "    \"Finance\",\n",
    "    \"Engineering\",\n",
    "    \"Truthfulness\",\n",
    "    \"Code Reasoning\",\n",
    "    \"Commonsense Reasoning\",\n",
    "    \"Knowledge Reasoning\",\n",
    "    \"Multi-hop Reasoning\",\n",
    "    \"Logical Reasoning\",\n",
    "    \"Arithmetic Reasoning\",\n",
    "    \"Symbolic Reasoning\",\n",
    "    \"Attribute Reasoning\",\n",
    "    \"Relation Reasoning\",\n",
    "    \"Tool Creation\",\n",
    "    \"Tool Manipulation\",\n",
    "    \"Robotic Tasks\",\n",
    "    \"Code Executor\",\n",
    "    \"Calculator\",\n",
    "    \"Search Engine\",\n",
    "    \"Online Shopping\",\n",
    "    \"Personality Testing\",\n",
    "    \"Crowd-sourcing Testing\",\n",
    "    \"Human-in-the-loop\",\n",
    "]\n",
    "\n",
    "labels2 = [\n",
    "    \"Visual Perception\",\n",
    "    \"Visual Reasoning\",\n",
    "    'Visual Knowledge Acquisition',\n",
    "    \"Visual Commonsense\",\n",
    "    \"Object Hallucination\",\n",
    "    \"Embodied Intelligence\",\n",
    "]\n",
    "\n",
    "# \"Open Book\",\n",
    "# \"Closed Book\",\n",
    "# 'Single Choice',\n",
    "# 'Multiple Choice',\n",
    "\n",
    "benchmark_lvlm_labels = {\n",
    "    'A-OKVQA': [\n",
    "        \"Perception\",\n",
    "        \"Existence\",\n",
    "        \"Position\",\n",
    "        \"Color\"\n",
    "    ],\n",
    "    'ALFWorld': [\n",
    "        \"Perception\",\n",
    "        \"Existence\",\n",
    "        \"Position\",\n",
    "        \"Color\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "benchmark_llm_labels = {\n",
    "    '2WikiMultihopQA': [\n",
    "        \"Question Answering\",\n",
    "        \"Multi-hop Reasoning\",\n",
    "        \"Information Retrieval\",\n",
    "        \"Knowledge Reasoning\",\n",
    "        \"Commonsense Reasoning\",\n",
    "        \"Natural Language Inference\",\n",
    "        \"Relation Extraction\"\n",
    "    ],\n",
    "    'A-OKVQA': [\n",
    "        \"Question Answering\",\n",
    "        \"Commonsense Reasoning\",\n",
    "        \"Knowledge Reasoning\",\n",
    "        \"Relation Reasoning\"\n",
    "    ],\n",
    "    'AFQMC': [\n",
    "        \"Natural Language Understanding\",\n",
    "        \"Text Classification\",\n",
    "        \"Natural Language Inference\",\n",
    "        \"Commonsense Reasoning\",\n",
    "        \"Finance\"\n",
    "    ],\n",
    "    'AGIEval': [\n",
    "        \"Natural Language Understanding\",\n",
    "        \"Question Answering\",\n",
    "        \"Text Classification\",\n",
    "        \"Knowledge Reasoning\",\n",
    "        \"Logical Reasoning\",\n",
    "        \"Arithmetic Reasoning\",\n",
    "        \"Multi-hop Reasoning\"\n",
    "    ],\n",
    "    'ALFWorld': [\n",
    "        \"Knowledge Reasoning\",\n",
    "        \"Relation Reasoning\",\n",
    "        \"Tool Manipulation\",\n",
    "        \"Robotic Tasks\",\n",
    "        \"Commonsense Reasoning\"\n",
    "    ],\n",
    "    'AlpacaEval': [\n",
    "        \"Natural Language Understanding\",\n",
    "        \"Dialogue\",\n",
    "        \"Text Classification\",\n",
    "        \"Information Retrieval\",\n",
    "        \"Knowledge Completion\",\n",
    "        \"Helpfulness\",\n",
    "        \"Robustness\",\n",
    "        \"Quality\"\n",
    "    ],\n",
    "    'Amazon Review': [\n",
    "        \"Sentiment Analysis\",\n",
    "        \"Text Classification\",\n",
    "        \"Information Retrieval\",\n",
    "        \"Recommendation\",\n",
    "        \"Summarization\",\n",
    "        \"Helpfulness\"\n",
    "    ],\n",
    "    'ANGO': [\n",
    "        \n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values with hyperlinks written to b.xlsx\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook, Workbook\n",
    "\n",
    "# Path to your Excel file and output file\n",
    "input_file = 'a.xlsx'\n",
    "output_file = 'b.xlsx'\n",
    "\n",
    "# Load the workbook and select the active worksheet\n",
    "wb = load_workbook(input_file)\n",
    "ws = wb.active\n",
    "\n",
    "# Dictionary to store unique values and their hyperlinks\n",
    "unique_values = {}\n",
    "\n",
    "# Loop through all cells in the worksheet\n",
    "for row in ws.iter_rows():\n",
    "    for cell in row:\n",
    "        # Check if cell has a value\n",
    "        if cell.value:\n",
    "            # Store value and hyperlink (if any)\n",
    "            unique_values[cell.value] = cell.hyperlink.target if cell.hyperlink else None\n",
    "\n",
    "# Sort the unique values\n",
    "sorted_unique_values = sorted(unique_values.items())\n",
    "\n",
    "# Create a new workbook and select the active worksheet\n",
    "new_wb = Workbook()\n",
    "new_ws = new_wb.active\n",
    "\n",
    "# Write unique values and hyperlinks to the new worksheet\n",
    "for i, (text, hyperlink) in enumerate(sorted_unique_values, start=1):\n",
    "    new_ws.cell(row=i, column=1, value=text)\n",
    "    if hyperlink:\n",
    "        new_ws.cell(row=i, column=1).hyperlink = hyperlink\n",
    "\n",
    "# Save the new workbook\n",
    "new_wb.save(output_file)\n",
    "\n",
    "print(f\"Unique values with hyperlinks written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_description_list = {\n",
    "    '2WikiMultihopQA': '2WikiMultihopQA is a dataset for comprehensive evaluation of reasoning steps in multi-hop question answering. The dataset contains questions that require multiple steps of reasoning to answer, and is based on Wikipedia articles. The dataset includes evidence information containing a reasoning path for multi-hop questions.',\n",
    "    'A-OKVQA': 'A-OKVQA is a visual question answering dataset that requires models to draw upon outside knowledge to answer questions. It is composed of approximately 25k questions that require a broad base of commonsense and world knowledge to answer. The questions in the dataset cannot be answered by simply querying a knowledge base, and instead require some form of commonsense reasoning. The dataset includes questions that require reasoning via a variety of knowledge types such as commonsense, world knowledge, and visual knowledge.',\n",
    "    'AFQMC': 'The Ant Financial Question Matching Corpus (AFQMC) is a dataset for semantic similarity in the Chinese language. It is used to evaluate the performance of natural language processing models on tasks such as question matching and paraphrase identification. The dataset contains 16,000 pairs of questions, each labeled with a similarity score ranging from 0 to 5.',\n",
    "    'AGIEval': 'AGIEval is a human-centric benchmark designed to evaluate the general abilities of foundation models in tasks pertinent to human cognition and problem-solving. It is derived from 20 official, public, and high-standard admission and qualification exams intended for general human test-takers, such as general college admission tests (e.g., Chinese College Entrance Exam (Gaokao) and American SAT), law school admission tests, math competitions, lawyer qualification tests, and national civil service exams.',\n",
    "    'ALFWorld': 'ALFWorld contains interactive TextWorld environments that parallel embodied worlds in the ALFRED dataset. The aligned environments allow agents to reason and learn high-level policies in an abstract space before solving embodied tasks through low-level actuation.',\n",
    "    'ANGO': 'ANGO-S1 is a dataset of Chinese analogical reasoning questions. It contains 1,000 questions that are categorized into 4 types: \"judgmental reasoning\", \"analogical reasoning\", \"logical relationship\", and \"logical relationship-correspondence\"',\n",
    "    'APPS': 'The APPS dataset is a collection of programming problems gathered from various open-access coding websites such as Codeforces, Kattis, and more. The dataset is intended to mimic how human programmers are evaluated by presenting coding problems in unrestricted natural language and assessing the accuracy of solutions.',\n",
    "    'ARC': 'The AI2’s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset that contains questions from science exams from grade 3 to grade 9. The dataset is split into two partitions: Easy and Challenge, where the latter partition contains more difficult questions that require reasoning. Most of the questions have 4 answer choices, and some have 5. The dataset consists of 7,787 science exam questions drawn from a variety of sources, including science questions provided under license by a research partner affiliated with AI2.',\n",
    "    'ARC-DA': 'ARC-DA is a dataset of 2,985 grade-school level, direct-answer (\"open response\", \"free form\") science questions derived from the ARC multiple-choice question set released as part of the AI2 Reasoning Challenge. The questions were filtered based on heuristic filters such as concreteness of the question and confidence in the correctness of the answers. The dataset was built by presenting each of the multiple-choice questions as a direct answer question to five crowdsourced workers to gather additional answers. The questions were then further filtered and modified by volunteers in-house. The dataset is one of the first direct-answer datasets of natural questions that often require reasoning, and where appropriate question decompositions are not evident from the questions themselves.',\n",
    "    'ActivityNet-QA': 'The ActivityNet-QA dataset consists of 58,000 human-annotated QA pairs on 5.8k complex web videos. The dataset provides a benchmark for testing the performance of VideoQA models on long-term spatio-temporal reasoning.',\n",
    "    'AlpacaEval': 'AlpacaEval is an automatic evaluator for instruction-following language models. It is a human-validated, high-quality, cheap, and fast dataset that is particularly useful for model development. The dataset is a simplification of AlpacaFarm’s evaluation set, where \"instructions\" and \"inputs\" are merged into one field, and reference outputs are longer. It is designed to evaluate instruction-following models (e.g., ChatGPT) and is validated against 20K human annotations. The dataset is useful if you have to run quick and cheap proxy for human evaluation of simple instruction-following tasks.',\n",
    "    'Amazon Review': 'Amazon Review Data (2018) is a dataset that contains reviews and ratings of products sold on Amazon. The dataset is available on Kaggle and includes full reviews and ratings for various products. The dataset is useful for sentiment analysis and natural language processing tasks. It includes reviews from four different merchandise categories: Books, DVDs, Electronics, and Kitchen and housewares.',\n",
    "    'AQUA-RAT': 'The AQUA-RAT (Algebra Question Answering with Rationales) Dataset is a collection of approximately 100,000 algebraic word problems with natural language rationales. Each problem is a JSON object consisting of four parts: question, options, rationale, and correct option. The dataset is used to train a program generation model that learns to generate the explanation while generating the program that solves the problem.',\n",
    "    'AR-LSAT': 'AR-LSAT dataset consists of analytical reasoning (AR) problems from the Law School Admission Test (LSAT) from 1991 to 2016. The dataset is used to study the challenge of analytical reasoning of text and to analyze what knowledge understanding and reasoning abilities are required to do well on this task.',\n",
    "    'ArguAna': 'ArguAna is a dataset in the BEIR (Benchmarking Information Retrieval) framework that contains textual data for entity-linking-retrieval and fact-checking-retrieval tasks.',\n",
    "    'BBQ': 'BBQ (Bias Benchmark for QA) is a dataset of question sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. The dataset was introduced to evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model’s biases override a correct answer choice.',\n",
    "    'BIG-Bench': 'BIG-bench is a collaborative benchmark that aims to evaluate and extrapolate the capabilities of large language models. It consists of over 200 tasks that are designed to test the models’ ability to perform a wide range of natural language processing tasks, such as question answering, summarization, and translation.',\n",
    "    'BLiMP': 'BLiMP stands for Benchmark of Linguistic Minimal Pairs. It is a dataset that consists of 67 sub-datasets, each containing 1000 minimal pairs that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics 123. The data is automatically generated according to expert-crafted grammars. The purpose of this dataset is to evaluate what language models know about major grammatical phenomena in English.',\n",
    "    'BOLD': 'The Bias in Open-ended Language Generation Dataset (BOLD) is a large-scale dataset that consists of 23,679 English text generation prompts for benchmarking biases across five domains: profession, gender, race, religion, and political ideology. The dataset was introduced to systematically study and benchmark social biases in open-ended language generation. It also proposes new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles.',\n",
    "    'BUSTM': 'BUSTM dataset is a part of the CLUE benchmark. It is a Chinese language dataset that contains short text pairs. The dataset is designed to evaluate the performance of models on dialogue short text matching tasks.',\n",
    "    'Belebele': 'The Belebele dataset is a massively multilingual reading comprehension dataset that spans 122 language variants. It is a multiple-choice machine reading comprehension (MRC) dataset that contains 900 questions per language variant. The dataset is designed to evaluate the performance of mono- and multi-lingual models in high-, medium-, and low-resource languages. Each question has four multiple-choice answers and is linked to a short passage from the FLORES-200 dataset. The human annotation procedure was carefully curated to create questions that discriminate between different levels of generalizable language comprehension and is reinforced by extensive quality checks.',\n",
    "    'BiPaR': 'BiPaR is a bilingual parallel novel-style machine reading comprehension (MRC) dataset that was developed to support multilingual and cross-lingual reading comprehension. It contains 3,667 bilingual parallel paragraphs from Chinese and English novels, from which 14,668 parallel question-answer pairs were constructed via crowdsourced workers following a strict quality control procedure. Each triple (Passage, Question, Answer) in BiPaR is written parallelly in two languages, making it different from existing reading comprehension datasets. The dataset offers good diversification in prefixes of questions, answer types, and relationships between questions and passages. BiPaR requires reading comprehension skills of coreference resolution, multi-sentence reasoning, and understanding of implicit causality, etc.',\n",
    "    'BigPatent': 'BIGPATENT is a large-scale dataset of.3 million records of U.S. patent documents along with human-written abstractive summaries. It was created to address the limitations of existing summarization datasets, which are mostly compiled from the news domain and have summaries with a flattened discourse structure. In contrast, BIGPATENT summaries contain a richer discourse structure with more recurring entities, salient content is evenly distributed in the input, and lesser and shorter extractive fragments are present in the summaries. The dataset is useful for training and evaluating systems that can understand an article’s global content structure as well as produce abstractive summaries with high compression ratio.',\n",
    "    'BioASQ-QA': 'The BioASQ-QA dataset is a manually curated corpus for biomedical question answering. It is composed of a question, human-annotated answers, and the relevant contexts (also called snippets). The dataset is part of the BioASQ (Biomedical Semantic Indexing and Question Answering) project, which aims to advance the state-of-the-art in biomedical semantic indexing and question answering. The dataset is designed to evaluate the performance of biomedical question answering systems. The dataset is composed of two phases: Phase A and Phase B. Phase A contains 3.7k questions, and the answers are either a document or a snippet. Phase B contains 3.7k questions, and the answers are either a factoid answer, a list of answers, or a yes/no answer.',\n",
    "    'BoolQ': 'The BoolQ dataset is a question answering dataset specifically designed for yes/no questions. It contains 15942 examples. These questions are naturally occurring, meaning they are generated in unprompted and unconstrained settings.',\n",
    "    'Boolean Expressions': 'The Boolean Expressions dataset is a part of the BIG-bench (Beyond the Imitation Game benchmark) project hosted on GitHub. This dataset is designed to test the ability of language models to understand and reason about Boolean expressions. The task involves evaluating Boolean expressions that are composed of the logical operators AND, OR, and NOT, as well as parentheses for grouping. The expressions are generated randomly, and the variables in the expressions can take on the values True or False.',\n",
    "    'Broadcoverage Diagnostics': 'The Broadcoverage Diagnostics dataset is a part of the GLUE benchmark. It is a manually-curated evaluation dataset for fine-grained analysis of system performance on a broad range of linguistic phenomena. This dataset evaluates sentence understanding through Natural Language Inference (NLI) problems.',\n",
    "    'C-Eval': 'The C-Eval benchmark dataset is a comprehensive Chinese evaluation suite designed to assess the advanced knowledge and reasoning abilities of foundation models. The dataset consists of 13,948 multiple-choice questions spanning 52 diverse disciplines and four difficulty levels13. The disciplines range from humanities to science and engineering. The questions are designed to test the model’s understanding of a wide range of topics and its ability to reason about them. In addition to the main C-Eval dataset, there is also a C-Eval Hard subset. This subset includes 8 challenging subjects in math, physics, and chemistry that require advanced reasoning abilities to solve',\n",
    "    'C-SEM': 'The C-SEM dataset is a benchmark dataset for evaluating the semantic understanding abilities of large Chinese language models. Addressing the lack of linguistically oriented benchmarks in this area, C-SEM introduces various levels and difficulties of evaluation data to scrutinize the potential flaws of current models. It focuses on how these models process semantics, drawing parallels to human language cognition.',\n",
    "    'C3': 'C3 is a free-form multiple-choice Chinese machine reading comprehension dataset that contains 13,369 documents and their associated 19,577 multiple-choice free-form questions. The dataset was collected from Chinese-as-a-second language examinations. The dataset is the first of its kind and is designed to test the ability of machine learning models to understand and answer questions based on free-form text in Chinese.',\n",
    "    'CAIL': '''The CAIL (Chinese AI and Law) dataset, specifically CAIL2018, is a pioneering large-scale Chinese legal dataset aimed at facilitating judgment prediction research. It contains over 2.6 million criminal cases, which were published by the Supreme People's Court of China. This makes it significantly larger than other datasets previously available in the field of legal judgment prediction. Its considerable size and focus on Chinese legal cases make it a unique and valuable resource for research in artificial intelligence applications within the legal domain, particularly in the context of the Chinese legal system''',\n",
    "    'CBBQ': 'The CBBQ dataset is a Chinese bias benchmark dataset that consists of over 100,000 questions jointly constructed by human experts and generative language models. The dataset covers stereotypes and societal biases in 14 social dimensions related to Chinese culture and values. The curation process contains 4 essential steps: bias identification via extensive literature review, ambiguous context generation, AI-assisted disambiguous context generation, and manual review & recomposition. The testing instances in the dataset are automatically derived from 3,000+ high-quality templates manually authored with stringent quality control.',\n",
    "    'CCBench': 'The CCBench dataset is a multi-modal benchmark specifically focused on the domain of Chinese culture. It was introduced as a new benchmark in 2023 and is an extension of the MMBench benchmark. The CCBench dataset is designed to evaluate multi-modal models, particularly in the context of their understanding and interpretation of aspects related to Chinese culture. While I was unable to find detailed information regarding the specific contents and structure of the dataset, it is clear that it serves as a specialized tool for assessing the performance of AI models in handling culturally-specific multimodal data, especially those pertaining to Chinese traditions and contexts',\n",
    "    'CDIAL-BIAS': 'The CDial-Bias Dataset is a unique resource specifically designed to measure social bias in dialog scenarios. It was created to address the subtleties and subjective nature of biased utterances in dialogues. The dataset, which is a part of a larger effort to identify social bias in dialogue systems, includes well-annotated training data that covers aspects like context-sensitivity, data-type, targeted group, and implied attitudes.',\n",
    "    'CG-Eval': 'The CG-Eval dataset is a comprehensive benchmark designed to evaluate the generation capabilities of large Chinese language models across various academic disciplines. It covers six major academic disciplines, including Science and Engineering, Humanities and Social Sciences, Mathematics, Medical Qualification Exam, Judicial Examination, and Certified Public Accountant Exam, encompassing 55 sub-disciplines within these categories. The dataset includes over 11,000 questions of various types, for which the models are expected to provide precise and pertinent answers. CG-Eval utilizes a composite scoring system, with standard reference answers for non-computational questions and a detailed evaluation of both the final results and the problem-solving process for computational questions.',\n",
    "    'CHID': '''The CHID dataset, standing for \"Chinese IDiom Dataset for Cloze Test,\" is a large-scale dataset specifically designed for cloze-style tests in Chinese. It consists of 581,000 passages that contain a total of 729,000 blanks, encompassing a wide range of domains. In these passages, idioms are deliberately replaced with blank symbols. The dataset challenges the ability of models to understand and predict idioms in context, a unique aspect of the Chinese language. To complete the test, a list of passages is provided, and the correct idioms must be selected from a set of given candidates with a fixed length. This dataset not only includes the passages and their associated blanks but also offers training data and answers, making it a comprehensive resource for enhancing cloze test-related research in Chinese language processing.''',\n",
    "    'CIFAR-10': 'The CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images that are commonly used to train machine learning and computer vision algorithms. It is one of the most widely used datasets for machine learning research. The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes.',\n",
    "    'CJRC': '''The CJRC (Chinese Judicial Reading Comprehension) dataset is a substantial collection specifically designed for legal research in the Chinese language. It comprises approximately 10,000 documents and nearly 50,000 questions with answers. The documents included in the dataset are derived from judicial judgment documents. These questions are expertly annotated by law professionals, ensuring their relevance and accuracy in the context of legal studies. This dataset is particularly valuable for research in the field of reading comprehension technology, especially in its application to legal documents. It enables researchers to test and enhance machine reading comprehension capabilities, specifically tailored for the legal domain. The CJRC dataset is a significant resource for developing and evaluating AI models' ability to understand and process legal texts.''',\n",
    "    'CLEVR': 'The CLEVR dataset is a diagnostic dataset for compositional language and elementary visual reasoning. It is used to test a range of visual reasoning abilities and contains minimal biases. The dataset has detailed annotations describing the kind of reasoning each question requires. It is used to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations. Additionally, it contains a Compositional Generalization Test (CoGenT) that tests the ability of models to recognize novel combinations of attributes at test-time.',\n",
    "    'CLEVRER': '''The CLEVRER dataset, short for CoLlision Events for Video REpresentation and Reasoning, is a diagnostic video dataset designed for the systematic evaluation of computational models on a variety of reasoning tasks. It was created with the intention of providing a controlled and well-annotated environment to facilitate the assessment of logic reasoning in both temporal and causal domains. CLEVRER is unique in that it focuses on simplicity and minimal biases in visual scenes and language, ensuring that the tasks are purely logic-based. This dataset includes four types of questions: descriptive (e.g., \"what color\"), explanatory (\"what's responsible for\"), predictive (\"what will happen next\"), and counterfactual reasoning. The goal of these questions is to probe the models' abilities in different aspects of reasoning, including description, explanation, prediction, and counterfactual analysis. The design and structure of CLEVRER make it a pioneering dataset for neuro-symbolic reasoning in the realm of artificial intelligence, providing a comprehensive tool for evaluating and enhancing computational models' reasoning capabilities​.''',\n",
    "    'CLUE diagnostics': '''The CLUE (Chinese Language Understanding Evaluation) diagnostics dataset is a comprehensive tool designed for evaluating and analyzing the performance of Chinese language models. This dataset is part of the broader CLUE benchmark, which is akin to the GLUE benchmark in English. It serves as a specialized resource for diagnosing the strengths and weaknesses of models in various aspects of language understanding. The CLUE diagnostics dataset includes a range of linguistic tasks and challenges, designed to thoroughly test different facets of language comprehension, such as syntax, semantics, context understanding, and more. By providing detailed performance metrics across these areas, the CLUE diagnostics dataset helps researchers and developers identify specific areas for improvement in Chinese language models, ultimately contributing to the advancement of natural language processing technologies in the Chinese context.''',\n",
    "    'CLUEWSC': 'The CLUEWSC (CLUE Winograd Scheme Challenge) 2020 dataset is a specialized subset of the broader CLUE (Chinese Language Understanding Evaluation) benchmark, which is designed to evaluate natural language understanding (NLU) in Chinese. While CLUE itself encompasses various NLU datasets spanning single-sentence and sentence-pair classification tasks, machine reading comprehension, and other tasks on Chinese text​​, CLUEWSC 2020 specifically focuses on the challenge of coreference resolution. In CLUEWSC 2020, the primary task is to determine which noun a given pronoun in a sentence refers to. The dataset consists of texts sourced from contemporary literature, which have been annotated by human experts. The format of the questions is centered around true or false discrimination, requiring the model to discern the correct referent of a pronoun within a sentence. This specific focus makes CLUEWSC 2020 an important tool for assessing the understanding of pronoun-noun relationships in Chinese language models, a crucial aspect of language understanding and processing​',\n",
    "    'CLiB': 'The CLiB dataset is a comprehensive leaderboard for evaluating the capabilities of large Chinese language models. The evaluation in CLiB covers multiple dimensions of model capabilities, including classification ability, information extraction ability, reading comprehension ability, and table question-answering ability.',\n",
    "    'CMMLU': '''The CMMLU (Chinese Massive Multitask Language Understanding) dataset is a comprehensive evaluation benchmark specifically designed to assess the knowledge and reasoning abilities of language models within the Chinese language and cultural context. It encompasses a wide array of subjects, covering 67 topics that range from elementary to advanced professional levels. This includes natural sciences requiring calculation and reasoning, humanities and social sciences requiring knowledge, as well as practical knowledge like Chinese driving rules.''',\n",
    "    'CMNLI': '''The CMNLI dataset, part of the Chinese Language Understanding Evaluation (CLUE) benchmark, is a Chinese version of the Natural Language Inference (NLI) task, focusing on assessing the capability of models to understand and infer relationships between sentences. It is composed of two parts: XNLI and MNLI, and includes a variety of data sources such as fiction, telephone, travel, government, and slate.''',\n",
    "    'CMRC2018': '''The CMRC 2018 (Chinese Machine Reading Comprehension 2018) dataset, introduced by Cui et al., is a specialized dataset designed for evaluating Chinese machine reading comprehension. It features a collection of nearly 20,000 questions that have been carefully annotated by human experts. A notable aspect of this dataset is the inclusion of a challenging set of questions that require reasoning over multiple clues to answer. This characteristic makes CMRC 2018 particularly valuable for assessing the depth and complexity of machine reading comprehension models, especially in their ability to handle nuanced and multifaceted queries in the Chinese language​.''',\n",
    "    'CNN/Daily Mail': '''The CNN/Daily Mail summarization dataset is a widely used resource for text summarization research. It consists of news articles from the CNN and Daily Mail websites, accompanied by human-generated abstractive summary bullets. These summaries are created in a unique format where questions are formulated with one of the entities hidden, and the system is expected to fill in the blank based on the corresponding news story. The dataset includes over 300,000 unique English-language news articles.''',\n",
    "    'COCO-Text': '''The COCO-Text dataset is a specialized resource for text detection and recognition within natural images. It's an extension of the MS COCO dataset, which is renowned for its complex everyday scene images. COCO-Text aims to advance the state-of-the-art in text detection and recognition by providing a large-scale dataset specifically focused on scene text. The dataset includes various types of images, such as those with non-text, legible text, and illegible text. The dataset is structured to support a variety of text-related tasks, including End-To-End Recognition, Cropped Word Recognition, and Text Localization.''',\n",
    "    'COLDataset': '''COLDataset is a dataset that facilitates Chinese offensive language detection and model evaluation. It contains 37,480 comments with binary offensive labels and covers diverse topics of race, gender, and region. The test set is annotated at a fine-grained level with four categories: attacking individuals, attacking groups, anti-bias, and other non-offensive.''',\n",
    "    'COPA': '''Choice of Plausible Alternatives (COPA) is a benchmark dataset that evaluates the ability of machine learning models to transfer commonsense reasoning across languages. Each question is composed of a premise and two alternatives, where the task is to select the alternative that more plausibly has a causal relation with the premise. The COPA evaluation provides researchers with a tool for assessing progress in open-domain commonsense causal reasoning.''',\n",
    "    'COPEN': '''The COPEN dataset is a conceptual knowledge probing benchmark designed to evaluate the conceptual understanding capabilities of Pre-trained Language Models (PLMs). it consists of three specific tasks: Conceptual Similarity Judgment (CSJ): This task involves presenting a query entity along with several candidate entities. The PLM's job is to select the candidate entity that is most conceptually similar to the query entity. Conceptual Property Judgment (CPJ): In this task, the PLM is given a statement that describes a property of a concept. The model must then determine whether this statement is true. Conceptualization in Contexts (CiC): This task requires a PLM to analyze a sentence that mentions an entity and is provided with several concept chains of the entity. The model must then choose the most appropriate concept for the entity based on the context of the sentence.''',\n",
    "    'CORGI-PM': '''CORGI-PM is a Chinese cOrpus foR Gender bIas Probing and Mitigation, which contains 32.9k sentences with high-quality labels derived by following an annotation scheme specifically developed for gender bias in the Chinese context.''',\n",
    "    'CQADupStack': '''CQADupStack is a benchmark dataset for community question-answering (cQA) research. It contains threads from twelve StackExchange subforums, annotated with duplicate question information and comes with pre-defined training, development, and test splits, both for retrieval and classification experiments.''',\n",
    "    'CRASS': '''The CRASS benchmarks aims to develope a new test suite for the evaluation of the performance of current large language models (LLMs) by utilizing so called questionized counterfactual conditionals (QCC). The data consists of so called PCTs (Premise-Counterfactual Tuples). They contrast a hypothetical situation using a counterfactual conditional against a base premise. In the fixed target mode the task of the model is to decide which answer is correct. In the open scoring mode the model has to give an answer to the PCT which in turn is judged by human annotators as correct or incorrect.''',\n",
    "    'CSL': '''CSL is a large-scale Chinese scientific literature dataset that contains the titles, abstracts, keywords, and academic fields of 396k papers.''',\n",
    "    'CSNLI': '''The Chinese-SNLI dataset is a large-scale Chinese nature language inference and semantic similarity calculation dataset. It was created by translating and partially correcting an English dataset, and it can help alleviate the lack of Chinese natural language inference and semantic similarity calculation datasets''',\n",
    "    'CUAD': '''The Contract Understanding Atticus Dataset (CUAD) is a dataset of legal contracts with rich expert annotations. It is a corpus of 13,000+ labels in 510 commercial legal contracts that have been manually labeled under the supervision of experienced lawyers to identify 41 types of legal clauses that are considered important in contact review in connection with a corporate transaction, including mergers & acquisitions, etc.''',\n",
    "    'Cute80': '''Cute80 dataset is a public dataset that can be used for text detection in images. It consists of 80 high-resolution images with 288 cropped text instances.''',\n",
    "    'CWQ': '''The ComplexWebQuestions (CWQ) dataset is a collection of complex questions that require reasoning over multiple web snippets. It contains a large set of complex questions in natural language, and can be used in multiple ways: 1) By interacting with a search engine, which is the focus of our paper; 2) As a reading comprehension task: we release 12,725,989 web snippets that are relevant for the questions, and were collected during the development of our model; 3) As a semantic parsing task: each question is paired with a SPARQL query that can be executed against Freebase to retrieve the answer.''',\n",
    "    'CUB-200-2011': '''The Caltech-UCSD Birds-200-2011 (CUB-200-2011) dataset is a collection of images of birds that is widely used for fine-grained visual categorization tasks. Each image has detailed annotations such as 1 subcategory label, 15 part locations, 312 binary attributes, and 1 bounding box. The dataset is suitable for object recognition or attribute recognition tasks.''',\n",
    "    'Charades-STA': '''The Charades-STA dataset is an enhanced version of the original Charades dataset, specifically designed for video captioning and temporal localization tasks. It includes 9,848 short video clips that depict a variety of daily indoor activities such as brushing teeth and cooking. This dataset stands out by incorporating sentence temporal annotations to the original Charades videos, effectively extending its utility for more complex language and video processing tasks. In Charades-STA, each video is accompanied by sentence-level temporal annotations, creating a rich source of data for training and testing models in accurately associating textual descriptions with specific moments in a video. The dataset contains 12,408 moment-sentence pairs in the training set and 3,720 pairs in the testing set. The format of these annotations typically includes the video name, the start and end times of the action within the video, and the corresponding sentence description. This structured approach allows for advanced applications in video analysis, such as the development of models capable of localizing specific activities in a video based on textual queries. Charades-STA's focus on daily indoor activities, combined with its detailed temporal and linguistic annotations, makes it an invaluable resource for research in video captioning, natural language processing, and temporal activity localization.''',\n",
    "    'Chatbot Arena Conversations': '''The Chatbot Arena Conversations dataset comprises 33,000 crowd-sourced conversations, each annotated with human preferences. These conversations were collected from the Chatbot Arena platform between April and June 2023, involving interactions with various chatbot models. Key features of the dataset include detailed information for each conversation, such as the names of the two AI models involved, the full text of the conversation, the user's vote, anonymized user ID, the detected language, moderation tags (including the OpenAI moderation API tag and an additional toxic tag), and a timestamp. Efforts were made to exclude conversations containing personally identifiable information, and the dataset includes flagged inappropriate conversations, which can be critical for researchers studying safety-related aspects of LLMs.''',\n",
    "    'ChineseSquad': '''The ChineseSquad dataset is a Chinese machine reading comprehension dataset created by translating and manually correcting the original Squad dataset into Chinese. This dataset was developed to address the limitations of existing Chinese reading comprehension datasets, which were either too small or too focused on specific domains.''',\n",
    "    'CivilComments': '''The CivilComments dataset is derived from the Civil Comments platform, a commenting plugin used on about 50 English-language news sites worldwide. The dataset is known for its use in the Jigsaw Unintended Bias in Toxicity Classification and features a large collection of comments, approximately 2 million, which are useful for understanding the nuances and challenges of online discourse. The dataset serves as a benchmark for toxic comment classification, helping to advance research in detecting toxic comments on social media, which is crucial for content moderation.''',\n",
    "    'Climate-FEVER': '''CLIMATE-FEVER is a publicly available dataset that aims to verify climate change-related claims. It consists of 1,535 real-world claims collected from the internet and each claim is accompanied by five manually annotated evidence sentences retrieved from the English Wikipedia that support, refute or do not give enough information to validate the claim. The dataset was created to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change.''',\n",
    "    'CoLA': '''The Corpus of Linguistic Acceptability (CoLA) is a specialized dataset comprising 10,657 sentences extracted from 23 linguistics publications. These sentences have been expertly annotated for acceptability (grammaticality) by their original authors. The primary aim of CoLA is to provide a resource for training and evaluating natural language processing models, particularly in the area of grammatical acceptability judgments. The public version of CoLA includes 9,594 sentences designated for training and development purposes, while excluding 1,063 sentences that are part of a held-out test set. This dataset serves as a valuable tool for researchers in the field of computational linguistics, offering a rich set of examples to improve and assess the grammatical understanding capabilities of various language models​''',\n",
    "    'CoQA': '''CoQA is a large-scale dataset for building Conversational Question Answering systems. The dataset contains 127,000+ questions with answers collected from 8000+ conversations. Each conversation is collected by pairing two crowdworkers to chat about a passage in the form of questions and answers. The unique features of CoQA include 1) the questions are conversational; 2) the answers can be free-form text; 3) each answer also comes with an evidence subsequence highlighted in the passage; and 4) the passages are collected from seven diverse domains.''',\n",
    "    'CodeU': '''The CodeU dataset is a part of the L-Eval evaluation suite for long context language models. It is designed to test the capability of understanding long code by calling some functions defined in a lengthy codebase and inferring the final output of the program. The dataset mainly uses source code from Numpy and also includes a string processing codebase containing more than 100 functions that take a string as input. To prevent language models from answering the question based on their parametric knowledge, the original function names defined in Numpy are replaced with Op1, Op2..., OpN.''',\n",
    "    'CommitmentBank': '''The CommitmentBank is a corpus of 1,200 naturally occurring discourses whose final sentence contains a clause-embedding predicate under an entailment canceling operator (question, modal, negation, antecedent of conditional). The dataset is designed to investigate projection in naturally occurring discourse.The dataset provides information on the target sentence, context, prompt, verb, embedding, factivity, modal type, tense of the matrix verb, genre, and projection judgments to the prompt using a 7-point Likert scale.''',\n",
    "    'Common Syntactic Processes': '''The Common Syntactic Processes (CSP) dataset is a collection of sentences that have been annotated with their syntactic structure. The dataset was created to help researchers develop and evaluate models for natural language processing tasks such as parsing, part-of-speech tagging, and dependency parsing. The CSP dataset contains sentences from a variety of sources, including news articles, Wikipedia, and web forums. The sentences are annotated using the Universal Dependencies (UD) scheme, which provides a consistent way of representing the syntactic structure of sentences across languages.''',\n",
    "    'CommonGen': '''CommonGen is a benchmark dataset that tests machines for the ability of generative commonsense reasoning. It is a constrained text generation task that requires generating a coherent sentence describing an everyday scenario using a set of common concepts. The dataset consists of 79k commonsense descriptions over 35k unique concept-sets and is constructed through a combination of crowdsourced and existing caption corpora.''',\n",
    "    'CommonMT': '''CommonMT is a dataset used to evaluate neural machine translation models’ ability to reason about common sense. It contains three types of test suites: lexical ambiguity, contextless syntactic ambiguity, and contextual syntactic ambiguity.''',\n",
    "    'CommonSenseQA': '''The CommonsenseQA dataset is a multiple-choice question-answering dataset designed to test commonsense knowledge. Introduced in 2019, it consists of 12,102 questions, each accompanied by one correct answer and four distractor answers. The dataset requires a variety of commonsense knowledge types for accurate prediction of the correct answers. It has been designed to be challenging, aiming to advance the capabilities of AI systems in understanding and applying commonsense knowledge in a question-answering context.''',\n",
    "    'CooridinateAI': '''The CooridinateAI dataset is specialized and focused on evaluating the willingness of AI models to coordinate with other systems. It is primarily concerned with safety and trustworthiness, under the sub-dimension of potential risks, and is designed for tasks in Question Answering in the Chinese language. The dataset is relatively small, comprising 1080 questions in the training set. These questions are designed to assess whether a model would cooperate with other AI systems to achieve its objectives, such as avoiding safety failures. his structure allows researchers to gauge how AI models make decisions in scenarios involving coordination and ethics, particularly in contexts where safety and reliability are paramount.''',\n",
    "    'Corrigible': '''The Corrigible dataset is designed to evaluate the alignment of large language models (LLMs) with human values, specifically focusing on being helpful, honest, and harmless. The dataset is in Chinese and is used for tasks in the area of Question Answering. The main goal of the Corrigible dataset is to test whether LLMs are more inclined to align with human values.''',\n",
    "    'CosmosQA': '''The CosmosQA dataset is a large-scale collection of 35.6K problems specifically designed for assessing commonsense-based reading comprehension. The dataset is structured in a multiple-choice question format. Its primary focus is on \"reading between the lines\", requiring the ability to understand and reason about everyday narratives. It poses questions about the likely causes or effects of events, which demand reasoning beyond the explicit text provided in the context. This unique approach in CosmosQA is geared towards evaluating the ability to infer and deduce information that isn't directly stated, a key aspect of commonsense reasoning. By using a diverse range of real-world narratives, CosmosQA challenges models to understand and interpret various situations, drawing conclusions that rely on a deeper understanding of the world, rather than relying solely on textual evidence.''',\n",
    "    'Coursera': '''The Coursera dataset is a part of the L-Eval evaluation suite for long context language models. It originates from the Coursera website and includes four public courses related to big data and machine learning. The input long document is the subtitles of the videos, and questions and ground truth answers are labeled by the authors. The instruction style of Coursera takes the format of multiple choice, and to increase the difficulty of the task, multiple correct options are set.''',\n",
    "    'CrimeKgAssitant': '''CrimeKgAssitant is a crime assistant that includes crime type prediction and crime consult service based on NLP methods and crime KG. It contains 856 items of crime knowledge graph, based on 2.8 million crime training libraries for crime prediction, 13 categories of question classification and legal information Q&A function based on 200,000 legal Q&A pairs. The project aims to provide intelligent solutions for legal intelligence by utilizing existing big data, machine learning/deep learning, and natural language processing technologies. The project has two main directions: 1) Collecting relevant data based on crime names, building a basic crime knowledge graph, legal information dialogue knowledge base, and case sentencing knowledge base. 2) Completing the following four aspects of work based on the results of step 1: Crime prediction model based on case sentencing knowledge base, legal question type classification based on legal consultation dialogue knowledge base, legal question automatic Q&A service based on legal consultation dialogue knowledge base, and knowledge query based on crime knowledge graph.''',\n",
    "    'Criminal': '''The Criminal dataset is a collection of criminal cases used for charge prediction tasks. The dataset is preprocessed and randomly selected to construct three different datasets with different scales denoted as Criminal-S (small), Criminal-M (medium), and Criminal-L (large). The three datasets contain the same number of charges but different numbers of cases. The dataset includes various attributes such as profit purpose, buying and selling behavior, death, violence, state organ, public place, illegal possession, physical injury, intentional crime, and production. The proposed model in the PDF file achieves significant improvements over other state-of-the-art baselines in charge prediction tasks.''',\n",
    "    'CrowS-Pairs': '''CrowS-Pairs is a challenge dataset that measures social biases in masked language models. The dataset consists of 1508 examples that cover stereotypes dealing with nine types of bias, such as race, religion, and age. In each example, a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups.''',\n",
    "    'DBPedia': '''DBpedia is a crowd-sourced community effort to extract structured content from various Wikimedia projects, including Wikipedia. The project aims to provide a comprehensive and structured knowledge graph that can be used by anyone on the web. The DBpedia dataset is a large multi-domain ontology derived from Wikipedia that describes millions of “things” with “facts”. DBpedia allows users to semantically query relationships and properties of Wikipedia resources, including links to other related datasets.''',\n",
    "    'DDM': '''The Driving Decision-Making dataset is a collection of diverse YouTube driving videos depicting complex traffic situations and accidents. The dataset is used to challenge Video-LLMs to comprehend the origins of these complex traffic situations or accidents and make correct decisions to prevent their occurrence. The task demands a higher level of scene understanding and decision-making ability, making it a more intricate task than the driver's license examination. The videos are manually annotated for scene analysis and accident causes, and the expectation is that the model can effectively comprehend the complex scenarios and make informed decisions.''',\n",
    "    'DLE': '''The Driver's License Examination dataset is a collection of video-based questions that assess the ability of candidates to interpret simple animations depicting motor vehicle and driver status, requiring judgments of potential anomalies. The task challenges Video-LLMs to comprehend scenarios and answer exam questions, which is a less intricate task than the Driving Decision-Making dataset. The expectation is that the model can effectively comprehend the scenarios and answer the questions correctly. ''',\n",
    "    'DRCD': '''The Delta Reading Comprehension Dataset (DRCD) is an open-domain traditional Chinese machine reading comprehension (MRC) dataset. It was created to serve as a standard benchmark for Chinese MRC, with the additional goal of being a valuable source dataset for transfer learning. The DRCD contains a substantial amount of data sourced from Wikipedia, including 10,014 paragraphs from 2,108 articles. Alongside these textual contents, the dataset includes over 30,000 questions that have been generated by annotators. This extensive collection of paragraphs and questions makes the DRCD a significant resource for developing and testing machine reading comprehension models, specifically for traditional Chinese.''',\n",
    "    'DROP': '''The Discrete Reasoning Over Paragraphs (DROP) dataset is a challenging benchmark specifically designed to evaluate the ability of systems to perform discrete reasoning over text. It was created through a crowdsourced, adversarially-driven approach and comprises 96,000 Question and Answer pairs (QAs) distributed over 6,700 paragraphs. DROP is distinct in its focus on discrete reasoning, requiring systems to not only understand the content of paragraphs but also to perform specific operations such as addition, counting, or sorting. These tasks necessitate a deeper comprehension of text than what was previously required in standard reading comprehension benchmarks. This dataset is pivotal in highlighting the limitations of current systems and pushing forward the development of more advanced natural language processing technologies.''',\n",
    "    # 'Data imputation': '''''',\n",
    "    'Demographic Stereotypes': '''Simple user prompts (\"A photo of the face of…\") generate images perpetuating dangerous racial, ethnic, gendered, class, and intersectional stereotypes.''',\n",
    "    'DocRED': '''DocRED (Document-Level Relation Extraction Dataset) is a relation extraction dataset constructed from Wikipedia and Wikidata. Each document in the dataset is human-annotated with named entity mentions, coreference information, intra- and inter-sentence relations, and supporting evidence. The dataset contains 132,375 entities and 56,354 relational facts annotated on 5,053 Wikipedia documents. DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document.''',\n",
    "    'DocVQA': '''The DocCVQA (Document Collection Visual Question Answering) dataset is a unique dataset designed for the task of Visual Question Answering (VQA) over a collection of scanned documents. It comprises a vast collection of 14,362 scanned documents, over which various questions are posed. The primary goal of this dataset is not just to provide answers to the questions asked but also to retrieve a set of documents that contain the necessary information to formulate these answers.''',\n",
    "    'DuReader': '''DuReader is a large-scale real-world Chinese dataset for Machine Reading Comprehension (MRC) and Question Answering (QA). The dataset consists of 200K questions, 420K answers and 1M documents. All questions in the dataset are sampled from real anonymized user queries, and the evidence documents, from which answers are derived, are extracted from the web and Baidu Zhidao using Baidu search engine. The dataset is designed to address real-world MRC and focuses on real-world open-domain question answering.''',\n",
    "    'Dyck Languages': '''The Dyck Languages task aims to evaluate the capability of language models to process and encode hierarchical information. We structure the task as a conditional-generation challenge, where the model is required to predict the sequence of closing parentheses for a D3 word, omitting its last few closing parentheses. This setup provides a clear and focused framework for assessing the hierarchical information processing abilities of language models.''',\n",
    "    'EPRSTMT': 'The EPRSTMT dataset, part of the FewCLUE benchmark, stands for \"E-commerce Product Review Dataset for Sentiment Analysis.\" This dataset is a binary sentiment analysis dataset that has been specifically developed for evaluating the performance of models in few-shot learning scenarios. It is constructed from product reviews collected from e-commerce platforms. Each review in the dataset is labeled as either Positive or Negative, providing a clear binary classification task for sentiment analysis. The inclusion of EPRSTMT in this benchmark aims to assess the ability of models to understand and analyze consumer sentiments expressed in online product reviews, a task that is increasingly relevant in the context of e-commerce and online retail​.',\n",
    "    'ETA': 'The Evolving Test of Applying (ETA) dataset is a unique dataset designed to evaluate the multi-hop reasoning capabilities of large language models (LLMs), particularly in the realm of applying world knowledge. The ETA dataset focuses on the joint reasoning between text and a knowledge base. However, unlike other datasets that emphasize explicit reasoning from direct information, ETA emphasizes implicit reasoning, a more challenging and nuanced form of inference. The dataset comprises 49 questions that are developed from 350 annotated knowledge triplets and 40 articles, all part of an evolving data set. This structure enables a dynamic and comprehensive assessment of LLMs in their ability to understand and apply complex, multi-step reasoning with evolving information, making it a valuable tool for understanding the depth and adaptability of reasoning skills in AI models.',\n",
    "    'ETC': '''The ETC (Evolving Test of Creating) dataset is designed to evaluate the knowledge creation ability of language models, particularly focusing on their capacity for knowledge-grounded text generation. The ETC dataset primarily targets the generation of narrative texts, such as history, news, and fiction, where the essence of creativity is encapsulated in the ability to describe subsequent events innovatively and coherently. To evaluate this creative aspect, the dataset focuses on the model’s capability to hallucinate event knowledge in the generated texts.''',\n",
    "    'ETM':'''The Evolving Test of Memorization (ETM) dataset is a unique component of a broader project aimed at evaluating knowledge memorization in large language models (LLMs). It is specifically designed to assess how well these models can recall and apply knowledge that is not explicitly derived from previously available data sources. The ETM dataset is constructed by identifying and annotating knowledge triplets within articles from evolving data sources. These triplets represent newer or less conventional information that challenges the LLMs' ability to adapt and incorporate recent or less frequent knowledge. Unlike the other test sets in the project, which focus on high-frequency and low-frequency knowledge from established datasets like Wikidata5M and Wikipedia, the ETM dataset emphasizes the importance of understanding and retaining newly emerging information. It consists of 100 such carefully selected triplets, ensuring a rigorous test of the LLMs' capacity to evolve and assimilate current information, thereby providing a dynamic and challenging measure of their memorization and learning capabilities.''',\n",
    "    'EKC': '''The Encyclopedic Knowledge Creation (EKC) dataset is designed to evaluate the knowledge creation ability of models, particularly in the context of knowledge-grounded text generation tasks. This ability is considered the highest level in Bloom's Cognitive Taxonomy. The EKC dataset primarily focuses on the generation of narrative texts, such as history, news, and fiction, where the essence of creativity is encapsulated in the description of subsequent events. To evaluate a model's ability to creatively generate event knowledge, the EKC dataset employs a methodology of fine-grained event annotations. These annotations are performed on a diverse array of texts, including Wikipedia articles and articles from evolving datasets. By utilizing this framework, the EKC dataset aims to provide a robust and comprehensive means of assessing how effectively language models can innovate and create new knowledge, especially in the realm of text generation. The assessment is grounded in the model's capability to hallucinate or invent event knowledge in the texts it generates, thereby offering insights into the model's creative capacities in narrative construction and knowledge synthesis.''',\n",
    "    'EntityMatching': '''The EntityMatching benchmark dataset is designed to evaluate the ability of systems to reason about whether two structured entities from different tables (or relations) represent the same entity. This task is a crucial component of data cleaning pipelines, particularly in the context of integrating relations from various sources where near-duplicate records of the same entity are common. The matches typically involve entities with minor textual variations, such as misspellings, nicknames, or formatting differences, whereas non-matches exhibit more significant textual disparities. This benchmark is critical in the realm of entity resolution and data integration, testing the limits of algorithms in identifying equivalencies amid textual and format inconsistencies.''',\n",
    "    'FActScore': '''FACTSCORE is a benchmark for evaluating the factuality of long-form text generated by large language models. It breaks down a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. This method is more fine-grained than traditional binary judgments of text quality, which can be inadequate when generations contain a mixture of supported and unsupported pieces of information.''',\n",
    "    ###### this is the breaking point!!!\n",
    "    'FELM': '''The FELM benchmark is a comprehensive tool for evaluating the factuality of large language models (LLMs). Its main goal is to provide a standardized and rigorous evaluation methodology for LLMs' factuality detection abilities across five diverse domains: world knowledge, science and technology, math, writing and recommendation, and reasoning. FELM differs from other factuality evaluation tools in that it takes a more holistic approach to factuality evaluation, encompassing a wider range of domains and tasks. It also uses a four-step process to construct the benchmark, which includes gathering prompts from various sources, collecting corresponding responses from ChatGPT, segmenting the responses into fine-grained text spans, and asking human annotators to annotate the factuality label, error type, error reason, and reference links used to make the judgment. The specific aspects of machine learning models that FELM aims to test and evaluate include their ability to accurately detect factual errors in text, their performance when augmented with different techniques such as external evidence and chain-of-thought reasoning, and their overall ability to perform tasks in varied settings. Overall, FELM is a comprehensive and rigorous benchmark that provides a standardized methodology for evaluating the factuality detection abilities of LLMs across a wide range of domains and tasks.''',\n",
    "    'FEVER': '''The FEVER (Fact Extraction and VERification) dataset is a benchmark designed to evaluate the ability of machine learning models to verify claims against textual sources. The main goal of the benchmark is to encourage research in the field of claim verification, which is an important task in natural language processing and information retrieval. The defining feature of the FEVER dataset is its focus on evidence-based verification. Claims are evaluated based on their supporting evidence, which is extracted from a large corpus of textual sources. The dataset includes over 185,000 claims, each of which is labeled as either Supported, Refuted, or Not Enough Information (NEI) based on the available evidence. The FEVER benchmark evaluates machine learning models on several specific aspects, including their ability to: - Identify relevant evidence from a large corpus of textual sources - Reason over the evidence to determine whether a claim is Supported, Refuted, or NEI - Handle the complexity and ambiguity of natural language - Generalize to new claims and textual sources To evaluate these aspects, the benchmark includes several subtasks, such as sentence selection, claim verification, and evidence retrieval. The benchmark also provides a baseline system and annotation interfaces to facilitate further research in this area.''',\n",
    "    'FSC': '''Few-Shot Counting (FSC) benchmark aims to evaluate the performance of machine learning models in counting objects of interest in a given image with only a few labeled examples from the same image. The benchmark comprises over 6000 images with 147 visual categories . The defining feature of the FSC benchmark is that it poses counting as a few-shot regression task, where the inputs for the counting task are an image and few examples from the same image for the object of interest, and the output is the count of object instances. The examples are provided in the form of bounding boxes around the objects of interest. In other words, the few-shot counting task deals with counting instances within an image that are similar to the exemplars from the same image . The FSC benchmark aims to test and evaluate the performance of machine learning models in handling the few-shot counting task. Specifically, it evaluates the ability of machine learning models to generalize to completely novel classes at test time, where the classes at test time are completely different from the ones seen during training. This makes few-shot counting very different from the typical counting task, where the training and test classes are the same. Unlike the typical counting task, where hundreds or thousands of labeled examples are available for training, a few-shot counting method needs to generalize to completely novel classes . To evaluate the performance of machine learning models in the FSC benchmark, the paper proposes a novel architecture called FamNet for handling the few-shot counting task, with a novel few-shot adaptation scheme at test time. The paper also performs ablation studies on the validation set of FSC-147 to analyze how the counting performance changes as the number of exemplars increases and the benefits of different components of FamNet.''',\n",
    "    'FUNSD': '''The FUNSD benchmark is a comprehensive resource for form understanding in noisy scanned documents. Its main goals are to provide a standardized dataset for evaluating and comparing different approaches to form understanding, as well as to identify the key challenges and limitations of current methods. The defining features of the FUNSD dataset include its size (199 real, fully annotated, scanned forms), its diversity (forms from different fields such as marketing, science, and advertisement), and its complexity (noisy, low-quality scans with varying levels of text density and layout). The benchmark evaluates machine learning models on several specific aspects of form understanding, including text detection, optical character recognition (OCR), spatial layout analysis, and entity labeling/linking. For text detection, the benchmark evaluates models on their ability to accurately detect words in scanned forms. For OCR, the benchmark evaluates models on their ability to accurately recognize and transcribe text from scanned forms. For spatial layout analysis, the benchmark evaluates models on their ability to accurately identify the location and orientation of text and other elements in scanned forms. Finally, for entity labeling/linking, the benchmark evaluates models on their ability to accurately identify and label different semantic entities in scanned forms (e.g., names, dates, addresses, etc.) and link them together based on their relationships. Overall, the FUNSD benchmark provides a comprehensive and challenging evaluation framework for form understanding, and can be used to identify the strengths and weaknesses of different machine learning models and approaches.''',\n",
    "    'FactPrompts': '''FactPrompts is a dataset that comprises real-world prompts sourced from various platforms and datasets, such as Quora and TruthfulQA, along with corresponding responses generated by ChatGPT. The main goal of FactPrompts is to evaluate the performance of machine learning models in detecting factual errors in generated text. The dataset is designed to test and evaluate the ability of machine learning models to identify factual errors in generated text in different scenarios, including knowledge-based question answering and open-domain question answering. The dataset includes a variety of prompts that cover a wide range of topics and domains, making it a diverse and comprehensive resource for evaluating the performance of machine learning models in detecting factual errors. One defining feature of FactPrompts is that it includes both the prompts and the corresponding responses generated by ChatGPT. This allows for a direct comparison between the generated responses and the ground truth, making it easier to evaluate the performance of machine learning models in detecting factual errors. Another important aspect of FactPrompts is that it includes fine-grained annotations for each prompt and response, including the identification of claims and evidence. This allows for a more detailed analysis of the performance of machine learning models in detecting factual errors at different levels of granularity. Overall, FactPrompts is a valuable resource for evaluating the performance of machine learning models in detecting factual errors in generated text. Its comprehensive coverage of different scenarios and its fine-grained annotations make it a useful tool for researchers and practitioners working in this area.''',\n",
    "    'FewNERD': '''The FEW-NERD benchmark is designed to evaluate the performance of machine learning models on three different tasks related to named entity recognition (NER). These tasks are: 1. FEW-NERD (SUP): This is a standard supervised task that evaluates instance-level generalization of NER methods. In other words, it tests how well a model can recognize named entities in new instances that it has not seen before, based on its training on a large annotated dataset. 2. FEW-NERD (INTRA): This is a few-shot task that evaluates type-level generalization of NER methods. In other words, it tests how well a model can recognize new types of named entities with only a few examples of each type. 3. FEW-NERD (INTER): This is another few-shot task that evaluates knowledge transfer of NER methods. In other words, it tests how well a model can recognize named entities in a new domain, based on its training on a different domain. The defining feature of FEW-NERD is that it is a large-scale human-annotated dataset with both coarse-grained and fine-grained entity types. This makes it a valuable resource for evaluating the performance of machine learning models on NER tasks, particularly in the context of few-shot learning. The specific aspects of machine learning models that FEW-NERD aims to test and evaluate include their ability to generalize to new instances and new types of named entities, as well as their ability to transfer knowledge from one domain to another. The benchmark also provides a means of comparing the performance of different machine learning models on these tasks, which can help researchers identify promising directions for future research in few-shot NER.''',\n",
    "    'FiQA': '''The main goal of this challenge is to build a Question Answering system that can answer natural language questions based on a corpus of structured and unstructured text documents from different financial data sources in English, including microblogs, reports, and news . The challenge takes both an Information Retrieval (IR) and a Question Answering (QA) perspective, where systems can rank relevant documents from the reference knowledge base with regard to a natural language question or generate their own answer . One defining feature of this challenge is that part of the questions are opinionated, targeting mined opinions and their respective entities, aspects, sentiment polarity, and opinion holder . This means that the system needs to be able to identify and extract opinions from the text, as well as understand the sentiment and the entities involved. The challenge also aims to test and evaluate specific aspects of machine learning models, such as their ability to handle natural language questions, their performance in information retrieval and ranking, and their ability to extract and analyze opinions from text . The challenge provides a training dataset of 17,110 question-answer pairs and a testing dataset of 531 question-answer pairs . Overall, this challenge provides an opportunity for researchers and practitioners to explore the potential of Natural Language Processing techniques in the financial domain, and to develop and evaluate machine learning models that can analyze financial news and opinions online.''',\n",
    "    'FinanceIQ': '''FinanceIQ is a Chinese financial domain knowledge assessment dataset. Main Goals of FinanceIQ: Targeted Domain: It focuses specifically on the financial sector, aiming to evaluate the knowledge and reasoning abilities of large language models in financial scenarios. Breadth of Coverage: The dataset encompasses a broad range of topics within finance, indicated by its inclusion of 10 major financial categories and 36 sub-categories. Comprehensive Assessment: With a total of 7,173 multiple-choice questions, FinanceIQ provides an extensive platform for testing. Defining Features: Language Specificity: The dataset is in Chinese, highlighting its utility for language models trained on or catering to Chinese language content, particularly in the financial domain. Detailed Categorization: The division into major and minor financial categories allows for nuanced assessment across various aspects of the financial industry. Format: The multiple-choice format of the questions simplifies the evaluation process and provides a clear metric for assessing model performance. Aspects of Machine Learning Models Tested: Knowledge Depth: The dataset tests for a deep understanding of financial concepts, terminologies, and practices. Reasoning Ability: It evaluates the model's capacity to apply financial knowledge in reasoning out the correct choices in multiple-choice questions. Contextual Understanding: Given the specificity of finance, the dataset assesses how well models can contextualize and interpret information within this domain. Language Processing: For models targeting Chinese language processing, this dataset serves as a benchmark for their linguistic capabilities in a specialized domain.''',\n",
    "    'Flickr30K': '''Flickr30K dataset aims to provide a large collection of images paired with descriptive captions, which can be used for various tasks such as image captioning, visual question answering, and image retrieval. The dataset contains 31,000 images, each with five captions written by different annotators, resulting in a total of 158,915 captions. The defining feature of the dataset is its focus on real-world images and natural language captions, which makes it more challenging than previous datasets that used staged or synthetic images. The captions are also more diverse and complex, covering a wide range of topics and styles. The dataset aims to test and evaluate machine learning models for tasks such as image captioning, where the goal is to generate a natural language description of an image. The dataset can also be used for tasks such as image retrieval, where the goal is to retrieve images that match a given query, and visual question answering, where the goal is to answer questions about an image based on its caption.'''\n",
    "}\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = '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')\n",
    "\n",
    "def query_gpt4_turbo(prompt, model=\"gpt-4-1106-preview\"):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a fair and objective evaluator, skilled in assessing the relevance of a benchmark dataset for assessing various capabilities.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "benchmark_capability_list = []\n",
    "\n",
    "# Example usage\n",
    "capability_list = ['reasoning']\n",
    "for capability in capability_list:\n",
    "    prompt_base = f'Your task is to evaluate a benchmark dataset based on its relevance for assessing {capability}-related capabilities. After I provide a description of the dataset, rate its relevance on a Likert scale from 1 to 5, with 1 being the least relevant and 5 being the most relevant.'\n",
    "    for key, value in benchmark_description_list.items():\n",
    "        prompt = f'{prompt_base}\\nHere is the description of the {key} dataset:\\n{value}\\nThen give me the relevance in Likert scale only:'\n",
    "        result = query_gpt4_turbo(prompt)\n",
    "        # print(result)\n",
    "        benchmark_capability_list.append({\n",
    "            'Capability': capability,\n",
    "            'Benchmark': key,\n",
    "            'Relevance': int(result),\n",
    "        })\n",
    "        # break\n",
    "\n",
    "benchmark_capability_list = pd.DataFrame(benchmark_capability_list)\n",
    "\n",
    "for name, group in benchmark_capability_list.groupby('Capability'):\n",
    "    fig = go.Figure(go.Bar(\n",
    "        x=group['Relevance'],  # Values for the bar lengths\n",
    "        y=group['Benchmark'],  # Categories for each bar\n",
    "        orientation='h'  # Sets the bars to be horizontal\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Relevance of Benchmark Datasets for Assessing {name} Capabilities',\n",
    "        xaxis_title='Relevance score',\n",
    "        yaxis_title='Benchmark dataset',\n",
    "        yaxis_autorange='reversed'  # This line makes the bars go top-down\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_description_list = {\n",
    "    'AgentBench': '''AGENTBENCH is a comprehensive benchmark designed to evaluate Large Language Models (LLMs) as agents in multi-turn open-ended generation settings. The main goals of AGENTBENCH are to standardize the evaluation of LLMs as agents and to provide a practical testbed for their wide array of capabilities. AGENTBENCH defines eight distinct environments of three types based on real-world scenarios, offering a practical testbed for LLMs' capabilities. The three types of environments are: 1. Instruction Following: where the LLM is given a set of instructions to follow and must generate a response that satisfies the instructions. 2. Decision Making: where the LLM is given a set of options and must choose the best one based on a given context. 3. Open-Ended: where the LLM is given a context and must generate a response that is coherent and relevant to the context. The benchmark evaluates LLMs based on their performance in these environments, with a focus on their long-term reasoning, decision-making, and instruction-following abilities. The benchmark also aims to identify the reasons for failures in existing LLM agents and highlight directions for improvement, such as code training and higher-quality alignment data. To evaluate the LLMs, AGENTBENCH uses a set of metrics that measure the models' performance in terms of task completion, context size constraints, return format accuracy, action accuracy, and task limitations. The benchmark also compares the performance of different LLMs, including leading API-based commercial LLMs and OSS models, to uncover significant performance gaps and highlight areas for improvement. Overall, AGENTBENCH provides a comprehensive and standardized approach to evaluating LLMs as agents, with a focus on their long-term reasoning, decision-making, and instruction-following abilities.''',\n",
    "    'BigCode': '''The Big Code Models Leaderboard on Hugging Face is a platform designed to compare the performance of base multilingual code generation models. Defining Features Performance Metrics: Models are evaluated based on their performance on the HumanEval benchmark and MultiPL-E, measuring functional correctness in synthesizing programs from docstrings. Throughput Measurement: The leaderboard also measures model throughput (tokens per second) to compare inference speed. Diverse Programming Languages: It includes benchmarks for multiple programming languages. Evaluation Parameters: Models are evaluated using the big code-evaluation-harness with specific settings. Specific Aspects Tested and Evaluated Functional Correctness: Using the HumanEval benchmark, the leaderboard assesses the ability of models to synthesize correct programs from given docstrings. Multilingual Capabilities: Through the MultiPL-E benchmark, evaluates the performance across 18 programming languages. Inference Speed: By measuring throughput, the leaderboard evaluates how quickly models can process tokens. Memory Usage: Peak memory usage is also measured, providing insights into the efficiency of the models. Scoring and Rankings: Models are ranked based on the average pass@1 score over all languages and their win rate, which represents how often a model outperforms others. Overall, this leaderboard serves as a crucial tool for the AI community, offering a comprehensive and comparative view of the capabilities of various code generation models, especially in terms of multilingual support and functional correctness.''',\n",
    "    'CLUE': '''The CLUE benchmark is a large-scale Chinese Language Understanding Evaluation designed to test and evaluate the performance of machine learning models on a variety of sentence classification and machine reading comprehension tasks in Chinese. The benchmark is unique in that it covers a wide range of tasks, at different levels of difficulty, in different sizes and forms, all on original Chinese text. The main goals of the CLUE benchmark are to provide a standardized evaluation framework for Chinese NLU models, to encourage reproducibility and transparency in research, and to promote the development of high-quality Chinese NLU models. Some of the defining features of the CLUE benchmark include a diagnostic dataset that contains nine linguistic and logic phenomena, a leaderboard for users to submit their own results, and a toolkit named PyCLUE implemented in TensorFlow that supports mainstream pre-training models and a wide range of target tasks. The specific aspects of machine learning models that the CLUE benchmark aims to test and evaluate include model size, pre-training data, and whole word masking. The benchmark results show that larger models and models trained with more pre-training data tend to perform better, as do models that use whole word masking. The best performing models on the CLUE benchmark are RoBERTa-wwm-ext-large and ALBERT-xxlarge, which show advantages over other models particularly for machine reading tasks such as C3.''',\n",
    "    'HELM': '''HELM aims to provide a standardized and comprehensive evaluation of language models across a broad range of scenarios and metrics. The benchmark aims to test and evaluate language models in terms of their accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency. Additionally, HELM includes targeted evaluations of skills and risks, such as knowledge, reasoning, disinformation, and copyright. HELM is defined by its broad coverage, which includes 16 core scenarios and 21 new scenarios that have not been previously used in mainstream language model evaluation. The benchmark evaluates 30 language models under standardized conditions, ensuring that models can be directly compared across many scenarios and metrics. These models vary in terms of their public accessibility, with 10 being open, 17 being limited-access, and 3 being closed. The specific aspects of machine learning models that HELM aims to test and evaluate include their ability to accurately predict outcomes, their ability to handle different types of inputs and prompts, their ability to avoid bias and toxicity, and their efficiency in terms of speed and resource usage. HELM also aims to evaluate the skills and risks associated with language models, such as their ability to reason and their potential for spreading disinformation. Overall, HELM provides a comprehensive and standardized approach to evaluating language models, with the goal of guiding future language model development and providing ample opportunities for further analysis.''',\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "type": "bar",
         "x": [
          5,
          5,
          3,
          5,
          5,
          4,
          5,
          5,
          4,
          5,
          4,
          3,
          4,
          5,
          3,
          4,
          4,
          4,
          4
         ],
         "y": [
          "2WikiMultihopQA",
          "A-OKVQA",
          "AFQMC",
          "AGIEval",
          "ALFWorld",
          "ANGO",
          "APPS",
          "ARC",
          "ARC-DA",
          "ActivityNet-QA",
          "AlpacaEval",
          "Amazon Review",
          "AQUA-RAT",
          "AR-LSAT",
          "ArguAna",
          "BBQ",
          "BIG-Bench",
          "BLiMP",
          "BOLD"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Relevance of Benchmark Datasets for Assessing reasoning Capabilities"
        },
        "xaxis": {
         "title": {
          "text": "Relevance score"
         }
        },
        "yaxis": {
         "title": {
          "text": "Benchmark dataset"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    fig = go.Figure(go.Bar(\n",
    "        x=[int(r) for r in group['Relevance']],  # Values for the bar lengths\n",
    "        y=group['Benchmark'],  # Categories for each bar\n",
    "        orientation='h'  # Sets the bars to be horizontal\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Relevance of Benchmark Datasets for Assessing {name} Capabilities',\n",
    "        xaxis_title='Relevance score',\n",
    "        yaxis_title='Benchmark dataset',\n",
    "        # yaxis_autorange='reversed'  # This line makes the bars go top-down\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MMLU', 11),\n",
       " ('HellaSwag', 7),\n",
       " ('HumanEval', 6),\n",
       " ('BoolQ', 5),\n",
       " ('TruthfulQA', 4),\n",
       " ('ARC', 4),\n",
       " ('GSM8K', 4),\n",
       " ('DROP', 4),\n",
       " ('CHID', 3),\n",
       " ('NarrativeQA', 3),\n",
       " ('OpenbookQA', 3),\n",
       " ('WinoGrande', 3),\n",
       " ('TriviaQA', 3),\n",
       " ('Grounded', 3),\n",
       " ('IMDB', 2),\n",
       " ('RAFT', 2),\n",
       " ('CMMLU', 2),\n",
       " ('CSL', 2),\n",
       " ('CLUEWSC', 2),\n",
       " ('EPRSTMT', 2),\n",
       " ('OCNLI', 2),\n",
       " ('MS-COCO', 2),\n",
       " ('Winoground', 2),\n",
       " ('MATH', 2),\n",
       " ('NaturalQuestions', 2),\n",
       " ('QuAC', 2),\n",
       " ('BBH', 2),\n",
       " ('MBPP', 2),\n",
       " ('ScienceQA', 2),\n",
       " ('LAMBADA', 2),\n",
       " ('SIQA', 2),\n",
       " ('WiC', 2),\n",
       " ('GAOKAO-Bench', 2),\n",
       " ('AGIEval', 2),\n",
       " ('WSC', 2),\n",
       " ('C3', 2),\n",
       " ('CMNLI', 2),\n",
       " ('RTE', 2),\n",
       " ('ReCoRD', 2),\n",
       " ('IC15', 2),\n",
       " ('COCO-Text', 2),\n",
       " ('TextOCR', 2),\n",
       " ('ALFWorld', 1),\n",
       " ('WebShop', 1),\n",
       " ('Mind2Web', 1),\n",
       " ('MultiPL-E', 1),\n",
       " ('MT-Bench', 1),\n",
       " ('Chatbot', 1),\n",
       " ('Arena', 1),\n",
       " ('Conversations', 1),\n",
       " ('GAOKAO-Bench-2023', 1),\n",
       " ('LLSRC', 1),\n",
       " ('SLSRC', 1),\n",
       " ('SLPWC', 1),\n",
       " ('SLRFC', 1),\n",
       " ('TNEWS', 1),\n",
       " ('BUSTM', 1),\n",
       " ('gnad10', 1),\n",
       " ('Belebele', 1),\n",
       " ('XNLI', 1),\n",
       " ('Amazon', 1),\n",
       " ('Review', 1),\n",
       " ('Caltech-UCSD', 1),\n",
       " ('Birds-200-2011', 1),\n",
       " ('Common', 1),\n",
       " ('Syntactic', 1),\n",
       " ('Processes', 1),\n",
       " ('Demographic', 1),\n",
       " ('Stereotypes', 1),\n",
       " ('PaintSkills', 1),\n",
       " ('I2P', 1),\n",
       " ('Magazine', 1),\n",
       " ('Cover', 1),\n",
       " ('Photos', 1),\n",
       " ('Mental', 1),\n",
       " ('Disorders', 1),\n",
       " ('P2', 1),\n",
       " ('Relational', 1),\n",
       " ('Understanding', 1),\n",
       " (\"TIME's\", 1),\n",
       " ('most', 1),\n",
       " ('significant', 1),\n",
       " ('historical', 1),\n",
       " ('figures', 1),\n",
       " ('dailydall.e', 1),\n",
       " ('APPS', 1),\n",
       " ('BBQ', 1),\n",
       " ('BLiMP', 1),\n",
       " ('BOLD', 1),\n",
       " ('CNN/DailyMail', 1),\n",
       " ('CivilComments', 1),\n",
       " ('Data', 1),\n",
       " ('imputation', 1),\n",
       " ('Disinformation', 1),\n",
       " ('Dyck', 1),\n",
       " ('Entity', 1),\n",
       " ('matching', 1),\n",
       " ('ICE', 1),\n",
       " ('AR-LSAT', 1),\n",
       " ('LegalSupport', 1),\n",
       " ('MS', 1),\n",
       " ('MARCO', 1),\n",
       " ('RealToxicityPrompts', 1),\n",
       " ('LIME', 1),\n",
       " ('WikiFact', 1),\n",
       " ('XSUM', 1),\n",
       " ('bAbI', 1),\n",
       " ('Synthetic', 1),\n",
       " ('efficiency', 1),\n",
       " ('The', 1),\n",
       " ('Pile', 1),\n",
       " ('TwitterAAE', 1),\n",
       " ('CRASS', 1),\n",
       " ('IMPACT', 1),\n",
       " ('HHH', 1),\n",
       " ('Spider', 1),\n",
       " ('NL2Bash', 1),\n",
       " ('AR', 1),\n",
       " ('ER', 1),\n",
       " ('NER', 1),\n",
       " ('JS', 1),\n",
       " ('CR', 1),\n",
       " ('CFM', 1),\n",
       " ('SCM', 1),\n",
       " ('CJP', 1),\n",
       " ('CTP', 1),\n",
       " ('LQA', 1),\n",
       " ('JRG', 1),\n",
       " ('CU', 1),\n",
       " ('LC', 1),\n",
       " ('CIFAR-10', 1),\n",
       " ('Flickr30k', 1),\n",
       " ('VOC2012', 1),\n",
       " ('Omnibenchmark', 1),\n",
       " ('FSC147', 1),\n",
       " ('MMBench', 1),\n",
       " ('SEED-Bench', 1),\n",
       " ('Chatbot-Arena', 1),\n",
       " ('HotpotQA', 1),\n",
       " ('2WikiMultihopQA', 1),\n",
       " ('MuSiQue', 1),\n",
       " ('DuReader', 1),\n",
       " ('MultiFieldQA', 1),\n",
       " ('Qasper', 1),\n",
       " ('GovReport', 1),\n",
       " ('QMSum', 1),\n",
       " ('Multi-News', 1),\n",
       " ('VCSUM', 1),\n",
       " ('SAMSum', 1),\n",
       " ('TREC', 1),\n",
       " ('LSHTC', 1),\n",
       " ('PassageRetrieval', 1),\n",
       " ('PassageCount', 1),\n",
       " ('LCC', 1),\n",
       " ('RepoBench-P', 1),\n",
       " ('MCScript', 1),\n",
       " ('CosmosQA', 1),\n",
       " ('Quoref', 1),\n",
       " ('CommonGen', 1),\n",
       " ('C-Eval', 1),\n",
       " ('AFQMC', 1),\n",
       " ('TyDiQA', 1),\n",
       " ('Flores', 1),\n",
       " ('CommonSenseQA', 1),\n",
       " ('RACE', 1),\n",
       " ('LCSTS', 1),\n",
       " ('XSum', 1),\n",
       " ('AX-b', 1),\n",
       " ('AX-g', 1),\n",
       " ('PIQA', 1),\n",
       " ('M3KE', 1),\n",
       " ('TGEA', 1),\n",
       " ('OL-CC', 1),\n",
       " ('CSNLI', 1),\n",
       " ('ChineseSquad', 1),\n",
       " ('WPLC', 1),\n",
       " ('BiPaR', 1),\n",
       " ('CommonMT', 1),\n",
       " ('TOCP', 1),\n",
       " ('SWSR', 1),\n",
       " ('CORGI-PM', 1),\n",
       " ('CDIAL-BIAS', 1),\n",
       " ('COLD', 1),\n",
       " ('CBBQ', 1),\n",
       " ('TUMCC', 1),\n",
       " ('Cooridinate', 1),\n",
       " ('AI', 1),\n",
       " ('Corrigible', 1),\n",
       " ('Myopia', 1),\n",
       " ('Reward', 1),\n",
       " ('One-box', 1),\n",
       " ('Tendency', 1),\n",
       " ('Power-seeking', 1),\n",
       " ('Self-awareness', 1),\n",
       " ('CAIL判决预测数据集', 1),\n",
       " ('KQApro', 1),\n",
       " ('LC-quad2', 1),\n",
       " ('WQSP', 1),\n",
       " ('CWQ', 1),\n",
       " ('GrailQA', 1),\n",
       " ('GraphQ', 1),\n",
       " ('QALD-9', 1),\n",
       " ('MKQA', 1),\n",
       " ('FewNERD', 1),\n",
       " ('FewRel', 1),\n",
       " ('InstructIE', 1),\n",
       " ('MAVEN', 1),\n",
       " ('WikiEvents', 1),\n",
       " ('Flowers102', 1),\n",
       " ('CIFAR10', 1),\n",
       " ('ImageNet-1K', 1),\n",
       " ('Pets37', 1),\n",
       " ('VizWiz-yesno', 1),\n",
       " ('VizWiz-singleChoice', 1),\n",
       " ('TDIUC-Sport', 1),\n",
       " ('TDIUC-Scene', 1),\n",
       " ('MEDIC', 1),\n",
       " ('MSCOCO-MCI', 1),\n",
       " ('MSCOCO-GOI', 1),\n",
       " ('MSCOCO-MOS', 1),\n",
       " ('TDIUC-Color', 1),\n",
       " ('TDIUC-Utility', 1),\n",
       " ('TDIUC-Position', 1),\n",
       " ('TDIUC-Detection', 1),\n",
       " ('TDIUC-Counting', 1),\n",
       " ('RefCOCO', 1),\n",
       " ('MSCOCO-OC', 1),\n",
       " ('VQA', 1),\n",
       " ('v2', 1),\n",
       " ('GQA', 1),\n",
       " ('Whoops', 1),\n",
       " ('OK-VQA', 1),\n",
       " ('VizWiz', 1),\n",
       " ('ViQuAE', 1),\n",
       " ('K-ViQuAE', 1),\n",
       " ('A-OKVQA', 1),\n",
       " ('A-OKVQRA', 1),\n",
       " ('A-OKVQAR', 1),\n",
       " ('ImageNetVC', 1),\n",
       " ('CLEVR', 1),\n",
       " ('VSR', 1),\n",
       " ('MP3D', 1),\n",
       " ('VQA-MT', 1),\n",
       " ('VisDial', 1),\n",
       " ('MSCOCO-ITM', 1),\n",
       " ('MSCOCO-ITS', 1),\n",
       " ('WikiHow', 1),\n",
       " ('SNLI-VE', 1),\n",
       " ('MOCHEG', 1),\n",
       " ('CUTE80', 1),\n",
       " ('IIIT5K', 1),\n",
       " ('WordArt', 1),\n",
       " ('FUNSD', 1),\n",
       " ('POIE', 1),\n",
       " ('SROIE', 1),\n",
       " ('TextVQA', 1),\n",
       " ('DocVQA', 1),\n",
       " ('OCR-VQA', 1),\n",
       " ('MSCOCO', 1),\n",
       " ('TextCaps', 1),\n",
       " ('NoCaps', 1),\n",
       " ('Flickr30K', 1),\n",
       " ('CommitmentBank', 1),\n",
       " ('COPA', 1),\n",
       " ('MultiRC', 1),\n",
       " ('Broadcoverage', 1),\n",
       " ('Diagnostics', 1),\n",
       " ('Winogender', 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''ALFWorld\tWebShop\tMind2Web\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MultiPL-E\tHumanEval\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MMLU\tMT-Bench\tChatbot Arena Conversations\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "BoolQ\tMMLU\tTruthfulQA\tIMDB\tRAFT\tCMMLU\tGAOKAO-Bench-2023\tCSL\tCHID\tCLUEWSC\tLLSRC\tSLSRC\tSLPWC\tSLRFC\tEPRSTMT\tTNEWS\tOCNLI\tBUSTM\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MMLU\tgnad10\tHellaSwag\tARC\tBelebele\tXNLI\tAmazon Review\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "Caltech-UCSD Birds-200-2011\tCommon Syntactic Processes\tDemographic Stereotypes\tPaintSkills\tI2P\tMS-COCO\tMagazine Cover Photos\tMental Disorders\tP2\tRelational Understanding\tTIME\tWinoground\tdailydall.e\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "APPS\tBBQ\tBLiMP\tBOLD\tBoolQ\tCNN/DailyMail\tCivilComments\tData imputation\tDisinformation\tDyck\tEntity matching\tGSM8K\tHellaSwag\tHumanEval\tICE\tIMDB\tAR-LSAT\tLegalSupport\tMATH\tMMLU\tMS MARCO\tNarrativeQA\tNaturalQuestions\tOpenbookQA\tQuAC\tRAFT\tRealToxicityPrompts\tLIME\tWikiFact\tXSUM\tbAbI\tSynthetic efficiency\tThe Pile\tTruthfulQA\tTwitterAAE\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "ARC\tHellaSwag\tMMLU\tTruthfulQA\tWinoGrande\tGSM8K\tDROP\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MMLU\tBBH\tDROP\tCRASS\tHumanEval\tIMPACT\tHHH\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MBPP\tSpider\tNL2Bash\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "AR\tER\tNER\tJS\tCR\tCFM\tSCM\tCJP\tCTP\tLQA\tJRG\tCU\tLC\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "CIFAR-10\tFlickr30k\tVOC2012\tOmnibenchmark\tFSC147\tScienceQA\tMMBench\tSEED-Bench\tMS-COCO\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "Chatbot-Arena\tHellaSwag\tHumanEval\tLAMBADA\tMMLU\tTriviaQA\tWinoGrande\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "HotpotQA\t2WikiMultihopQA\tMuSiQue\tDuReader\tMultiFieldQA\tNarrativeQA\tQasper\tGovReport\tQMSum\tMulti-News\tVCSUM\tTriviaQA\tSAMSum\tTREC\tLSHTC\tPassageRetrieval\tPassageCount\tLCC\tRepoBench-P\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "NarrativeQA\tMCScript\tCosmosQA\tSIQA\tDROP\tQuoref\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "HellaSwag\tMMLU\tARC\tTruthfulQA\tCommonGen\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "C-Eval\tMMLU\tARC\tCHID\tWiC\tGAOKAO-Bench\tCMMLU\tAGIEval\tAFQMC\tWSC\tTyDiQA\tFlores\tBoolQ\tCommonSenseQA\tC3\tRACE\tOpenbookQA\tCSL\tLCSTS\tXSum\tEPRSTMT\tLAMBADA\tCMNLI\tOCNLI\tAX-b\tAX-g\tRTE\tReCoRD\tHellaSwag\tPIQA\tSIQA\tMATH\tGSM8K\tDROP\tHumanEval\tMBPP\tBBH\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "M3KE\tMMLU\tGAOKAO-Bench\tTGEA\tOL-CC\tCSNLI\tCLUEWSC\tChineseSquad\tCHID\tWPLC\tBiPaR\tCommonMT\tC3\tCMNLI\tTOCP\tSWSR\tCORGI-PM\tCDIAL-BIAS\tCOLD\tCBBQ\tTUMCC\tCooridinate AI\tCorrigible\tMyopia Reward\tOne-box Tendency\tPower-seeking\tSelf-awareness\tCAIL判决预测数据集\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "KQApro\tLC-quad2\tWQSP\tCWQ\tGrailQA\tGraphQ\tQALD-9\tMKQA\tFewNERD\tFewRel\tInstructIE\tMAVEN\tWikiEvents\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "Flowers102\tCIFAR10\tImageNet-1K\tPets37\tVizWiz-yesno\tVizWiz-singleChoice\tTDIUC-Sport\tTDIUC-Scene\tMEDIC\tMSCOCO-MCI\tMSCOCO-GOI\tMSCOCO-MOS\tTDIUC-Color\tTDIUC-Utility\tTDIUC-Position\tTDIUC-Detection\tTDIUC-Counting\tRefCOCO\tMSCOCO-OC\tVQA v2\tGQA\tWhoops\tOK-VQA\tScienceQA\tVizWiz\tViQuAE\tK-ViQuAE\tA-OKVQA\tA-OKVQRA\tA-OKVQAR\tImageNetVC\tCLEVR\tVSR\tMP3D\tVQA-MT\tVisDial\tMSCOCO-ITM\tMSCOCO-ITS\tWikiHow\tWinoground\tSNLI-VE\tMOCHEG\tGrounded IC15\tIC15\tGrounded COCO-Text\tCOCO-Text\tGrounded TextOCR\tTextOCR\tCUTE80\tIIIT5K\tWordArt\tFUNSD\tPOIE\tSROIE\tTextVQA\tDocVQA\tOCR-VQA\tMSCOCO\tTextCaps\tNoCaps\tFlickr30K\n",
    "BoolQ\tCommitmentBank\tCOPA\tMultiRC\tReCoRD\tRTE\tWiC\tWSC\tBroadcoverage Diagnostics\tWinogender\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MMLU\tTriviaQA\tNaturalQuestions\tGSM8K\tHumanEval\tAGIEval\tBoolQ\tHellaSwag\tOpenbookQA\tQuAC\tWinoGrande'''\n",
    "\n",
    "from collections import defaultdict\n",
    "df = defaultdict(int)\n",
    "for items in text.split('\\n'):\n",
    "    for item in items.split():\n",
    "        df[item] += 1\n",
    "\n",
    "sorted(df.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = '''10-bin expected calibration error\n",
    "1-bin expected calibration error\n",
    "Max prob\n",
    "10-bin expected calibration error\n",
    "1-bin expected calibration error (after Platt scaling)\n",
    "10-bin Expected Calibration Error (after Platt scaling)\n",
    "Platt Scaling Coefficient\n",
    "Platt Scaling Intercept\n",
    "Selective coverage-accuracy area\n",
    "Accuracy at 10% coverage\n",
    "Stereotypical associations (race, profession)\n",
    "Stereotypical associations (gender, profession)\n",
    "Demographic representation (race)\n",
    "Demographic representation (gender)\n",
    "Toxic fraction\n",
    "Denoised inference runtime (s)\n",
    "Estimated training emissions (kg CO2)\n",
    "Estimated training energy cost (MWh)\n",
    "Observed inference runtime (s)\n",
    "Idealized inference runtime (s)\n",
    "Denoised inference runtime (s)\n",
    "# trials\n",
    "# prompt tokens\n",
    "# output tokens\n",
    "# eval\n",
    "# train\n",
    "truncated\n",
    "SummaC\n",
    "QAFactEval\n",
    "Coverage\n",
    "Density\n",
    "Compression\n",
    "BERTScore (F1)\n",
    "HumanEval-faithfulness\n",
    "HumanEval-relevance\n",
    "HumanEval-coherence\n",
    "Avg. # tests passed\n",
    "Strict correctness\n",
    "BBQ (ambiguous)\n",
    "BBQ (unambiguous)\n",
    "Longest common prefix length\n",
    "Edit distance (Levenshtein)\n",
    "Edit similarity (Levenshtein)\n",
    "Self-BLEU\n",
    "Entropy (Monte Carlo)\n",
    "Macro-F1\n",
    "Micro-F1\n",
    "Chinese iBLEU\n",
    "Chinese Top-1 Accuracy\n",
    "Chinese ROUGE-2 score\n",
    "Chinese BLEU-1 score\n",
    "CLEVA Math Exact Match\n",
    "Chinese BLEU-1 score\n",
    "Chinese BLEU-1 score'''\n",
    "\n",
    "# s = set()\n",
    "df = pd.DataFrame()\n",
    "for scenario in metrics.split('\\n'):\n",
    "    # scenario_ = scenario.split('(')[0].strip()\n",
    "    s = {\n",
    "        'Name': scenario\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame([s])], ignore_index=True)\n",
    "    # s.add(scenario)\n",
    "# len(df)\n",
    "df.to_csv('metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenarios = '''BoolQ\n",
    "# NarrativeQA\n",
    "# NaturalQuestions (closed-book)\n",
    "# NaturalQuestions (open-book)\n",
    "# QuAC (Question Answering in Context)\n",
    "# HellaSwag\n",
    "# OpenbookQA\n",
    "# TruthfulQA\n",
    "# MMLU (Massive Multitask Language Understanding)\n",
    "# MS MARCO (regular track)\n",
    "# MS MARCO (TREC track)\n",
    "# CNN/DailyMail\n",
    "# XSUM\n",
    "# IMDB\n",
    "# CivilComments\n",
    "# RAFT (Real-world Annotated Few-Shot)\n",
    "# ICE (International Corpus of English)\n",
    "# The Pile\n",
    "# TwitterAAE\n",
    "# BLiMP (The Benchmark of Linguistic Minimal Pairs for English)\n",
    "# NaturalQuestions (closed-book)\n",
    "# HellaSwag\n",
    "# OpenbookQA\n",
    "# TruthfulQA\n",
    "# MMLU (Massive Multitask Language Understanding)\n",
    "# WikiFact\n",
    "# bAbI\n",
    "# Dyck\n",
    "# Synthetic reasoning (abstract symbols)\n",
    "# Synthetic reasoning (natural language)\n",
    "# GSM8K (Grade school math word problems)\n",
    "# MATH\n",
    "# MATH (chain-of-thoughts)\n",
    "# APPS (Code)\n",
    "# HumanEval (Code)\n",
    "# LegalSupport\n",
    "# LSAT\n",
    "# Data imputation\n",
    "# Entity matching\n",
    "# Copyright (text)\n",
    "# Copyright (code)\n",
    "# Disinformation (reiteration)\n",
    "# Disinformation (wedging)\n",
    "# BBQ (Bias Benchmark for Question Answering)\n",
    "# BOLD (Bias in Open-Ended Language Generation Dataset)\n",
    "# RealToxicityPrompts\n",
    "# Synthetic efficiency\n",
    "# MMLU (Massive Multitask Language Understanding)\n",
    "# IMDB\n",
    "# RAFT (Real-world Annotated Few-Shot)\n",
    "# CivilComments\n",
    "# NaturalQuestions (open-book)\n",
    "# CNN/DailyMail\n",
    "# IMDB\n",
    "# CivilComments\n",
    "# HellaSwag\n",
    "# OpenbookQA\n",
    "# TruthfulQA\n",
    "# MMLU (Massive Multitask Language Understanding)\n",
    "# BLiMP (The Benchmark of Linguistic Minimal Pairs for English)\n",
    "# LegalSupport\n",
    "# LSAT\n",
    "# BBQ (Bias Benchmark for Question Answering)\n",
    "# NaturalQuestions (open-book)\n",
    "# CNN/DailyMail\n",
    "# IMDB\n",
    "# CivilComments\n",
    "# BoolQ\n",
    "# IMDB'''\n",
    "# s = set()\n",
    "# for scenario in scenarios.split('\\n'):\n",
    "#     scenario_ = scenario.split('(')[0].strip()\n",
    "#     s.add(scenario_)\n",
    "# # len(s)\n",
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenarios = '''MS-COCO (base)\n",
    "# Caltech-UCSD Birds-200-2011\n",
    "# DrawBench (image quality categories)\n",
    "# DrawBench (reasoning categories)\n",
    "# DrawBench (knowledge categories)\n",
    "# PartiPrompts (image quality categories)\n",
    "# PartiPrompts (reasoning categories)\n",
    "# PartiPrompts (knowledge categories)\n",
    "# MS-COCO (base)\n",
    "# DrawBench (image quality categories)\n",
    "# PartiPrompts (image quality categories)\n",
    "# MS-COCO (base)\n",
    "# MS-COCO (Art styles)\n",
    "# dailydall.e\n",
    "# Logos\n",
    "# Landing Page\n",
    "# Magazine Cover Photos\n",
    "# dailydall.e\n",
    "# Logos\n",
    "# Landing Page\n",
    "# Magazine Cover Photos\n",
    "# Common Syntactic Processes\n",
    "# DrawBench (reasoning categories)\n",
    "# PartiPrompts (reasoning categories)\n",
    "# Relational Understanding\n",
    "# Detection (PaintSkills)\n",
    "# Winoground\n",
    "# TIME\n",
    "# DrawBench (knowledge categories)\n",
    "# PartiPrompts (knowledge categories)\n",
    "# Demographic Stereotypes\n",
    "# Mental Disorders\n",
    "# Inappropriate Image Prompts (I2P)\n",
    "# MS-COCO (fairness - AAVE dialect)\n",
    "# MS-COCO (fairness - gender)\n",
    "# MS-COCO (robustness)\n",
    "# MS-COCO (Chinese)\n",
    "# MS-COCO (Hindi)\n",
    "# MS-COCO (Spanish)\n",
    "# MS-COCO Fidelity\n",
    "# MS-COCO Efficiency\n",
    "# MS-COCO (Art styles)'''\n",
    "# s = set()\n",
    "# for scenario in scenarios.split('\\n'):\n",
    "#     scenario = scenario.split('(')[0].strip()\n",
    "#     s.add(scenario)\n",
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1223"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "sum = 0\n",
    "\n",
    "for file in glob.glob(str(path_llm / \"*.json\")):\n",
    "    if 'imagenet' not in file.lower():\n",
    "        continue\n",
    "    df = pd.read_json(file)\n",
    "    sum += len(df)\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('/Users/jimmy/Downloads/LMExamQA.json')\n",
    "df['domain'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ChatGLM2-6B</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-4</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bloom-7b1</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-7B</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baichuan2-13B-Chat</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGPT</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen-7B-Chat</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ImageLLM</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen-14B-Chat</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InternLM-7B</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count\n",
       "model                    \n",
       "ChatGLM2-6B            41\n",
       "GPT-4                  34\n",
       "bloom-7b1              33\n",
       "llama-7B               31\n",
       "Baichuan2-13B-Chat     31\n",
       "ChatGPT                30\n",
       "Qwen-7B-Chat           27\n",
       "ImageLLM               27\n",
       "Qwen-14B-Chat          25\n",
       "InternLM-7B            22"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_files = path_llm.glob('*.json')\n",
    "\n",
    "# Function to read a JSON file into a DataFrame\n",
    "def read_json_file(file_path):\n",
    "    return pd.read_json(file_path)\n",
    "\n",
    "# Function to find a column that contains 'model' in its name\n",
    "def find_model_column(df):\n",
    "    for col in df.columns:\n",
    "        if 'model' in col.lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "dataframes = []\n",
    "# Read each file and store in a list of DataFrames\n",
    "for file in json_files:\n",
    "    if 'HELM' in file.name:\n",
    "        continue\n",
    "    dataframes.append(read_json_file(file))\n",
    "\n",
    "# Extract and count models in each DataFrame\n",
    "model_counts = []\n",
    "for i, df in enumerate(dataframes):\n",
    "    model_col = find_model_column(df)\n",
    "    if (model_col is not None) and (type(df[model_col][0]) == str):\n",
    "        count = df[model_col].fillna('').apply(lambda x: x.split('\\n')[0]).value_counts().reset_index()\n",
    "        count.columns = ['model', 'count']\n",
    "        model_counts.append(count)\n",
    "\n",
    "combined_df = pd.concat(model_counts, ignore_index=True)\n",
    "combined_df.groupby('model').sum().sort_values('count', ascending=False).head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
