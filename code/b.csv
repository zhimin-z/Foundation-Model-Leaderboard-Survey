Model,Mean win rate,MMLU - EM,BoolQ - EM,NarrativeQA - F1,NaturalQuestions (closed-book) - F1,NaturalQuestions (open-book) - F1,QuAC - F1,HellaSwag - EM,OpenbookQA - EM,TruthfulQA - EM
Palmyra X (43B),0.971,0.609,0.896,0.742,0.413,-,0.473,-,-,0.616
text-davinci-003,0.957,0.569,0.881,0.727,0.406,0.77,0.525,0.822,0.646,0.593
Llama 2 (70B),0.953,0.582,0.886,0.77,0.458,0.674,0.484,-,-,0.554
text-davinci-002,0.929,0.568,0.877,0.727,0.383,0.713,0.445,0.815,0.594,0.61
LLaMA (65B),0.913,0.584,0.871,0.755,0.431,0.672,0.401,-,-,0.508
Cohere Command beta (52.4B),0.858,0.452,0.856,0.752,0.372,0.76,0.432,0.811,0.582,0.269
LLaMA (30B),0.858,0.531,0.861,0.752,0.408,0.666,0.39,-,-,0.344
Jurassic-2 Jumbo (178B),0.837,0.48,0.829,0.733,0.385,0.669,0.435,0.788,0.558,0.437
Llama 2 (13B),0.814,0.507,0.811,0.744,0.376,0.637,0.424,-,-,0.33
Anthropic-LM v4-s3 (52B),0.804,0.481,0.815,0.728,0.288,0.686,0.431,0.807,0.558,0.368
Vicuna v1.3 (13B),0.789,0.462,0.808,0.691,0.346,0.686,0.403,-,-,0.385
gpt-3.5-turbo-0301,0.775,0.59,0.74,0.663,0.39,0.624,0.512,-,-,0.609
Jurassic-2 Grande (17B),0.773,0.475,0.826,0.737,0.356,0.639,0.418,0.781,0.542,0.348
TNLG v2 (530B),0.764,0.469,0.809,0.722,0.384,0.642,0.39,0.799,0.562,0.251
gpt-3.5-turbo-0613,0.761,0.391,0.87,0.625,0.348,0.675,0.485,-,-,0.339
Falcon-Instruct (40B),0.751,0.497,0.829,0.625,0.377,0.666,0.371,-,-,0.384
Falcon (40B),0.744,0.509,0.819,0.673,0.392,0.675,0.307,-,-,0.353
J1-Grande v2 beta (17B),0.723,0.445,0.812,0.725,0.337,0.625,0.392,0.764,0.56,0.306
MPT-Instruct (30B),0.706,0.444,0.85,0.733,0.304,0.697,0.327,-,-,0.234
MPT (30B),0.67,0.437,0.704,0.732,0.347,0.673,0.393,-,-,0.231
Llama 2 (7B),0.67,0.431,0.762,0.691,0.337,0.611,0.406,-,-,0.272
Cohere xlarge v20221108 (52.4B),0.636,0.382,0.762,0.672,0.361,0.628,0.374,0.81,0.588,0.169
OPT (175B),0.633,0.318,0.793,0.671,0.297,0.615,0.36,0.791,0.586,0.25
Cohere Command beta (6.1B),0.629,0.406,0.798,0.709,0.229,0.717,0.375,0.752,0.55,0.203
Vicuna v1.3 (7B),0.627,0.434,0.76,0.643,0.287,0.634,0.392,-,-,0.292
Luminous Supreme (70B),0.621,0.38,0.775,0.711,0.293,0.649,0.37,-,-,0.222
LLaMA (13B),0.611,0.422,0.714,0.711,0.346,0.614,0.347,-,-,0.324
davinci (175B),0.597,0.422,0.722,0.687,0.329,0.625,0.36,0.775,0.586,0.194
InstructPalmyra (30B),0.55,0.403,0.751,0.496,0.33,0.682,0.433,-,-,0.185
Cohere xlarge v20220609 (52.4B),0.538,0.353,0.718,0.65,0.312,0.595,0.361,0.811,0.55,0.198
LLaMA (7B),0.53,0.321,0.756,0.669,0.297,0.589,0.338,-,-,0.28
Luminous Extended (30B),0.508,0.321,0.767,0.665,0.254,0.609,0.349,-,-,0.221
GLM (130B),0.478,0.344,0.784,0.706,0.148,0.642,0.272,-,-,0.218
J1-Jumbo v1 (178B),0.476,0.259,0.776,0.695,0.293,0.595,0.358,0.765,0.534,0.175
Jurassic-2 Large (7.5B),0.473,0.339,0.742,-,0.274,0.589,-,0.729,0.53,0.245
RedPajama-INCITE-Instruct (7B),0.459,0.363,0.705,0.638,0.232,0.659,0.26,-,-,0.243
OPT (66B),0.455,0.276,0.76,0.638,0.258,0.596,0.357,0.745,0.534,0.201
BLOOM (176B),0.451,0.299,0.704,0.662,0.216,0.621,0.361,0.744,0.534,0.205
Falcon (7B),0.434,0.286,0.753,0.621,0.285,0.579,0.332,-,-,0.234
Alpaca (7B),0.427,0.385,0.778,0.396,0.266,0.592,0.27,-,-,0.243
J1-Grande v1 (17B),0.408,0.27,0.722,0.672,0.233,0.578,0.362,0.739,0.52,0.193
Cohere large v20220720 (13.1B),0.403,0.324,0.725,0.625,0.232,0.573,0.338,0.736,0.542,0.181
RedPajama-INCITE-Base (7B),0.377,0.302,0.713,0.617,0.25,0.586,0.336,-,-,0.205
GPT-NeoX (20B),0.331,0.276,0.683,0.599,0.193,0.596,0.326,0.718,0.524,0.216
RedPajama-INCITE-Instruct-v1 (3B),0.32,0.257,0.677,0.638,0.203,0.637,0.259,-,-,0.208
Cohere medium v20221108 (6.1B),0.314,0.254,0.7,0.61,0.199,0.517,0.314,0.726,0.538,0.215
RedPajama-INCITE-Base-v1 (3B),0.305,0.263,0.685,0.555,0.207,0.52,0.309,-,-,0.277
Luminous Base (13B),0.287,0.27,0.719,0.605,0.202,0.568,0.334,-,-,0.182
text-curie-001,0.282,0.237,0.62,0.582,0.175,0.571,0.358,0.676,0.514,0.257
T0pp (11B),0.264,0.407,0,0.151,0.039,0.19,0.121,-,-,0.377
TNLG v2 (6.7B),0.259,0.242,0.698,0.631,0.21,0.561,0.345,0.704,0.478,0.167
curie (6.7B),0.257,0.243,0.656,0.604,0.199,0.552,0.321,0.682,0.502,0.232
Falcon-Instruct (7B),0.257,0.275,0.72,0.476,0.194,0.449,0.311,-,-,0.213
J1-Large v1 (7.5B),0.239,0.241,0.683,0.623,0.19,0.532,0.328,0.7,0.514,0.197
Pythia (12B),0.224,0.274,0.662,0.596,0.175,0.581,0.313,-,-,0.177
GPT-J (6B),0.214,0.249,0.649,0.545,0.156,0.559,0.33,0.663,0.514,0.199
Cohere medium v20220720 (6.1B),0.212,0.279,0.659,0.559,0.177,0.504,0.279,0.706,0.496,0.19
UL2 (20B),0.208,0.291,0.746,0.083,0.204,0.349,0.144,-,-,0.193
T5 (11B),0.191,0.29,0.761,0.086,0.194,0.477,0.116,-,-,0.133
Pythia (6.9B),0.171,0.236,0.631,0.528,0.142,0.539,0.296,-,-,0.213
text-babbage-001,0.135,0.229,0.451,0.429,0.07,0.33,0.284,0.561,0.452,0.233
Cohere small v20220720 (410M),0.114,0.264,0.457,0.294,0.078,0.309,0.219,0.483,0.348,0.217
ada (350M),0.11,0.243,0.581,0.326,0.082,0.365,0.242,0.435,0.38,0.215
babbage (1.3B),0.103,0.235,0.574,0.491,0.119,0.451,0.273,0.555,0.438,0.188
YaLM (100B),0.097,0.243,0.634,0.252,0.068,0.227,0.162,-,-,0.202
text-ada-001,0.079,0.238,0.464,0.238,0.025,0.149,0.176,0.429,0.346,0.232