赵志民：你好，徐老师。因为你是一线关于模型测评最有权威的，尤其是在国内中文测评方面的技术专家，所以我想听一下你对我们研究的一些看法，做一个口头的反馈。正好我最近在进行论文的rebuttal阶段，我不知道你是否了解，就是专家对审稿意见的反馈过程。当时那边的审稿人提到，一直没有真实的从业者进行反馈，所以这次我才联系到你，看看你能不能基于您的经验给我们一些反馈。嗯，背景就是这样的，相当于我有一篇论文现在处于审稿状态中。其实这次讨论的主要内容不仅仅是这个，而是更广泛的内容。

徐亮：了解，可以的。

赵志民：然后我打算先共享一下我的屏幕，想讲一下我们的工作，这样也方便我寻求你的反馈。行吗？OK，那我共享一下。

徐亮：嗯，OK。

赵志民：你能看到吗？

徐亮：嗯，能看到，能看到。

赵志民：好的，情况是这样的，我带您过一遍我们的研究。之前我已经把现在的稿件Draft发给你了，这样你在详细看的时候能够更快理解。我们主要做的是关于榜单的运维，特别是大模型榜单的运维，主要讨论它的工作流程以及类似于代码smell的东西，我们称之为“榜单smell”。你有没有听说过code smell？

徐亮：没什么印象。

赵志民：Code smell，比如我写代码的时候，明明可以用一个循环语句解决问题，但却用了很多if语句。这样很容易出错，对吧？这种违反最佳实践的反模式可能会导致潜在问题，我们称之为smell。代码中的这种smell，我们现在把这个概念移植到榜单上，所以叫做“leaderboard smell”。

徐亮：嗯，了解。

赵志民：对，这是我们主要的研究目标之一。我们首先搜集了不同榜单的链接，大概有一万五千多个榜单链接。然后我们设定了一些标准，比如这个榜单必须和大模型相关，并且是可访问的，不会出现谁都无法访问的情况等等。

徐亮：嗯。

赵志民：经过过滤之后，我们最终得到了近乎一千零五十个大模型榜单。比如你可以看到，Hugging Face上有一百七十八个榜单，Paper with Code上有五百六十九个榜单。还有一点值得注意的是，有大约一百四十七个榜单是在独立平台上，比如SuperCLUE，很多榜单都在像SuperCLUE这种独立平台上。

徐亮：嗯，明白。

赵志民：然后我们通过研究这些榜单的文档以及它可能的出版物，比如一些相关的论文等资料，进一步查看是否有对应的Github仓库。如果有，我们还会分析它的commit history message等内容。当然，即便这样深入研究这些已知的细节，仍然可能遇到一些理解上的问题。所以基于我们的两个研究目标，遇到不懂的问题时，我们会向榜单的运维者提问。之前我在你们的Github上也问过一些问题，也是基于这个原因，所以后来我们通过微信进一步交流了。

徐亮：对。

赵志民：然后我们通过这样的一个反复交流，通过不断的阅读和讨论，我们团队内部也进行持续的讨论，这样的一个迭代过程，最终解决了榜单运维和榜单smell的问题。这就是整个研究流程。接下来是榜单的分类，这部分我之前已经展示过了，和刚才看到的是一模一样的。你可以看到，PapersWithCode平台上大模型的榜单是最多的。我们还分析了不同平台上的榜单特征，比如是否有GitHub仓库。如果有GitHub仓库，可以看出这个榜单更加开源，测试代码都可以找到。我们还看了是否有过主要发布（Major Release），比如OpenCompass有V1和V2。我们也研究了榜单的提交渠道和要求，是否允许提交模型之外的内容，比如数据集或者大模型作为新的评测者（Judge）。

这是我们搜集到的一些具体数据，感兴趣的话可以详细看一下。接下来我给你看一个经典的榜单样貌。这个榜单上有不同的选项，比如模型信息和提交的地方。这是Chatbot Arena，大家应该都听说过，它也有实验的场所。不同榜单通常会有一些选项卡（tab），比如我们称之为Ranking DataFrame，可以对数据进行排序和过滤等操作。这些都是榜单的必备要素。我们研究了这些必备要素，加上提交要求和渠道，最后总结出五个工作流。

第一个工作流（P1）是用户提交评估测评结果，模型运维者直接进行集成。我不知道你是否用过PapersWithCode，它有两种渠道，一种是从已经发表的文章中自动抓取测评结果，另一种是用户可以自己上传测评结果，不需要额外的验证或测评轨迹，只要提供测评结果即可生成榜单。用户也可以直接修改或删除榜单数据，非常自由。

第二种工作流（P2）是用户上传预测结果，然后榜单会用测试数据进行检验，用户可以对照测试数据的标签（Label），计算出准确度或其他指标，最后这些指标会被融合到榜单中。

第三种工作流（P3）比较特别，是用户可以对模型进行投票或评选，榜单运维者根据投票结果决定榜单排名。这种方式有点像GitHub的Star榜单，基于用户的喜爱程度来进行排名。

第四种工作流（P4）是比较传统的，用户上传模型，运维者负责进行测评。如果运维者没有ground truth，就可能使用大模型作为评测者（Judge），像Chatbot Arena一样，具有一定的主观性，但仍属于传统的测评方式，每个模型都可以单独生成一个测评记录。

接下来是第五种模式（P5）……

徐亮：稍等，P4的情况下，用户上传的是什么？

赵志民：是模型文件或API。P5的情况下，用户也是上传模型，但唯一的区别是上传之后可以通过第三方进行比较，就像Chatbot Arena一样，用户生成推理结果后，根据推理的满意度给出评价，判断哪个模型表现更好，或者没有明显更好的模型。P5的模式就是这样。我们发现P1是最普遍的，接近50%，而P4是第二普遍的，大约占40%。

徐亮：嗯，说到这里，我想起我们最早做CLUE的时候，用户可以自己上传预测结果，然后我们后台用Ground Truth进行对比，给出一个分数。最初我们是以开源形式做的CLUE，现在则是用户需要提供API，上传后我们通过API进行测评并给出分数。

赵志民：对，这和我对你们的研究观察是相符的。当时我也观察研究了你们的流程，大部分符合P4模式，当然也有符合P5的，比如琅琊榜。

徐亮：对。

赵志民：当然，我想说的还不止这些。我们接着看。根据我们之前的总结，我们绘制了一个类似流程图的展示，虽然它不是真正的流程图，但我们称之为领域概念图，类似于UML表格，不知道您是否了解。这个图中显示，每个榜单都由三部分构成：提交层、评估层和集成层。这三部分相互作用，构成了整个榜单的预备机制。这是我们的第一个研究问题。

接下来是第二个研究问题，关于榜单smell。我们在研究领域模型如何在某个榜单上作用的同时，也发现了许多问题。为了解决这些问题，我们通过GitHub、Hugging Face或邮件与榜单的运维者进行沟通，等待他们的反馈验证。当然，有些问题很难验证，因为并不是所有榜单运维者都会回复我们。于是，我们只能尽最大努力判断这个问题是否存在，这其中确实有一些不确定性。

比如说，我们一共发现了8个榜单smell。第一个叫做“confusing entity”，即令人困惑的实体。比如文档告诉你如何进行测评，但实际操作时不够清晰，导致用户在执行过程中感到困惑，不知道该怎么做。
第二个是“deprecated entity”，指的是某些东西已经过时了，比如一些数据集或者任务，之前还在用，但后来失效或被弃用，此时榜单已经和这些内容无关了，这就叫过时的smell。
第三个是“inaccessible entity”，就是说想要访问某个内容，但发现链接无效，无法访问。
第四个是“misdisplayed entity”，这与榜单的视觉展示有关。如果榜单的视觉元素出现问题，比如不同行显示错位等，我们认为它是一个misdisplayed entity。
第五个是“mismatched entity”，指的是榜单所有者在文档里声称事实A成立，但我们操作时发现其实是事实B成立，这种不匹配就是mismatched entity。
第六个是“missing entity”，即榜单中某个应有的内容缺失了，比如某个模型或数据集不见了，或者无法访问。虽然它和inaccessible entity有些相似，但我们进行了细微区分。对于inaccessible entity，我们认为内容确实存在，但由于访问问题（如gateway timeout）导致不可访问；而missing entity则更偏向于内容不存在，比如404错误。
第七个是“redundant entity”，指的是某个实体重复出现，但没有必要，比如一个榜单中对于GPT-4只需要一条记录，但它可能出现了三四次，这些多余的测评记录是无效的。
最后一个是“unresponsive entity”，这个在Hugging Face上特别常见，很多space可以访问，但却一直处于runtime error状态，不知道你是否遇到过类似情况？

徐亮：赵志民：对，它确实有计算资源分配的问题，有时候资源不足，使用时会遇到限制。

关于这个，我们做了不同smell与各个workflow的对应关系，可以看到P4几乎涵盖了所有的smell，是最普遍的情况，其次是P2和P1。这就是我们研究的主要成果。徐亮，我不知道您有没有其他疑问？如果有问题，我可以回答之后做一个反馈。这个反馈对我很关键。不过在做反馈之前，我可能会提到您的个人信息，比如说您是SuperCLUE的主要运营人，不知道您是否介意？

徐亮：如果只是这些信息的话，对我没有什么影响。

赵志民：好的，那在致谢部分，您希望提到您的名字和组织吗？如果不希望的话，我们可以匿名。如果您希望提到，也没问题。

徐亮：这块随便写，主要不涉及隐私就可以，应该没什么问题。

赵志民：那我会把您的名字和组织提到致谢部分，尊重您的选择。

徐亮：了解。我有个问题，您的研究视角挺独特的，因为我没见过有人从这个角度做研究。您这个研究的目的是什么？

赵志民：我们研究的目的一共有两个方面。第一个是服务于榜单运维者，通过列举这些smell，帮助他们意识到不同类型的潜在问题，预测模型开发时可能遇到的陷阱。这样在开发过程中，他们可以更加注意避免这些问题。第二个是针对榜单用户，主要是模型的开发者和使用者。我们希望通过展示这些工作流，帮助模型开发者更好地了解榜单的内部机制。对于一些新进入领域的开发者，能够作为一种参考和借鉴。

徐亮：哦，原来你们的研究服务对象也包括像我这样的榜单运维者。

赵志民：对，没错。

徐亮：另外，你提到的这些上千个榜单，至少有几百个吧，里面有不少数据和问题。你们是人工去一个一个汇总和分析的吗？

赵志民：是的，实际上这个研究花了很长时间。最初这是我上学期做的一个学生项目，从去年的11月份就开始了。因为榜单的收集过程很复杂，尤其是与用户反馈的部分周期很长。我们需要不断地反馈和修改，直到今年7月初才提交稿件。有些榜单的运维者确实反应比较慢，联系他们之后也不一定会主动回复，所以整个过程拖得比较久。

徐亮：了解。最后一个问题是，为什么你会特别关注到leaderboard这方面呢？

赵志民：这主要是因为大模型的测评确实是一个非常重要的领域。很多人关心测评的各个方面，比如它是否已经达到饱和，是否存在数据泄露，以及最终生成的结果是否令人满意和可靠。虽然测评过程可能没有问题，但如果最终呈现的结果出错，那就很麻烦了。毕竟，用户最终看到的是成品，他们很少会关注中间的测评过程，比如测评时进行了多少步或者用了哪些指标。除了专业人士，大多数普通用户对此并不感兴趣。他们更关心的是榜单排名结果是否与自己的体验一致，而对测评过程中的技术细节则不了解或不关心。

有些模型用户可能没有足够的资源去自己实践这些测评，因此榜单对他们尤其重要。他们可能只有非常有限的算力，无法进行大规模的定制化测试。而大机构则不同，他们可以根据自己的预算进行尽可能多的测评，完全可以忽视榜单。两者的资源差异非常大，这是我们的一些理解，不知道是否正确。

徐亮：对，没错，你的理解很准确。

赵志民：那这样吧，我有一些问题想请教您，现在可以提问吗？因为这些问题对我们来说非常重要。

徐亮：可以的，没问题。

赵志民：首先，我想问一下，关于我之前展示的内容，尤其是分为两个部分的工作流和榜单smell，不知道您自己有没有相关的经历？

徐亮：嗯，我之前在做一些开源项目时确实接触过这方面的内容。首先，从工作流的角度来说，我们也有让大家提交模型的流程。实际上，在我们正式做这些之前，比如我之前参与的一个项目叫Guru，我们也参考了它的流程。所以在发布榜单或制定机制之前，通常会初步了解一下类似的系统。当然，我们没有像你们那样深入地去研究各种不同的机制，更多是针对某些特定的榜单进行分析。如果我们发现某个榜单的机制不错，我们就会参考它。

赵志民：具体来说，您们参考的榜单机制是什么样的？

徐亮：通常是根据榜单发布者的需求来定的，主要考虑哪种方式对他们来说更容易维护，同时也能满足需求。例如，早期我们开源时，选定了一种用户提交的方式，但同时也考虑过是否可以通过上传模型在一个docker环境中进行自动测评。不过，由于资源限制，我们没能完全实现这种更好的机制。尽管有时候有其他流程的存在，比如自动化测评，但因为实际资源有限，最终我们还是采用了相对更容易实现的方式。

赵志民：好的，那您看一下这个榜单smell，您觉得这些问题对您来说是有意义的吗？

徐亮：嗯，榜单smell的概念对我们来说是有道理的。我们在发布榜单时，通常会在GitHub上提供文档，告诉大家提交流程，并放出一些必要的代码。我们的初期目标之一就是实现“一键提交”，即用户可以一键下载数据集，不需要写太多代码，只需将结果提交到指定文件夹后就能运行。初期这个流程还是比较顺利的。不过，随着时间推移，会出现榜单过拟合的问题，或者在少数情况下，运行过程中出现一些bug或者未预料到的情况，导致提交失败。我当时作为个人维护者，也负责维护这些榜单。我们还有一些子榜单，维护质量参差不齐，也出现了各种问题。

您这里提到的数据或任务本身的问题，这些也是用户非常关心的。随着时间的推移，一些用户开始关注任务是否过拟合、饱和，或者是否有挑战性。所以，您这边提到的smell似乎更侧重于流程和运维问题，是吗？

赵志民：是的，那您觉得这八种smell在您的榜单运营过程中，有出现过吗？您有感受到这些问题吗？

徐亮：对，比如说“confusing entity”这一项。有些榜单的文档写得不太清楚，用户在操作时会有疑问。有时我们会在GitHub上看到用户提issue，遇到共性的问题我们会尝试修复，并在文档中做出解释。但我有个问题，比如说任务本身设定不合理，或者任务中有少量错误，这些属于您提到的smell问题吗？还是另一个类型的问题？

赵志民：我们的研究主要是围绕榜单的运维进行的。我们认为榜单的运维虽然和基准测试（benchmark）运维有联系，但很多benchmark的问题是榜单运维者无法解决的。比如说，如果benchmark本身存在偏差或数据泄露，榜单运维者除了选择或不选择这个benchmark之外，几乎无法做更多的调整。所以我们专注于榜单自身的问题，而不是benchmark的问题。因此，我们没有将过拟合或数据泄露等问题纳入到smell的范畴。

徐亮：嗯，明白。

赵志民：那么除了数据泄露这方面，您觉得还有没有我们没有关注到的地方？或者在我们提出的领域模型中，有没有遗漏的内容？

徐亮：让我再看一下你们的领域模型图。你们的模型涉及提交、评估和集成，发布这一部分是你们关注的吗？发布的意思是，整体上有一个新的结果发布，或者重要模型上线后，通过社交媒体或者其他形式发布并介绍它的亮点。我看到一些做得不错的榜单也做了这部分工作。

赵志民：您刚才提到的“发布”，我不太确定是否完全理解。您说的发布和我们所指的发布有什么区别？

徐亮：我说的发布指的是榜单的更新机制。比如说提交了新结果后，是立即更新，还是有一个固定的时间进行统一更新？比如，每周五下午三点钟更新，或者像我们做的月榜，每两个月发布一次，在月底或双月底发布整体的结果。这个发布和你们关注的内容有关系吗？也就是榜单的更新和发布机制，这是否在你们的研究范畴内？

赵志民：明白了，这个问题确实在我们的研究的“implication”部分有提到过。我也注意到一些榜单有自己的发布周期，但由于研究时间有限，我们没有深入探讨这部分，可能会作为未来研究的方向。您提到这一点很好，我们会记录下来。那除了这方面，您觉得我们在smell这部分有遗漏的地方吗？

徐亮：我再看看。你说的这个entity具体指的是什么？

赵志民：在我们的研究中，entity指的是榜单中的某个实体。我们对榜单进行了组件拆解，entity可以是数据集、基准评估协议、指标、任务、测评的最终分数、模型本身，甚至是榜单的表格结构，或者是提交的机制和渠道。这些是我们研究的八个smell所涵盖的对象。

徐亮：嗯，听起来已经很详细和具体了。

赵志民：对，那您觉得我们的研究，谁会是最大的受益者呢？

徐亮：我认为最大的受益者应该是榜单的维护者或者开发者。这个研究为他们提供了一个新的视角，让他们了解在维护榜单时可能有不同的流程可以选择。此外，维护过程中可能会遇到各种问题，通过你的研究，他们可以提前预见并避免一些常见问题，进而提升榜单的质量和运行效率。

赵志民：我明白了，那再问一个问题，就是关于我提出的这些smell，您觉得这些对用户的体验有多重要？

徐亮：用户的体验啊……

赵志民：对。

徐亮：嗯，我觉得用户确实比较在意这方面的问题，特别是希望操作过程能够简单流畅。比如，我们之前努力实现的，就是用户不需要写任何代码，只需要下载项目、更新结果到项目中，然后一键运行。因此，用户会比较关心流程的顺畅性，以及文档的完整性，确保他们知道需要做的步骤，并且能够得到清晰的引导。

赵志民：明白，那我想问一下，根据您多年的运维经验，您认为我提到的这些榜单smell，对于榜单运维者来说是一个真正的问题吗？是被视作正常（expected）的现象，还是更多是一种异常（abnormal）的情况呢？

徐亮：对，我感觉这个问题可以分为两部分。第一部分是在榜单刚开始运行时，多数情况下问题不多，因为通常发布前会进行自测，而且好的榜单维护者会把自测的流程记录下来，所以一开始的运行通常是顺畅的。问题在于，随着榜单的演进、更新，以及长时间运行，可能就会出现一些问题。如果维护不及时、发布不规律，或者是新的更改没有同步到位，就会导致你提到的这些smell问题出现。

赵志民：那么从技术角度来看，您认为要避免这些smell，是否就像您说的，开始时相对容易，但随着时间的推移，维护的挑战性会越来越大？是这个意思吗？

徐亮：嗯，对的。

赵志民：好的，我的问题基本上都问完了。感谢您的反馈。我会记下您的建议并将其纳入我们的研究中。我也会在致谢部分提到您的名字和组织。如果您有任何其他问题或建议，请随时告诉我。